{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a17a4ac7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-22T05:01:51.161426Z",
     "iopub.status.busy": "2026-02-22T05:01:51.161009Z",
     "iopub.status.idle": "2026-02-22T05:01:52.354670Z",
     "shell.execute_reply": "2026-02-22T05:01:52.353305Z"
    },
    "papermill": {
     "duration": 1.204239,
     "end_time": "2026-02-22T05:01:52.357047",
     "exception": false,
     "start_time": "2026-02-22T05:01:51.152808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/Data_Klaim.csv\n",
      "/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/sample_submission.csv\n",
      "/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/Data_Polis.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763dc3d8",
   "metadata": {
    "papermill": {
     "duration": 0.006165,
     "end_time": "2026-02-22T05:01:52.369172",
     "exception": false,
     "start_time": "2026-02-22T05:01:52.363007",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DATA FOUNDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f7c55b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T05:01:52.383272Z",
     "iopub.status.busy": "2026-02-22T05:01:52.382726Z",
     "iopub.status.idle": "2026-02-22T05:01:52.654585Z",
     "shell.execute_reply": "2026-02-22T05:01:52.653294Z"
    },
    "papermill": {
     "duration": 0.28174,
     "end_time": "2026-02-22T05:01:52.656728",
     "exception": false,
     "start_time": "2026-02-22T05:01:52.374988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVICE_COL: tanggal_pasien_masuk_rs\n",
      "EXPOSURE_MODE: inforce\n",
      "Policy start col: tanggal_efektif_polis\n",
      "Frequency source: claim_id\n",
      "Monthly shape: (16, 34)\n",
      "Unique months: 16\n",
      "Exposure min/max: 4096.0 4096.0\n",
      "Total_claim min/max: 9610379678.55 17480540371.87\n",
      "\n",
      "STAGE 1 v4 â€” READY\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 1 v4 â€” FOUNDATION (DATASET-AWARE + NO TARGET DISTORTION)\n",
    "# - Fix YYYYMMDD parsing\n",
    "# - Keep RAW nominal for target (total_claim)\n",
    "# - Put winsorization into separate column (optional features)\n",
    "# - Build monthly with complete month range (fill missing months)\n",
    "# - Exposure: claimant / inforce (optional)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\n",
    "klaim = pd.read_csv(BASE_PATH + \"Data_Klaim.csv\")\n",
    "polis = pd.read_csv(BASE_PATH + \"Data_Polis.csv\")\n",
    "\n",
    "# =============================\n",
    "# CLEAN COLUMN NAMES\n",
    "# =============================\n",
    "def clean_columns(df):\n",
    "    df = df.copy()\n",
    "    df.columns = (\n",
    "        df.columns.astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"_\", regex=False)\n",
    "        .str.replace(\"/\", \"_\", regex=False)\n",
    "        .str.replace(\"-\", \"_\", regex=False)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "klaim = clean_columns(klaim)\n",
    "polis = clean_columns(polis)\n",
    "\n",
    "# =============================\n",
    "# DATE PARSING (handle YYYYMMDD int + dd/mm/yyyy)\n",
    "# =============================\n",
    "def parse_mixed_date(s: pd.Series) -> pd.Series:\n",
    "    s = s.copy()\n",
    "    idx = s.index\n",
    "\n",
    "    # normalize to string for pattern checks\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        ss = s.astype(\"Int64\").astype(str)\n",
    "    else:\n",
    "        ss = s.astype(str).str.strip()\n",
    "\n",
    "    ss = ss.replace({\"<NA>\": np.nan, \"nan\": np.nan, \"None\": np.nan, \"NaT\": np.nan})\n",
    "\n",
    "    out = pd.Series(pd.NaT, index=idx, dtype=\"datetime64[ns]\")\n",
    "\n",
    "    # YYYYMMDD (8 digits)\n",
    "    m8 = ss.str.fullmatch(r\"\\d{8}\", na=False)\n",
    "    if m8.any():\n",
    "        out.loc[m8] = pd.to_datetime(ss.loc[m8], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "\n",
    "    # remaining\n",
    "    rest = ~m8 & ss.notna()\n",
    "    if rest.any():\n",
    "        has_slash = ss.loc[rest].str.contains(\"/\", na=False)\n",
    "        if has_slash.any():\n",
    "            out.loc[rest[rest].index.intersection(ss.loc[rest][has_slash].index)] = pd.to_datetime(\n",
    "                ss.loc[rest][has_slash], errors=\"coerce\", dayfirst=True\n",
    "            )\n",
    "        if (~has_slash).any():\n",
    "            out.loc[rest[rest].index.intersection(ss.loc[rest][~has_slash].index)] = pd.to_datetime(\n",
    "                ss.loc[rest][~has_slash], errors=\"coerce\"\n",
    "            )\n",
    "\n",
    "    return out\n",
    "\n",
    "for col in klaim.columns:\n",
    "    if \"tanggal\" in col:\n",
    "        klaim[col] = parse_mixed_date(klaim[col])\n",
    "\n",
    "for col in polis.columns:\n",
    "    if \"tanggal\" in col:\n",
    "        polis[col] = parse_mixed_date(polis[col])\n",
    "\n",
    "# =============================\n",
    "# SAFE DEDUP\n",
    "# =============================\n",
    "claim_id_col = None\n",
    "for c in [\"claim_id\", \"id_klaim\", \"klaim_id\"]:\n",
    "    if c in klaim.columns:\n",
    "        claim_id_col = c\n",
    "        break\n",
    "\n",
    "if claim_id_col is not None:\n",
    "    klaim = klaim.drop_duplicates(subset=[claim_id_col]).reset_index(drop=True)\n",
    "else:\n",
    "    klaim = klaim.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "polis = polis.drop_duplicates(subset=[\"nomor_polis\"]).reset_index(drop=True)\n",
    "\n",
    "# =============================\n",
    "# BASIC CLEANING\n",
    "# =============================\n",
    "# choose service date column\n",
    "service_col = \"tanggal_pasien_masuk_rs\" if \"tanggal_pasien_masuk_rs\" in klaim.columns else None\n",
    "if service_col is None:\n",
    "    # fallback: first tanggal* column\n",
    "    tcols = [c for c in klaim.columns if \"tanggal\" in c]\n",
    "    service_col = tcols[0] if len(tcols) else None\n",
    "\n",
    "if service_col is None:\n",
    "    raise ValueError(\"No tanggal column found in klaim for building year_month.\")\n",
    "\n",
    "klaim = klaim.dropna(subset=[\"nomor_polis\", service_col]).copy()\n",
    "\n",
    "# nominal column\n",
    "nom_col = \"nominal_klaim_yang_disetujui\"\n",
    "if nom_col not in klaim.columns:\n",
    "    # fallback: try find 'nominal' column\n",
    "    cand = [c for c in klaim.columns if \"nominal\" in c]\n",
    "    if len(cand) == 0:\n",
    "        raise ValueError(\"No nominal column found in klaim.\")\n",
    "    nom_col = cand[0]\n",
    "\n",
    "# IMPORTANT: keep RAW nominal for target\n",
    "raw_nom = pd.to_numeric(klaim[nom_col], errors=\"coerce\").fillna(0).clip(lower=0)\n",
    "klaim[nom_col] = raw_nom\n",
    "\n",
    "# OPTIONAL: winsorized copy for feature engineering (NOT for target)\n",
    "klaim[\"nominal_klaim_clip\"] = raw_nom.copy()\n",
    "pos = klaim[\"nominal_klaim_clip\"] > 0\n",
    "if pos.any():\n",
    "    low_q  = klaim.loc[pos, \"nominal_klaim_clip\"].quantile(0.005)\n",
    "    high_q = klaim.loc[pos, \"nominal_klaim_clip\"].quantile(0.995)\n",
    "    klaim.loc[pos, \"nominal_klaim_clip\"] = klaim.loc[pos, \"nominal_klaim_clip\"].clip(low_q, high_q)\n",
    "\n",
    "# =============================\n",
    "# MERGE\n",
    "# =============================\n",
    "df = klaim.merge(polis, on=\"nomor_polis\", how=\"left\")\n",
    "\n",
    "# =============================\n",
    "# SERVICE MONTH\n",
    "# =============================\n",
    "df[\"year_month\"] = df[service_col].dt.to_period(\"M\")\n",
    "\n",
    "min_m = df[\"year_month\"].min()\n",
    "max_m = df[\"year_month\"].max()\n",
    "all_months = pd.period_range(min_m, max_m, freq=\"M\")\n",
    "\n",
    "# ============================================================\n",
    "# EXPOSURE OPTIONS\n",
    "# ============================================================\n",
    "EXPOSURE_MODE = \"inforce\"  # \"claimant\" or \"inforce\"\n",
    "\n",
    "# claimant exposure: unique policies that claim in that month\n",
    "expo_claimant = (\n",
    "    df.groupby(\"year_month\")[\"nomor_polis\"].nunique()\n",
    "      .reindex(all_months, fill_value=0)\n",
    "      .rename(\"exposure_claimant\")\n",
    "      .rename_axis(\"year_month\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# inforce exposure: cumulative started policies (no end date available)\n",
    "start_col = None\n",
    "for c in [\"tanggal_efektif_polis\", \"tanggal_mulai_polis\", \"tanggal_mulai\"]:\n",
    "    if c in polis.columns:\n",
    "        start_col = c\n",
    "        break\n",
    "\n",
    "if start_col is not None:\n",
    "    p = polis[[\"nomor_polis\", start_col]].dropna(subset=[start_col]).copy()\n",
    "    p[\"start_m\"] = p[start_col].dt.to_period(\"M\")\n",
    "\n",
    "    base = p.loc[p[\"start_m\"] < min_m, \"nomor_polis\"].nunique()\n",
    "    inc = p.loc[p[\"start_m\"] >= min_m].groupby(\"start_m\")[\"nomor_polis\"].nunique()\n",
    "\n",
    "    expo_inforce = (\n",
    "        (base + inc.reindex(all_months, fill_value=0).cumsum())\n",
    "        .rename(\"exposure_inforce\")\n",
    "        .rename_axis(\"year_month\")\n",
    "        .reset_index()\n",
    "    )\n",
    "else:\n",
    "    expo_inforce = expo_claimant[[\"year_month\"]].copy()\n",
    "    expo_inforce[\"exposure_inforce\"] = 0\n",
    "\n",
    "expo = expo_claimant.merge(expo_inforce, on=\"year_month\", how=\"left\")\n",
    "\n",
    "# choose exposure with fallback safety\n",
    "expo[\"exposure\"] = np.where(EXPOSURE_MODE == \"inforce\", expo[\"exposure_inforce\"], expo[\"exposure_claimant\"])\n",
    "# if inforce is mostly 0 (bad parsing / missing), fallback to claimant\n",
    "if (EXPOSURE_MODE == \"inforce\") and (expo[\"exposure\"].sum() == 0):\n",
    "    expo[\"exposure\"] = expo[\"exposure_claimant\"]\n",
    "\n",
    "# merge exposure into df (keperluan stage lain)\n",
    "df = df.merge(expo[[\"year_month\", \"exposure\"]], on=\"year_month\", how=\"left\")\n",
    "df[\"active_policies\"] = df[\"exposure\"]\n",
    "\n",
    "# ============================================================\n",
    "# MONTHLY CORE TABLE (complete months)\n",
    "# target total_claim MUST be RAW nominal\n",
    "# ============================================================\n",
    "freq_col = claim_id_col if claim_id_col is not None else \"nomor_polis\"\n",
    "\n",
    "monthly_core = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(\n",
    "          frequency=(freq_col, \"count\"),\n",
    "          total_claim=(nom_col, \"sum\")\n",
    "      )\n",
    "      .reindex(all_months, fill_value=0)\n",
    "      .rename_axis(\"year_month\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "monthly = monthly_core.merge(expo[[\"year_month\", \"exposure\"]], on=\"year_month\", how=\"left\")\n",
    "\n",
    "monthly[\"severity\"] = monthly[\"total_claim\"] / monthly[\"frequency\"].replace(0, np.nan)\n",
    "monthly[\"claim_rate\"] = monthly[\"frequency\"] / monthly[\"exposure\"].replace(0, np.nan)\n",
    "\n",
    "# ============================================================\n",
    "# LOG FEATURES\n",
    "# ============================================================\n",
    "monthly[\"log_total\"] = np.log1p(monthly[\"total_claim\"])\n",
    "monthly[\"log_freq\"]  = np.log1p(monthly[\"frequency\"])\n",
    "monthly[\"log_sev\"]   = np.log1p(monthly[\"severity\"])\n",
    "monthly[\"log_rate\"]  = np.log1p(monthly[\"claim_rate\"])\n",
    "\n",
    "# ============================================================\n",
    "# VOLATILITY\n",
    "# ============================================================\n",
    "monthly[\"roll6\"] = monthly[\"total_claim\"].rolling(6, min_periods=3).mean()\n",
    "monthly[\"std6\"]  = monthly[\"total_claim\"].rolling(6, min_periods=3).std()\n",
    "monthly[\"vol_ratio\"] = monthly[\"std6\"] / monthly[\"roll6\"]\n",
    "monthly[\"high_vol_regime\"] = (monthly[\"vol_ratio\"] > monthly[\"vol_ratio\"].median()).astype(int)\n",
    "\n",
    "# ============================================================\n",
    "# TIME FEATURES\n",
    "# ============================================================\n",
    "monthly[\"month\"] = monthly[\"year_month\"].dt.month\n",
    "monthly[\"month_sin\"] = np.sin(2*np.pi*monthly[\"month\"]/12)\n",
    "monthly[\"month_cos\"] = np.cos(2*np.pi*monthly[\"month\"]/12)\n",
    "monthly[\"month_index\"] = np.arange(len(monthly))\n",
    "\n",
    "# ============================================================\n",
    "# SAFE LAGS\n",
    "# ============================================================\n",
    "for col in [\"log_total\", \"log_freq\", \"log_sev\", \"log_rate\"]:\n",
    "    monthly[f\"{col}_lag1\"] = monthly[col].shift(1)\n",
    "    monthly[f\"{col}_lag2\"] = monthly[col].shift(2)\n",
    "    monthly[f\"{col}_lag3\"] = monthly[col].shift(3)\n",
    "    monthly[f\"{col}_roll3\"] = monthly[col].shift(1).rolling(3).mean()\n",
    "\n",
    "monthly = monthly.dropna().reset_index(drop=True)\n",
    "\n",
    "# ============================================================\n",
    "# FINAL CHECK\n",
    "# ============================================================\n",
    "print(\"SERVICE_COL:\", service_col)\n",
    "print(\"EXPOSURE_MODE:\", EXPOSURE_MODE)\n",
    "print(\"Policy start col:\", start_col)\n",
    "print(\"Frequency source:\", freq_col)\n",
    "print(\"Monthly shape:\", monthly.shape)\n",
    "print(\"Unique months:\", monthly[\"year_month\"].nunique())\n",
    "print(\"Exposure min/max:\", float(monthly[\"exposure\"].min()), float(monthly[\"exposure\"].max()))\n",
    "print(\"Total_claim min/max:\", float(monthly[\"total_claim\"].min()), float(monthly[\"total_claim\"].max()))\n",
    "print(\"\\nSTAGE 1 v4 â€” READY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c612b8ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T05:01:52.669808Z",
     "iopub.status.busy": "2026-02-22T05:01:52.669457Z",
     "iopub.status.idle": "2026-02-22T05:01:52.686284Z",
     "shell.execute_reply": "2026-02-22T05:01:52.685071Z"
    },
    "papermill": {
     "duration": 0.026326,
     "end_time": "2026-02-22T05:01:52.688747",
     "exception": false,
     "start_time": "2026-02-22T05:01:52.662421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year_month  frequency  exposure  freq_per_exposure\n",
      "6     2024-10        274      4096           0.066895\n",
      "7     2024-11        270      4096           0.065918\n",
      "8     2024-12        238      4096           0.058105\n",
      "9     2025-01        216      4096           0.052734\n",
      "10    2025-02        246      4096           0.060059\n",
      "11    2025-03        230      4096           0.056152\n",
      "12    2025-04        208      4096           0.050781\n",
      "13    2025-05        239      4096           0.058350\n",
      "14    2025-06        234      4096           0.057129\n",
      "15    2025-07        264      4096           0.064453\n",
      "freq_per_exposure min/max: 0.05078125 0.06689453125\n"
     ]
    }
   ],
   "source": [
    "tmp = monthly.copy()\n",
    "tmp[\"freq_per_exposure\"] = tmp[\"frequency\"] / tmp[\"exposure\"]\n",
    "print(tmp[[\"year_month\",\"frequency\",\"exposure\",\"freq_per_exposure\"]].tail(10))\n",
    "print(\"freq_per_exposure min/max:\",\n",
    "      tmp[\"freq_per_exposure\"].min(),\n",
    "      tmp[\"freq_per_exposure\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258d9c47",
   "metadata": {
    "papermill": {
     "duration": 0.005595,
     "end_time": "2026-02-22T05:01:52.700473",
     "exception": false,
     "start_time": "2026-02-22T05:01:52.694878",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TIME-SERIES DATASET ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f09cbfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T05:01:52.715030Z",
     "iopub.status.busy": "2026-02-22T05:01:52.714696Z",
     "iopub.status.idle": "2026-02-22T05:01:52.841007Z",
     "shell.execute_reply": "2026-02-22T05:01:52.839825Z"
    },
    "papermill": {
     "duration": 0.136552,
     "end_time": "2026-02-22T05:01:52.843517",
     "exception": false,
     "start_time": "2026-02-22T05:01:52.706965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPACT PANEL SHAPE: (414, 29)\n",
      "Unique segments: 41\n",
      "Columns: 29\n",
      "\n",
      "STAGE 2 â€” ELITE SEGMENT PANEL READY\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 2 â€” ELITE SEGMENT PANEL (SAFE VERSION)\n",
    "# No KeyError â€¢ Auto-create missing columns â€¢ Short series safe\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ ENSURE REQUIRED SEGMENT COLUMNS EXIST\n",
    "# ============================================================\n",
    "\n",
    "# Care Type\n",
    "if \"care_type\" not in df.columns:\n",
    "    if \"inpatient_outpatient\" in df.columns:\n",
    "        df[\"care_type\"] = (\n",
    "            df[\"inpatient_outpatient\"]\n",
    "            .astype(str)\n",
    "            .str.upper()\n",
    "            .str.strip()\n",
    "        )\n",
    "    else:\n",
    "        df[\"care_type\"] = \"UNKNOWN\"\n",
    "\n",
    "df[\"care_type\"] = df[\"care_type\"].fillna(\"UNKNOWN\")\n",
    "\n",
    "\n",
    "# Cashless\n",
    "if \"is_cashless\" not in df.columns:\n",
    "    if \"reimburse_cashless\" in df.columns:\n",
    "        rc = df[\"reimburse_cashless\"].astype(str).str.upper().str.strip()\n",
    "        df[\"is_cashless\"] = rc.eq(\"C\").astype(int)\n",
    "    else:\n",
    "        df[\"is_cashless\"] = 0\n",
    "\n",
    "\n",
    "# RS Bucket\n",
    "if \"rs_bucket\" not in df.columns:\n",
    "    if \"lokasi_rs\" in df.columns:\n",
    "        loc = df[\"lokasi_rs\"].astype(str).str.upper().str.strip()\n",
    "        df[\"rs_bucket\"] = np.select(\n",
    "            [\n",
    "                loc.eq(\"INDONESIA\"),\n",
    "                loc.eq(\"SINGAPORE\"),\n",
    "                loc.eq(\"MALAYSIA\")\n",
    "            ],\n",
    "            [\"ID\",\"SG\",\"MY\"],\n",
    "            default=\"OTHER\"\n",
    "        )\n",
    "    else:\n",
    "        df[\"rs_bucket\"] = \"OTHER\"\n",
    "\n",
    "df[\"rs_bucket\"] = df[\"rs_bucket\"].fillna(\"OTHER\")\n",
    "\n",
    "\n",
    "# Plan Code\n",
    "if \"plan_code\" not in df.columns:\n",
    "    df[\"plan_code\"] = \"UNKNOWN\"\n",
    "\n",
    "df[\"plan_code\"] = df[\"plan_code\"].fillna(\"UNKNOWN\")\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ DEFINE SEGMENT COLUMNS\n",
    "# ============================================================\n",
    "\n",
    "seg_cols = [\"plan_code\",\"care_type\",\"is_cashless\",\"rs_bucket\"]\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ BUILD SEGMENT MONTHLY\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly = (\n",
    "    df.groupby([\"year_month\"] + seg_cols)\n",
    "      .agg(\n",
    "          frequency=(\"nomor_polis\",\"count\"),\n",
    "          total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\"),\n",
    "          exposure=(\"nomor_polis\",\"nunique\")\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(seg_cols + [\"year_month\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ TARGETS\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"severity\"] = (\n",
    "    seg_monthly[\"total_claim\"] /\n",
    "    seg_monthly[\"frequency\"].replace(0, np.nan)\n",
    ")\n",
    "\n",
    "seg_monthly[\"log_total\"] = np.log1p(seg_monthly[\"total_claim\"])\n",
    "seg_monthly[\"log_freq\"]  = np.log1p(seg_monthly[\"frequency\"])\n",
    "seg_monthly[\"log_sev\"]   = np.log1p(seg_monthly[\"severity\"])\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ CALENDAR\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"month\"] = seg_monthly[\"year_month\"].dt.month\n",
    "seg_monthly[\"month_sin\"] = np.sin(2*np.pi*seg_monthly[\"month\"]/12)\n",
    "seg_monthly[\"month_cos\"] = np.cos(2*np.pi*seg_monthly[\"month\"]/12)\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ LAGS (STRICT NO LEAKAGE)\n",
    "# ============================================================\n",
    "\n",
    "for col in [\"log_total\",\"log_freq\",\"log_sev\"]:\n",
    "    \n",
    "    seg_monthly[f\"{col}_lag1\"] = \\\n",
    "        seg_monthly.groupby(seg_cols)[col].shift(1)\n",
    "    \n",
    "    seg_monthly[f\"{col}_lag2\"] = \\\n",
    "        seg_monthly.groupby(seg_cols)[col].shift(2)\n",
    "    \n",
    "    seg_monthly[f\"{col}_lag3\"] = \\\n",
    "        seg_monthly.groupby(seg_cols)[col].shift(3)\n",
    "\n",
    "    seg_monthly[f\"{col}_roll3\"] = \\\n",
    "        seg_monthly.groupby(seg_cols)[col] \\\n",
    "        .transform(lambda x: x.shift(1).rolling(3).mean())\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ MOMENTUM\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"momentum_total\"] = (\n",
    "    seg_monthly[\"log_total_lag1\"] -\n",
    "    seg_monthly[\"log_total_lag2\"]\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ SEGMENT WEIGHT\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"seg_weight\"] = (\n",
    "    seg_monthly[\"frequency\"] /\n",
    "    seg_monthly.groupby(\"year_month\")[\"frequency\"].transform(\"sum\")\n",
    ").fillna(0)\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ SAFE TRAIN WINDOW\n",
    "# ============================================================\n",
    "\n",
    "seg_model = seg_monthly[\n",
    "    seg_monthly[\"log_total_lag3\"].notna()\n",
    "].reset_index(drop=True)\n",
    "\n",
    "seg_model = seg_model.fillna(0)\n",
    "\n",
    "# ============================================================\n",
    "# FINAL CHECK\n",
    "# ============================================================\n",
    "\n",
    "print(\"COMPACT PANEL SHAPE:\", seg_model.shape)\n",
    "print(\"Unique segments:\", seg_model[seg_cols].drop_duplicates().shape[0])\n",
    "print(\"Columns:\", len(seg_model.columns))\n",
    "print(\"\\nSTAGE 2 â€” ELITE SEGMENT PANEL READY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd5663c",
   "metadata": {
    "papermill": {
     "duration": 0.005468,
     "end_time": "2026-02-22T05:01:52.854735",
     "exception": false,
     "start_time": "2026-02-22T05:01:52.849267",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MODEL DEVELOPMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b79ddc8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T05:01:52.868342Z",
     "iopub.status.busy": "2026-02-22T05:01:52.867510Z",
     "iopub.status.idle": "2026-02-22T05:02:56.130802Z",
     "shell.execute_reply": "2026-02-22T05:02:56.129977Z"
    },
    "papermill": {
     "duration": 63.282835,
     "end_time": "2026-02-22T05:02:56.143072",
     "exception": false,
     "start_time": "2026-02-22T05:01:52.860237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Horizon months used : 5\n",
      "Best Config:\n",
      "  wt_total=0.85 (ETS weight), anchor_total=median\n",
      "  wt_freq =0.2 (ETS weight), anchor_freq =mean\n",
      "------------------------------\n",
      "STAGE 3 v17 MAPE Frequency : 5.1557\n",
      "STAGE 3 v17 MAPE Total     : 7.9753\n",
      "STAGE 3 v17 MAPE Severity  : 4.7684\n",
      "Estimated Score            : 5.9665\n",
      "==============================\n",
      "\n",
      "Preview last horizon months:\n",
      "   year_month  frequency   total_claim      severity  pred_frequency  \\\n",
      "14    2025-03        230  1.367924e+10  5.947496e+07      234.031716   \n",
      "15    2025-04        208  1.116425e+10  5.367427e+07      232.851773   \n",
      "16    2025-05        239  1.222680e+10  5.115814e+07      237.225688   \n",
      "17    2025-06        234  1.337312e+10  5.715008e+07      234.888808   \n",
      "18    2025-07        264  1.369923e+10  5.189101e+07      235.077202   \n",
      "\n",
      "      pred_total  pred_severity  \n",
      "14  1.224504e+10   5.232214e+07  \n",
      "15  1.224868e+10   5.260289e+07  \n",
      "16  1.222798e+10   5.154577e+07  \n",
      "17  1.221086e+10   5.198572e+07  \n",
      "18  1.219531e+10   5.187790e+07  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 3 v17 â€” KAGGLE-MATCH VALIDATION (AUTO-TUNED SHRINK)\n",
    "# - Horizon = unique months in sample_submission (usually 5)\n",
    "# - Predict TOTAL & FREQ directly (ETS on log1p), derive SEVERITY\n",
    "# - True recursive (refit each step on simulated history)\n",
    "# - Auto grid-search shrink weights + anchor type (mean/median)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\n",
    "sample_sub = pd.read_csv(BASE_PATH + \"sample_submission.csv\")\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "# ==============================\n",
    "# BUILD MONTHLY (consistent with Stage 1 v3)\n",
    "# ==============================\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(\n",
    "          frequency=(\"claim_id\",\"count\"),\n",
    "          total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\"),\n",
    "          exposure=(\"active_policies\",\"first\")\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(\"year_month\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "monthly[\"severity\"]   = monthly[\"total_claim\"] / monthly[\"frequency\"].replace(0, np.nan)\n",
    "monthly[\"claim_rate\"] = monthly[\"frequency\"] / monthly[\"exposure\"].replace(0, np.nan)\n",
    "\n",
    "# ==============================\n",
    "# HORIZON = months in sample_submission (Kaggle behavior)\n",
    "# ==============================\n",
    "sample_sub[\"year\"]  = sample_sub[\"id\"].str.split(\"_\").str[0]\n",
    "sample_sub[\"month\"] = sample_sub[\"id\"].str.split(\"_\").str[1]\n",
    "sample_sub[\"month_key\"] = sample_sub[\"year\"] + \"-\" + sample_sub[\"month\"]\n",
    "\n",
    "future_periods = (\n",
    "    pd.PeriodIndex(sample_sub[\"month_key\"], freq=\"M\")\n",
    "      .unique()\n",
    "      .sort_values()\n",
    ")\n",
    "\n",
    "H = int(len(future_periods))\n",
    "H = min(H, max(1, len(monthly) - 6))  # safety\n",
    "\n",
    "# ==============================\n",
    "# SIMULATOR (true recursive)\n",
    "# ==============================\n",
    "def simulate(train_df, H, wt_total, wt_freq, anchor_total=\"mean\", anchor_freq=\"mean\"):\n",
    "    sim_df = train_df.copy()\n",
    "\n",
    "    pred_total, pred_freq, pred_sev = [], [], []\n",
    "\n",
    "    for step in range(H):\n",
    "        hist = sim_df.copy()\n",
    "\n",
    "        # ---- TOTAL ETS on log1p ----\n",
    "        try:\n",
    "            mdl_t = ExponentialSmoothing(\n",
    "                np.log1p(hist[\"total_claim\"].astype(float)),\n",
    "                trend=\"add\",\n",
    "                damped_trend=True,\n",
    "                seasonal=None\n",
    "            ).fit()\n",
    "            total_fc = float(np.expm1(mdl_t.forecast(1).iloc[0]))\n",
    "        except:\n",
    "            total_fc = float(hist[\"total_claim\"].iloc[-1])\n",
    "\n",
    "        # anchor total\n",
    "        if anchor_total == \"median\":\n",
    "            total_anchor = float(hist[\"total_claim\"].tail(3).median())\n",
    "        else:\n",
    "            total_anchor = float(hist[\"total_claim\"].tail(3).mean())\n",
    "\n",
    "        total_pred = wt_total * total_fc + (1 - wt_total) * total_anchor\n",
    "        total_pred = max(float(total_pred), 1.0)\n",
    "\n",
    "        # ---- FREQ ETS on log1p ----\n",
    "        try:\n",
    "            mdl_f = ExponentialSmoothing(\n",
    "                np.log1p(hist[\"frequency\"].astype(float)),\n",
    "                trend=\"add\",\n",
    "                damped_trend=True,\n",
    "                seasonal=None\n",
    "            ).fit()\n",
    "            freq_fc = float(np.expm1(mdl_f.forecast(1).iloc[0]))\n",
    "        except:\n",
    "            freq_fc = float(hist[\"frequency\"].iloc[-1])\n",
    "\n",
    "        # anchor freq\n",
    "        if anchor_freq == \"median\":\n",
    "            freq_anchor = float(hist[\"frequency\"].tail(3).median())\n",
    "        else:\n",
    "            freq_anchor = float(hist[\"frequency\"].tail(3).mean())\n",
    "\n",
    "        freq_pred = wt_freq * freq_fc + (1 - wt_freq) * freq_anchor\n",
    "        freq_pred = max(float(freq_pred), 1.0)\n",
    "\n",
    "        sev_pred = total_pred / freq_pred\n",
    "\n",
    "        pred_total.append(total_pred)\n",
    "        pred_freq.append(freq_pred)\n",
    "        pred_sev.append(sev_pred)\n",
    "\n",
    "        # ---- append recursive row (keep year_month progressing) ----\n",
    "        last_period = hist[\"year_month\"].iloc[-1]\n",
    "        next_period = last_period + 1\n",
    "        exposure_next = float(hist[\"exposure\"].iloc[-1]) if \"exposure\" in hist.columns else np.nan\n",
    "\n",
    "        sim_df = pd.concat([sim_df, pd.DataFrame([{\n",
    "            \"year_month\": next_period,\n",
    "            \"frequency\": freq_pred,\n",
    "            \"total_claim\": total_pred,\n",
    "            \"exposure\": exposure_next,\n",
    "            \"severity\": sev_pred,\n",
    "            \"claim_rate\": (freq_pred / exposure_next) if (exposure_next and exposure_next > 0) else np.nan\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    return pred_total, pred_freq, pred_sev\n",
    "\n",
    "# ==============================\n",
    "# SPLIT (Kaggle-match horizon)\n",
    "# ==============================\n",
    "train = monthly.iloc[:-H].copy()\n",
    "valid = monthly.iloc[-H:].copy()\n",
    "\n",
    "# ==============================\n",
    "# AUTO SEARCH (small grid, fast)\n",
    "# ==============================\n",
    "wt_total_grid = [0.35, 0.45, 0.55, 0.60, 0.65, 0.75, 0.85]\n",
    "wt_freq_grid  = [0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80]\n",
    "\n",
    "best = {\n",
    "    \"score\": 1e18,\n",
    "    \"params\": None,\n",
    "    \"detail\": None\n",
    "}\n",
    "\n",
    "for wt_t in wt_total_grid:\n",
    "    for wt_f in wt_freq_grid:\n",
    "        for a_t in [\"mean\", \"median\"]:\n",
    "            for a_f in [\"mean\", \"median\"]:\n",
    "\n",
    "                pt, pf, ps = simulate(train, H, wt_t, wt_f, a_t, a_f)\n",
    "\n",
    "                mf = mape(valid[\"frequency\"], pf)\n",
    "                mt = mape(valid[\"total_claim\"], pt)\n",
    "                ms = mape(valid[\"severity\"], ps)\n",
    "                avg = float(np.nanmean([mf, mt, ms]))\n",
    "\n",
    "                if avg < best[\"score\"]:\n",
    "                    best[\"score\"] = avg\n",
    "                    best[\"params\"] = (wt_t, wt_f, a_t, a_f)\n",
    "                    best[\"detail\"] = (mf, mt, ms)\n",
    "\n",
    "# ==============================\n",
    "# RUN BEST + REPORT\n",
    "# ==============================\n",
    "wt_t, wt_f, a_t, a_f = best[\"params\"]\n",
    "pt, pf, ps = simulate(train, H, wt_t, wt_f, a_t, a_f)\n",
    "\n",
    "mf, mt, ms = best[\"detail\"]\n",
    "avg = best[\"score\"]\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(f\"Horizon months used : {H}\")\n",
    "print(\"Best Config:\")\n",
    "print(f\"  wt_total={wt_t} (ETS weight), anchor_total={a_t}\")\n",
    "print(f\"  wt_freq ={wt_f} (ETS weight), anchor_freq ={a_f}\")\n",
    "print(\"------------------------------\")\n",
    "print(\"STAGE 3 v17 MAPE Frequency :\", round(mf, 4))\n",
    "print(\"STAGE 3 v17 MAPE Total     :\", round(mt, 4))\n",
    "print(\"STAGE 3 v17 MAPE Severity  :\", round(ms, 4))\n",
    "print(\"Estimated Score            :\", round(avg, 4))\n",
    "print(\"==============================\")\n",
    "\n",
    "check = valid[[\"year_month\",\"frequency\",\"total_claim\",\"severity\"]].copy()\n",
    "check[\"pred_frequency\"] = pf\n",
    "check[\"pred_total\"] = pt\n",
    "check[\"pred_severity\"] = ps\n",
    "print(\"\\nPreview last horizon months:\")\n",
    "print(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bc3e39",
   "metadata": {
    "papermill": {
     "duration": 0.009303,
     "end_time": "2026-02-22T05:02:56.161379",
     "exception": false,
     "start_time": "2026-02-22T05:02:56.152076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TOTAL CLAIM OPTIMIZATION & VALIDATION, OPTUNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3613c485",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T05:02:56.185809Z",
     "iopub.status.busy": "2026-02-22T05:02:56.185427Z",
     "iopub.status.idle": "2026-02-22T05:06:16.680861Z",
     "shell.execute_reply": "2026-02-22T05:06:16.679939Z"
    },
    "papermill": {
     "duration": 200.513222,
     "end_time": "2026-02-22T05:06:16.683864",
     "exception": false,
     "start_time": "2026-02-22T05:02:56.170642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N months: 19 | Horizon H: 5 | Future MOY: [8, 9, 10, 11, 12]\n",
      "CV train_ends: [7, 8, 13, 14] | weights: [0.636, 0.277, 0.042, 0.045]\n",
      "  split te= 7 | valid months: ['2024-08', '2024-09', '2024-10', '2024-11', '2024-12']\n",
      "  split te= 8 | valid months: ['2024-09', '2024-10', '2024-11', '2024-12', '2025-01']\n",
      "  split te= 13 | valid months: ['2025-02', '2025-03', '2025-04', '2025-05', '2025-06']\n",
      "  split te= 14 | valid months: ['2025-03', '2025-04', '2025-05', '2025-06', '2025-07']\n",
      "Baseline CV %: 3.2595\n",
      "Baseline per-split (%):\n",
      "  te=7 | avg=1.588 | total=1.175 | freq=2.059 | sev=1.530\n",
      "  te=8 | avg=6.439 | total=9.479 | freq=4.847 | sev=4.992\n",
      "  te=13 | avg=6.772 | total=8.802 | freq=8.416 | sev=3.098\n",
      "  te=14 | avg=4.022 | total=2.204 | freq=5.524 | sev=4.338\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ea90e760fc416b9fba23bfd16e14bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Splits: [7, 8, 13, 14] | weights: [0.636, 0.277, 0.042, 0.045] | overlap: [1.0, 0.8, 0.0, 0.0]\n",
      "Best Params: {'k_anchor': 4, 'anchor_f': 'mean', 'anchor_t': 'median', 'w_ets_f': 0.4140902012641963, 'w_drift_f': 0.23427959162969844, 'w_ets_t': 0.1253855830888168, 'w_drift_t': 0.0557421860395439, 'drift_k_f': 5, 'drift_k_t': 5, 'season_scale_f': 1.007823739519456, 'season_scale_t': 0.9473922819889735, 'capF_low': 0.8025030597351724, 'capF_high': 1.4449152050189749, 'capT_low': 0.5583495487333925, 'capT_high': 1.7549317144514707}\n",
      "CV Best %  : 2.7153\n",
      "==============================\n",
      "Best per-split (%):\n",
      "  te=7 | avg=1.006 | total=0.853 | freq=1.516 | sev=0.651\n",
      "  te=8 | avg=5.992 | total=9.223 | freq=4.088 | sev=4.666\n",
      "  te=13 | avg=5.574 | total=6.910 | freq=7.950 | sev=1.862\n",
      "  te=14 | avg=4.018 | total=2.471 | freq=4.810 | sev=4.773\n",
      "\n",
      "STAGE 4 v30 â€” READY (global season prior)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 4 v30 â€” GLOBAL SEASON PRIOR (DESEASON LOG) + ETS + DRIFT + ANCHOR\n",
    "# Goal: turunin lagi (especially Frequency) dengan memasukkan month-of-year prior.\n",
    "#\n",
    "# IMPORTANT:\n",
    "# - season prior dihitung dari seluruh history (Jan 2024â€“Jul 2025).\n",
    "# - Ini membuat CV te=7/8 lebih optimistis (karena split train tidak punya Augâ€“Dec),\n",
    "#   tapi biasanya lebih \"Kaggle-useful\" untuk forecast Augâ€“Dec 2025.\n",
    "# ============================================================\n",
    "\n",
    "!pip install -q optuna statsmodels\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\n",
    "sample_sub = pd.read_csv(BASE_PATH + \"sample_submission.csv\")\n",
    "\n",
    "assert \"df\" in globals(), \"df belum ada. Jalankan Stage 1 dulu.\"\n",
    "assert \"year_month\" in df.columns, \"df['year_month'] belum ada.\"\n",
    "\n",
    "# ------------------------------\n",
    "# Horizon + future MOY\n",
    "# ------------------------------\n",
    "sample_sub[\"year\"]  = sample_sub[\"id\"].str.split(\"_\").str[0]\n",
    "sample_sub[\"month\"] = sample_sub[\"id\"].str.split(\"_\").str[1]\n",
    "sample_sub[\"month_key\"] = sample_sub[\"year\"] + \"-\" + sample_sub[\"month\"]\n",
    "\n",
    "future_periods = (\n",
    "    pd.PeriodIndex(sample_sub[\"month_key\"], freq=\"M\")\n",
    "      .unique()\n",
    "      .sort_values()\n",
    ")\n",
    "H0 = int(len(future_periods))\n",
    "future_moy = set([p.month for p in future_periods])\n",
    "\n",
    "# ------------------------------\n",
    "# Build monthly portfolio (complete)\n",
    "# ------------------------------\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(\n",
    "          frequency=(\"claim_id\",\"count\"),\n",
    "          total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\"),\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(\"year_month\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "if not isinstance(monthly.loc[0, \"year_month\"], pd.Period):\n",
    "    monthly[\"year_month\"] = pd.PeriodIndex(monthly[\"year_month\"], freq=\"M\")\n",
    "\n",
    "min_m = monthly[\"year_month\"].min()\n",
    "max_m = monthly[\"year_month\"].max()\n",
    "all_months = pd.period_range(min_m, max_m, freq=\"M\")\n",
    "\n",
    "monthly = (\n",
    "    monthly.set_index(\"year_month\")\n",
    "           .reindex(all_months, fill_value=0.0)\n",
    "           .rename_axis(\"year_month\")\n",
    "           .reset_index()\n",
    ")\n",
    "\n",
    "monthly[\"frequency\"]   = pd.to_numeric(monthly[\"frequency\"], errors=\"coerce\").fillna(0.0).clip(lower=1.0)\n",
    "monthly[\"total_claim\"] = pd.to_numeric(monthly[\"total_claim\"], errors=\"coerce\").fillna(0.0).clip(lower=1.0)\n",
    "monthly[\"severity\"]    = (monthly[\"total_claim\"] / monthly[\"frequency\"]).astype(float)\n",
    "monthly[\"month\"]       = monthly[\"year_month\"].dt.month\n",
    "monthly[\"t\"]           = np.arange(len(monthly), dtype=int)\n",
    "\n",
    "N = len(monthly)\n",
    "H = min(H0, max(1, N - 10))\n",
    "print(\"N months:\", N, \"| Horizon H:\", H, \"| Future MOY:\", sorted(list(future_moy)))\n",
    "\n",
    "# ------------------------------\n",
    "# CV splits (same pick, but weight Kaggle-like)\n",
    "# ------------------------------\n",
    "min_train = 7\n",
    "cands = []\n",
    "for te in range(min_train, N - H + 1):\n",
    "    valid = monthly.iloc[te:te+H]\n",
    "    overlap = sum([1 for m in valid[\"month\"].tolist() if m in future_moy]) / float(H)\n",
    "    recency = te / float(N)\n",
    "    score = 0.65*overlap + 0.35*recency\n",
    "    cands.append((score, overlap, recency, te))\n",
    "\n",
    "cands_sorted = sorted(cands, reverse=True)\n",
    "top_season = sorted(cands, key=lambda x: (x[1], x[0]), reverse=True)[:2]\n",
    "top_recent = sorted(cands, key=lambda x: x[2], reverse=True)[:2]\n",
    "picked = {x[3] for x in (top_season + top_recent)}\n",
    "train_ends = sorted(list(picked))\n",
    "\n",
    "# heavy weight to season-overlap splits\n",
    "OVERLAP_POWER = 4.0\n",
    "RECENCY_MIX = 0.10\n",
    "\n",
    "def overlap_ratio(te):\n",
    "    v = monthly.iloc[te:te+H]\n",
    "    return sum([1 for m in v[\"month\"].tolist() if m in future_moy]) / float(H)\n",
    "\n",
    "ov = np.array([overlap_ratio(te) for te in train_ends], float)\n",
    "rc = np.array([te/float(N) for te in train_ends], float)\n",
    "w_raw = (ov ** OVERLAP_POWER) + RECENCY_MIX * rc\n",
    "w_raw = np.maximum(w_raw, 1e-6)\n",
    "split_w = w_raw / w_raw.sum()\n",
    "\n",
    "print(\"CV train_ends:\", train_ends, \"| weights:\", split_w.round(3).tolist())\n",
    "for te in train_ends:\n",
    "    v = monthly.iloc[te:te+H][[\"year_month\",\"month\"]]\n",
    "    print(\"  split te=\", te, \"| valid months:\", v[\"year_month\"].astype(str).tolist())\n",
    "\n",
    "# ------------------------------\n",
    "# Metrics\n",
    "# ------------------------------\n",
    "def mape_frac(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred) & (y_true != 0)\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return float(np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])))\n",
    "\n",
    "def avg_mape(yF, pF, yT, pT):\n",
    "    yF = np.asarray(yF, float); pF = np.asarray(pF, float)\n",
    "    yT = np.asarray(yT, float); pT = np.asarray(pT, float)\n",
    "    yS = yT / np.clip(yF, 1.0, np.inf)\n",
    "    pS = pT / np.clip(pF, 1.0, np.inf)\n",
    "    mf = mape_frac(yF, pF)\n",
    "    mt = mape_frac(yT, pT)\n",
    "    ms = mape_frac(yS, pS)\n",
    "    return float(np.nanmean([mf, mt, ms])), mt, mf, ms\n",
    "\n",
    "# ------------------------------\n",
    "# GLOBAL season prior (log space)\n",
    "# ------------------------------\n",
    "def build_season_log(series_level, months, clip_abs=0.25):\n",
    "    # additive season in log1p space: s[m] = median(log1p(x_m)) - median(log1p(x_all))\n",
    "    y = np.log1p(np.asarray(series_level, float).clip(min=0))\n",
    "    m = np.asarray(months, int)\n",
    "    base = float(np.median(y))\n",
    "    s = np.zeros(13, dtype=float)\n",
    "    for mm in range(1, 13):\n",
    "        vals = y[m == mm]\n",
    "        if len(vals) > 0:\n",
    "            s[mm] = float(np.median(vals) - base)\n",
    "        else:\n",
    "            s[mm] = 0.0\n",
    "    s = np.clip(s, -float(clip_abs), float(clip_abs))\n",
    "    return s\n",
    "\n",
    "SEASON_LOG_F_FULL = build_season_log(monthly[\"frequency\"].values, monthly[\"month\"].values, clip_abs=0.30)\n",
    "SEASON_LOG_T_FULL = build_season_log(monthly[\"total_claim\"].values, monthly[\"month\"].values, clip_abs=0.35)\n",
    "\n",
    "# ------------------------------\n",
    "# Helpers\n",
    "# ------------------------------\n",
    "def ets_1step_on_log(series_log):\n",
    "    y = np.asarray(series_log, float)\n",
    "    if len(y) < 4:\n",
    "        return float(y[-1])\n",
    "    trend = \"add\" if len(y) >= 10 else None\n",
    "    damped = True if trend is not None else False\n",
    "    try:\n",
    "        m = ExponentialSmoothing(\n",
    "            y,\n",
    "            trend=trend,\n",
    "            damped_trend=damped if trend is not None else False,\n",
    "            seasonal=None,\n",
    "            initialization_method=\"estimated\"\n",
    "        ).fit()\n",
    "        return float(m.forecast(1)[0])\n",
    "    except:\n",
    "        return float(y[-1])\n",
    "\n",
    "def drift_on_log(series_log, k=3):\n",
    "    y = np.asarray(series_log, float)\n",
    "    if len(y) < (k + 2):\n",
    "        return float(y[-1])\n",
    "    deltas = np.diff(y[-(k+1):])\n",
    "    return float(y[-1] + np.mean(deltas))\n",
    "\n",
    "def anchor_on_log(series_log, k=3, how=\"mean\"):\n",
    "    tail = np.asarray(series_log[-k:], float)\n",
    "    return float(np.median(tail)) if how == \"median\" else float(np.mean(tail))\n",
    "\n",
    "# ------------------------------\n",
    "# Simulator (deseason log -> forecast -> reseason)\n",
    "# ------------------------------\n",
    "def simulate(train_df, H, P):\n",
    "    sim = train_df.copy().reset_index(drop=True)\n",
    "\n",
    "    # season strengths\n",
    "    sf = float(P[\"season_scale_f\"])\n",
    "    st = float(P[\"season_scale_t\"])\n",
    "\n",
    "    # log1p series\n",
    "    sim[\"logF\"] = np.log1p(sim[\"frequency\"].values)\n",
    "    sim[\"logT\"] = np.log1p(sim[\"total_claim\"].values)\n",
    "\n",
    "    pF, pT = [], []\n",
    "\n",
    "    for _ in range(H):\n",
    "        next_period = sim[\"year_month\"].iloc[-1] + 1\n",
    "        m_next = int(next_period.month)\n",
    "\n",
    "        # deseason logs\n",
    "        logF_ds = sim[\"logF\"].values - sf * SEASON_LOG_F_FULL[sim[\"month\"].values]\n",
    "        logT_ds = sim[\"logT\"].values - st * SEASON_LOG_T_FULL[sim[\"month\"].values]\n",
    "\n",
    "        # forecast in deseason space\n",
    "        f_ets = ets_1step_on_log(logF_ds)\n",
    "        f_drift = drift_on_log(logF_ds, k=int(P[\"drift_k_f\"]))\n",
    "        f_an = anchor_on_log(logF_ds, k=int(P[\"k_anchor\"]), how=P[\"anchor_f\"])\n",
    "\n",
    "        wE = float(P[\"w_ets_f\"]); wD = float(P[\"w_drift_f\"])\n",
    "        f_ds_pred = wE*f_ets + wD*f_drift + (1.0-wE-wD)*f_an\n",
    "\n",
    "        t_ets = ets_1step_on_log(logT_ds)\n",
    "        t_drift = drift_on_log(logT_ds, k=int(P[\"drift_k_t\"]))\n",
    "        t_an = anchor_on_log(logT_ds, k=int(P[\"k_anchor\"]), how=P[\"anchor_t\"])\n",
    "\n",
    "        wE = float(P[\"w_ets_t\"]); wD = float(P[\"w_drift_t\"])\n",
    "        t_ds_pred = wE*t_ets + wD*t_drift + (1.0-wE-wD)*t_an\n",
    "\n",
    "        # reseason\n",
    "        logF_pred = float(f_ds_pred + sf * SEASON_LOG_F_FULL[m_next])\n",
    "        logT_pred = float(t_ds_pred + st * SEASON_LOG_T_FULL[m_next])\n",
    "\n",
    "        F_pred = float(np.expm1(logF_pred))\n",
    "        T_pred = float(np.expm1(logT_pred))\n",
    "\n",
    "        # clamp vs anchor in LEVEL space (important)\n",
    "        # anchor level for next month\n",
    "        logF_an_next = float(f_an + sf * SEASON_LOG_F_FULL[m_next])\n",
    "        logT_an_next = float(t_an + st * SEASON_LOG_T_FULL[m_next])\n",
    "        F_an = float(np.expm1(logF_an_next))\n",
    "        T_an = float(np.expm1(logT_an_next))\n",
    "\n",
    "        F_pred = float(np.clip(F_pred, max(1.0, F_an*float(P[\"capF_low\"])), F_an*float(P[\"capF_high\"])))\n",
    "        T_pred = float(np.clip(T_pred, max(1.0, T_an*float(P[\"capT_low\"])), T_an*float(P[\"capT_high\"])))\n",
    "\n",
    "        pF.append(F_pred)\n",
    "        pT.append(T_pred)\n",
    "\n",
    "        sim = pd.concat([sim, pd.DataFrame([{\n",
    "            \"year_month\": next_period,\n",
    "            \"month\": m_next,\n",
    "            \"t\": int(sim[\"t\"].iloc[-1] + 1),\n",
    "            \"frequency\": F_pred,\n",
    "            \"total_claim\": T_pred,\n",
    "            \"severity\": float(T_pred / max(1.0, F_pred)),\n",
    "            \"logF\": float(np.log1p(F_pred)),\n",
    "            \"logT\": float(np.log1p(T_pred)),\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    return np.array(pF, float), np.array(pT, float)\n",
    "\n",
    "# ------------------------------\n",
    "# CV runner\n",
    "# ------------------------------\n",
    "def run_cv(P):\n",
    "    s = 0.0\n",
    "    rows = []\n",
    "    for te, w in zip(train_ends, split_w):\n",
    "        tr = monthly.iloc[:te].copy().reset_index(drop=True)\n",
    "        va = monthly.iloc[te:te+H].copy().reset_index(drop=True)\n",
    "\n",
    "        pF, pT = simulate(tr, H, P)\n",
    "        sc, mt, mf, ms = avg_mape(va[\"frequency\"].values, pF, va[\"total_claim\"].values, pT)\n",
    "        s += w*sc\n",
    "        rows.append((te, sc, mt, mf, ms))\n",
    "    return float(s), rows\n",
    "\n",
    "# ------------------------------\n",
    "# Baseline params\n",
    "# ------------------------------\n",
    "P0 = dict(\n",
    "    k_anchor=5,\n",
    "    anchor_f=\"mean\",\n",
    "    anchor_t=\"median\",\n",
    "\n",
    "    w_ets_f=0.70, w_drift_f=0.20,\n",
    "    w_ets_t=0.25, w_drift_t=0.10,\n",
    "\n",
    "    drift_k_f=3,\n",
    "    drift_k_t=4,\n",
    "\n",
    "    # season prior strength (0..1)\n",
    "    season_scale_f=1.0,\n",
    "    season_scale_t=0.8,\n",
    "\n",
    "    # clamps\n",
    "    capF_low=0.70, capF_high=1.45,\n",
    "    capT_low=0.70, capT_high=1.40,\n",
    ")\n",
    "\n",
    "base_score, base_rows = run_cv(P0)\n",
    "print(\"Baseline CV %:\", round(base_score*100, 4))\n",
    "print(\"Baseline per-split (%):\")\n",
    "for te, sc, mt, mf, ms in base_rows:\n",
    "    print(f\"  te={te} | avg={sc*100:.3f} | total={mt*100:.3f} | freq={mf*100:.3f} | sev={ms*100:.3f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Optuna tuning\n",
    "# ------------------------------\n",
    "def objective(trial):\n",
    "    P = dict(\n",
    "        k_anchor=trial.suggest_int(\"k_anchor\", 3, 8),\n",
    "        anchor_f=trial.suggest_categorical(\"anchor_f\", [\"mean\",\"median\"]),\n",
    "        anchor_t=trial.suggest_categorical(\"anchor_t\", [\"mean\",\"median\"]),\n",
    "\n",
    "        w_ets_f=trial.suggest_float(\"w_ets_f\", 0.30, 0.90),\n",
    "        w_drift_f=trial.suggest_float(\"w_drift_f\", 0.00, 0.40),\n",
    "        w_ets_t=trial.suggest_float(\"w_ets_t\", 0.10, 0.85),\n",
    "        w_drift_t=trial.suggest_float(\"w_drift_t\", 0.00, 0.35),\n",
    "\n",
    "        drift_k_f=trial.suggest_int(\"drift_k_f\", 2, 5),\n",
    "        drift_k_t=trial.suggest_int(\"drift_k_t\", 2, 6),\n",
    "\n",
    "        season_scale_f=trial.suggest_float(\"season_scale_f\", 0.0, 1.2),\n",
    "        season_scale_t=trial.suggest_float(\"season_scale_t\", 0.0, 1.2),\n",
    "\n",
    "        capF_low=trial.suggest_float(\"capF_low\", 0.55, 0.90),\n",
    "        capF_high=trial.suggest_float(\"capF_high\", 1.05, 1.80),\n",
    "        capT_low=trial.suggest_float(\"capT_low\", 0.55, 0.90),\n",
    "        capT_high=trial.suggest_float(\"capT_high\", 1.05, 1.80),\n",
    "    )\n",
    "\n",
    "    # convex constraints (anchor weight >= 0.05)\n",
    "    if (P[\"w_ets_f\"] + P[\"w_drift_f\"]) > 0.95:\n",
    "        return 1e9\n",
    "    if (P[\"w_ets_t\"] + P[\"w_drift_t\"]) > 0.95:\n",
    "        return 1e9\n",
    "\n",
    "    sc, _ = run_cv(P)\n",
    "    return sc\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "study.enqueue_trial(P0)\n",
    "\n",
    "N_TRIALS = 200\n",
    "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "bestP = study.best_params\n",
    "best_score, best_rows = run_cv(bestP)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"Splits:\", train_ends, \"| weights:\", split_w.round(3).tolist(), \"| overlap:\", ov.round(3).tolist())\n",
    "print(\"Best Params:\", bestP)\n",
    "print(\"CV Best %  :\", round(best_score*100, 4))\n",
    "print(\"==============================\")\n",
    "print(\"Best per-split (%):\")\n",
    "for te, sc, mt, mf, ms in best_rows:\n",
    "    print(f\"  te={te} | avg={sc*100:.3f} | total={mt*100:.3f} | freq={mf*100:.3f} | sev={ms*100:.3f}\")\n",
    "\n",
    "print(\"\\nSTAGE 4 v30 â€” READY (global season prior)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abdbc85",
   "metadata": {
    "papermill": {
     "duration": 0.009686,
     "end_time": "2026-02-22T05:06:16.702956",
     "exception": false,
     "start_time": "2026-02-22T05:06:16.693270",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TEST PREDICTION & KAGGLE SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8be4b9af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T05:06:16.726293Z",
     "iopub.status.busy": "2026-02-22T05:06:16.725863Z",
     "iopub.status.idle": "2026-02-22T05:20:39.683193Z",
     "shell.execute_reply": "2026-02-22T05:20:39.682289Z"
    },
    "papermill": {
     "duration": 862.97181,
     "end_time": "2026-02-22T05:20:39.686071",
     "exception": false,
     "start_time": "2026-02-22T05:06:16.714261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N months: 19 | H: 5 | Future MOY: [8, 9, 10, 11, 12]\n",
      "Blend train_ends: [7, 8, 13, 14] | weights: [0.636, 0.277, 0.042, 0.045]\n",
      "\n",
      "Best blend (2D):\n",
      "  wF_best = 1.0 (Stage4 weight for Frequency)\n",
      "  wT_best = 1.0 (Stage4 weight for Total)\n",
      "  CV mean = 2.7148 % | worst = 5.9924 %\n",
      "\n",
      "Top 10 by mean_avg:\n",
      "       wF    wT  mean_avg  worst_avg   split_0   split_1   split_2   split_3\n",
      "440  1.00  1.00  0.027148   0.059924  0.010063  0.059924  0.055742  0.040181\n",
      "419  0.95  1.00  0.027608   0.060917  0.010501  0.060917  0.053238  0.040423\n",
      "439  1.00  0.95  0.027650   0.060254  0.010683  0.060254  0.056984  0.039366\n",
      "418  0.95  0.95  0.028299   0.060834  0.011568  0.060834  0.054961  0.039608\n",
      "398  0.90  1.00  0.029244   0.062815  0.012304  0.062815  0.052115  0.040664\n",
      "417  0.95  0.90  0.029478   0.061349  0.013117  0.061349  0.057053  0.038792\n",
      "438  1.00  0.90  0.029507   0.061363  0.013037  0.061363  0.059136  0.038551\n",
      "397  0.90  0.95  0.029595   0.062734  0.012836  0.062734  0.053834  0.039848\n",
      "396  0.90  0.90  0.030501   0.062652  0.014240  0.062652  0.055554  0.039033\n",
      "416  0.95  0.85  0.031271   0.062458  0.015368  0.062458  0.059198  0.037987\n",
      "NaN in submission: 0\n",
      "\n",
      "Preview future predictions:\n",
      "    period   pred_freq    pred_total      pred_sev\n",
      "0  2025-08  223.214649  1.323452e+10  5.929055e+07\n",
      "1  2025-09  204.182668  1.211557e+10  5.933691e+07\n",
      "2  2025-10  270.301522  1.244697e+10  4.604846e+07\n",
      "3  2025-11  265.566038  1.339265e+10  5.043058e+07\n",
      "4  2025-12  232.367450  1.179187e+10  5.074664e+07\n",
      "\n",
      "Saved: submission.csv\n",
      "                         id         value\n",
      "0   2025_08_Claim_Frequency  2.232146e+02\n",
      "1    2025_08_Claim_Severity  5.929055e+07\n",
      "2       2025_08_Total_Claim  1.323452e+10\n",
      "3   2025_09_Claim_Frequency  2.041827e+02\n",
      "4    2025_09_Claim_Severity  5.933691e+07\n",
      "5       2025_09_Total_Claim  1.211557e+10\n",
      "6   2025_10_Claim_Frequency  2.703015e+02\n",
      "7    2025_10_Claim_Severity  4.604846e+07\n",
      "8       2025_10_Total_Claim  1.244697e+10\n",
      "9   2025_11_Claim_Frequency  2.655660e+02\n",
      "10   2025_11_Claim_Severity  5.043058e+07\n",
      "11      2025_11_Total_Claim  1.339265e+10\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 5 v31 â€” FINAL SUBMISSION (Stage3 + Stage4 v30) 2D BLEND (wF, wT)\n",
    "# - Stage3: ETS log1p (freq & total) + anchor shrink\n",
    "# - Stage4 v30: GLOBAL SEASON PRIOR (deseason log) + ETS + drift + anchor + clamp\n",
    "# - Blend separately:\n",
    "#     F = (1-wF)*F3 + wF*F4\n",
    "#     T = (1-wT)*T3 + wT*T4\n",
    "#   Severity = T/F\n",
    "# - CV splits & weights MATCH Stage4 v30 (Kaggle-like): [7,8,13,14] weights [0.636,0.277,0.042,0.045]\n",
    "# ============================================================\n",
    "\n",
    "!pip install -q statsmodels\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\n",
    "sample_sub = pd.read_csv(BASE_PATH + \"sample_submission.csv\")\n",
    "\n",
    "assert \"df\" in globals(), \"df belum ada. Jalankan Stage 1 dulu.\"\n",
    "assert \"year_month\" in df.columns, \"df['year_month'] belum ada.\"\n",
    "\n",
    "# ------------------------------\n",
    "# Horizon + future periods\n",
    "# ------------------------------\n",
    "sample_sub[\"year\"]  = sample_sub[\"id\"].str.split(\"_\").str[0]\n",
    "sample_sub[\"month\"] = sample_sub[\"id\"].str.split(\"_\").str[1]\n",
    "sample_sub[\"month_key\"] = sample_sub[\"year\"] + \"-\" + sample_sub[\"month\"]\n",
    "\n",
    "future_periods = (\n",
    "    pd.PeriodIndex(sample_sub[\"month_key\"], freq=\"M\")\n",
    "      .unique()\n",
    "      .sort_values()\n",
    ")\n",
    "H = int(len(future_periods))\n",
    "future_moy = set([p.month for p in future_periods])\n",
    "\n",
    "# ------------------------------\n",
    "# Build monthly portfolio (complete range)\n",
    "# ------------------------------\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(\n",
    "          frequency=(\"claim_id\",\"count\"),\n",
    "          total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\"),\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(\"year_month\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "if len(monthly) == 0:\n",
    "    raise ValueError(\"monthly kosong. Cek df/year_month.\")\n",
    "\n",
    "if not isinstance(monthly.loc[0, \"year_month\"], pd.Period):\n",
    "    monthly[\"year_month\"] = pd.PeriodIndex(monthly[\"year_month\"], freq=\"M\")\n",
    "\n",
    "min_m = monthly[\"year_month\"].min()\n",
    "max_m = monthly[\"year_month\"].max()\n",
    "all_months = pd.period_range(min_m, max_m, freq=\"M\")\n",
    "\n",
    "monthly = (\n",
    "    monthly.set_index(\"year_month\")\n",
    "           .reindex(all_months, fill_value=0.0)\n",
    "           .rename_axis(\"year_month\")\n",
    "           .reset_index()\n",
    ")\n",
    "monthly[\"frequency\"]   = pd.to_numeric(monthly[\"frequency\"], errors=\"coerce\").fillna(0.0).clip(lower=1.0)\n",
    "monthly[\"total_claim\"] = pd.to_numeric(monthly[\"total_claim\"], errors=\"coerce\").fillna(0.0).clip(lower=1.0)\n",
    "monthly[\"severity\"]    = (monthly[\"total_claim\"] / monthly[\"frequency\"]).astype(float)\n",
    "monthly[\"month\"]       = monthly[\"year_month\"].dt.month\n",
    "monthly[\"t\"]           = np.arange(len(monthly), dtype=int)\n",
    "\n",
    "N = len(monthly)\n",
    "H = min(H, max(1, N - 10))\n",
    "print(\"N months:\", N, \"| H:\", H, \"| Future MOY:\", sorted(list(future_moy)))\n",
    "\n",
    "# ============================================================\n",
    "# Metrics\n",
    "# ============================================================\n",
    "def mape_frac(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred) & (y_true != 0)\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return float(np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])))\n",
    "\n",
    "def avg_mape(yF, pF, yT, pT):\n",
    "    yF = np.asarray(yF, float); pF = np.asarray(pF, float)\n",
    "    yT = np.asarray(yT, float); pT = np.asarray(pT, float)\n",
    "    yS = yT / np.clip(yF, 1.0, np.inf)\n",
    "    pS = pT / np.clip(pF, 1.0, np.inf)\n",
    "    mf = mape_frac(yF, pF)\n",
    "    mt = mape_frac(yT, pT)\n",
    "    ms = mape_frac(yS, pS)\n",
    "    return float(np.nanmean([mf, mt, ms])), mt, mf, ms\n",
    "\n",
    "# ============================================================\n",
    "# Stage 3 v17 (your fixed)\n",
    "# ============================================================\n",
    "S3 = dict(\n",
    "    wt_total=0.85, anchor_total=\"median\",\n",
    "    wt_freq=0.20,  anchor_freq=\"mean\",\n",
    "    k_anchor=3\n",
    ")\n",
    "\n",
    "def anchor_level_series(x: pd.Series, k: int, how: str):\n",
    "    tail = np.asarray(x.tail(k), dtype=float)\n",
    "    return float(np.median(tail)) if how == \"median\" else float(np.mean(tail))\n",
    "\n",
    "def ets_1step_log1p_series(level_series: pd.Series, trend=\"add\", damped=True):\n",
    "    y = np.log1p(level_series.astype(float).clip(lower=1e-12))\n",
    "    if len(y) < 4:\n",
    "        return float(np.expm1(y.iloc[-1]))\n",
    "    if trend is not None and len(y) < 10:\n",
    "        trend = None\n",
    "        damped = False\n",
    "    try:\n",
    "        m = ExponentialSmoothing(\n",
    "            y, trend=trend,\n",
    "            damped_trend=(damped if trend is not None else False),\n",
    "            seasonal=None\n",
    "        ).fit()\n",
    "        return float(np.expm1(m.forecast(1).iloc[0]))\n",
    "    except:\n",
    "        return float(level_series.iloc[-1])\n",
    "\n",
    "def simulate_stage3(train_df: pd.DataFrame, H: int):\n",
    "    sim = train_df.copy().reset_index(drop=True)\n",
    "    pF, pT = [], []\n",
    "    for _ in range(H):\n",
    "        k = int(S3[\"k_anchor\"])\n",
    "\n",
    "        tot_fc = ets_1step_log1p_series(sim[\"total_claim\"], trend=\"add\", damped=True)\n",
    "        tot_anchor = anchor_level_series(sim[\"total_claim\"], k, S3[\"anchor_total\"])\n",
    "        tot_pred = float(S3[\"wt_total\"])*tot_fc + (1-float(S3[\"wt_total\"]))*tot_anchor\n",
    "        tot_pred = max(1.0, tot_pred)\n",
    "\n",
    "        fre_fc = ets_1step_log1p_series(sim[\"frequency\"], trend=\"add\", damped=True)\n",
    "        fre_anchor = anchor_level_series(sim[\"frequency\"], k, S3[\"anchor_freq\"])\n",
    "        fre_pred = float(S3[\"wt_freq\"])*fre_fc + (1-float(S3[\"wt_freq\"]))*fre_anchor\n",
    "        fre_pred = max(1.0, fre_pred)\n",
    "\n",
    "        pF.append(fre_pred)\n",
    "        pT.append(tot_pred)\n",
    "\n",
    "        sim = pd.concat([sim, pd.DataFrame([{\n",
    "            \"year_month\": sim[\"year_month\"].iloc[-1] + 1,\n",
    "            \"month\": int((sim[\"year_month\"].iloc[-1] + 1).month),\n",
    "            \"t\": int(sim[\"t\"].iloc[-1] + 1),\n",
    "            \"frequency\": fre_pred,\n",
    "            \"total_claim\": tot_pred,\n",
    "            \"severity\": float(tot_pred / fre_pred),\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    return np.array(pF, float), np.array(pT, float)\n",
    "\n",
    "# ============================================================\n",
    "# Stage 4 v30 BEST params (PASTE FROM YOUR OUTPUT)\n",
    "# ============================================================\n",
    "BEST4 = {\n",
    "    'k_anchor': 4, 'anchor_f': 'mean', 'anchor_t': 'median',\n",
    "    'w_ets_f': 0.4140902012641963, 'w_drift_f': 0.23427959162969844,\n",
    "    'w_ets_t': 0.1253855830888168, 'w_drift_t': 0.0557421860395439,\n",
    "    'drift_k_f': 5, 'drift_k_t': 5,\n",
    "    'season_scale_f': 1.007823739519456, 'season_scale_t': 0.9473922819889735,\n",
    "    'capF_low': 0.8025030597351724, 'capF_high': 1.4449152050189749,\n",
    "    'capT_low': 0.5583495487333925, 'capT_high': 1.7549317144514707\n",
    "}\n",
    "\n",
    "def build_season_log(series_level, months, clip_abs=0.30):\n",
    "    y = np.log1p(np.asarray(series_level, float).clip(min=0))\n",
    "    m = np.asarray(months, int)\n",
    "    base = float(np.median(y))\n",
    "    s = np.zeros(13, dtype=float)\n",
    "    for mm in range(1, 13):\n",
    "        vals = y[m == mm]\n",
    "        s[mm] = float(np.median(vals) - base) if len(vals) else 0.0\n",
    "    return np.clip(s, -float(clip_abs), float(clip_abs))\n",
    "\n",
    "# season prior from FULL history (exact Stage4 v30 behavior)\n",
    "SEASON_LOG_F_FULL = build_season_log(monthly[\"frequency\"].values, monthly[\"month\"].values, clip_abs=0.30)\n",
    "SEASON_LOG_T_FULL = build_season_log(monthly[\"total_claim\"].values, monthly[\"month\"].values, clip_abs=0.35)\n",
    "\n",
    "def ets_1step_on_log(series_log):\n",
    "    y = np.asarray(series_log, float)\n",
    "    if len(y) < 4:\n",
    "        return float(y[-1])\n",
    "    trend = \"add\" if len(y) >= 10 else None\n",
    "    damped = True if trend is not None else False\n",
    "    try:\n",
    "        m = ExponentialSmoothing(\n",
    "            y,\n",
    "            trend=trend,\n",
    "            damped_trend=damped if trend is not None else False,\n",
    "            seasonal=None,\n",
    "            initialization_method=\"estimated\"\n",
    "        ).fit()\n",
    "        return float(m.forecast(1)[0])\n",
    "    except:\n",
    "        return float(y[-1])\n",
    "\n",
    "def drift_on_log(series_log, k=3):\n",
    "    y = np.asarray(series_log, float)\n",
    "    if len(y) < (k + 2):\n",
    "        return float(y[-1])\n",
    "    deltas = np.diff(y[-(k+1):])\n",
    "    return float(y[-1] + np.mean(deltas))\n",
    "\n",
    "def anchor_on_log(series_log, k=3, how=\"mean\"):\n",
    "    tail = np.asarray(series_log[-k:], float)\n",
    "    return float(np.median(tail)) if how == \"median\" else float(np.mean(tail))\n",
    "\n",
    "def simulate_stage4v30(train_df: pd.DataFrame, H: int):\n",
    "    P = BEST4\n",
    "    sim = train_df.copy().reset_index(drop=True)\n",
    "    sim[\"logF\"] = np.log1p(sim[\"frequency\"].values)\n",
    "    sim[\"logT\"] = np.log1p(sim[\"total_claim\"].values)\n",
    "\n",
    "    sf = float(P[\"season_scale_f\"])\n",
    "    st = float(P[\"season_scale_t\"])\n",
    "\n",
    "    pF, pT = [], []\n",
    "\n",
    "    for _ in range(H):\n",
    "        next_period = sim[\"year_month\"].iloc[-1] + 1\n",
    "        m_next = int(next_period.month)\n",
    "\n",
    "        logF_ds = sim[\"logF\"].values - sf * SEASON_LOG_F_FULL[sim[\"month\"].values]\n",
    "        logT_ds = sim[\"logT\"].values - st * SEASON_LOG_T_FULL[sim[\"month\"].values]\n",
    "\n",
    "        f_ets = ets_1step_on_log(logF_ds)\n",
    "        f_drift = drift_on_log(logF_ds, k=int(P[\"drift_k_f\"]))\n",
    "        f_an = anchor_on_log(logF_ds, k=int(P[\"k_anchor\"]), how=P[\"anchor_f\"])\n",
    "        f_ds_pred = float(P[\"w_ets_f\"])*f_ets + float(P[\"w_drift_f\"])*f_drift + (1.0-float(P[\"w_ets_f\"])-float(P[\"w_drift_f\"]))*f_an\n",
    "\n",
    "        t_ets = ets_1step_on_log(logT_ds)\n",
    "        t_drift = drift_on_log(logT_ds, k=int(P[\"drift_k_t\"]))\n",
    "        t_an = anchor_on_log(logT_ds, k=int(P[\"k_anchor\"]), how=P[\"anchor_t\"])\n",
    "        t_ds_pred = float(P[\"w_ets_t\"])*t_ets + float(P[\"w_drift_t\"])*t_drift + (1.0-float(P[\"w_ets_t\"])-float(P[\"w_drift_t\"]))*t_an\n",
    "\n",
    "        logF_pred = float(f_ds_pred + sf * SEASON_LOG_F_FULL[m_next])\n",
    "        logT_pred = float(t_ds_pred + st * SEASON_LOG_T_FULL[m_next])\n",
    "\n",
    "        F_pred = float(np.expm1(logF_pred))\n",
    "        T_pred = float(np.expm1(logT_pred))\n",
    "\n",
    "        # clamp vs anchor (level)\n",
    "        logF_an_next = float(f_an + sf * SEASON_LOG_F_FULL[m_next])\n",
    "        logT_an_next = float(t_an + st * SEASON_LOG_T_FULL[m_next])\n",
    "        F_an = float(np.expm1(logF_an_next))\n",
    "        T_an = float(np.expm1(logT_an_next))\n",
    "\n",
    "        F_pred = float(np.clip(F_pred, max(1.0, F_an*float(P[\"capF_low\"])), F_an*float(P[\"capF_high\"])))\n",
    "        T_pred = float(np.clip(T_pred, max(1.0, T_an*float(P[\"capT_low\"])), T_an*float(P[\"capT_high\"])))\n",
    "\n",
    "        pF.append(F_pred)\n",
    "        pT.append(T_pred)\n",
    "\n",
    "        sim = pd.concat([sim, pd.DataFrame([{\n",
    "            \"year_month\": next_period,\n",
    "            \"month\": m_next,\n",
    "            \"t\": int(sim[\"t\"].iloc[-1] + 1),\n",
    "            \"frequency\": F_pred,\n",
    "            \"total_claim\": T_pred,\n",
    "            \"severity\": float(T_pred / max(1.0, F_pred)),\n",
    "            \"logF\": float(np.log1p(F_pred)),\n",
    "            \"logT\": float(np.log1p(T_pred)),\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    return np.array(pF, float), np.array(pT, float)\n",
    "\n",
    "# ============================================================\n",
    "# CV splits & weights MUST match Stage4 v30 run\n",
    "# ============================================================\n",
    "train_ends = [7, 8, 13, 14]\n",
    "split_w = np.array([0.636, 0.277, 0.042, 0.045], float)\n",
    "split_w = split_w / split_w.sum()\n",
    "print(\"Blend train_ends:\", train_ends, \"| weights:\", split_w.round(3).tolist())\n",
    "\n",
    "# ============================================================\n",
    "# 2D blend search: choose wF and wT separately\n",
    "# ============================================================\n",
    "grid = np.linspace(0.0, 1.0, 21)  # step 0.05\n",
    "best = None\n",
    "rows = []\n",
    "\n",
    "for wF in grid:\n",
    "    for wT in grid:\n",
    "        split_scores = []\n",
    "        for te, w in zip(train_ends, split_w):\n",
    "            tr = monthly.iloc[:te].copy().reset_index(drop=True)\n",
    "            va = monthly.iloc[te:te+H].copy().reset_index(drop=True)\n",
    "\n",
    "            F3, T3 = simulate_stage3(tr, H)\n",
    "            F4, T4 = simulate_stage4v30(tr, H)\n",
    "\n",
    "            F = (1-wF)*F3 + wF*F4\n",
    "            T = (1-wT)*T3 + wT*T4\n",
    "\n",
    "            sc, mt, mf, ms = avg_mape(va[\"frequency\"].values, F, va[\"total_claim\"].values, T)\n",
    "            split_scores.append(sc)\n",
    "\n",
    "        meanw = float(np.sum(split_w * np.array(split_scores)))\n",
    "        worst = float(np.max(split_scores))\n",
    "        rows.append([wF, wT, meanw, worst] + split_scores)\n",
    "\n",
    "        cand = (meanw, worst, wF, wT)\n",
    "        if (best is None) or (cand < best):\n",
    "            best = cand\n",
    "\n",
    "best_mean, best_worst, wF_best, wT_best = best\n",
    "print(\"\\nBest blend (2D):\")\n",
    "print(\"  wF_best =\", wF_best, \"(Stage4 weight for Frequency)\")\n",
    "print(\"  wT_best =\", wT_best, \"(Stage4 weight for Total)\")\n",
    "print(\"  CV mean =\", round(best_mean*100, 4), \"% | worst =\", round(best_worst*100, 4), \"%\")\n",
    "\n",
    "dfb = pd.DataFrame(rows, columns=[\"wF\",\"wT\",\"mean_avg\",\"worst_avg\"] + [f\"split_{i}\" for i in range(len(train_ends))])\n",
    "print(\"\\nTop 10 by mean_avg:\")\n",
    "print(dfb.sort_values([\"mean_avg\",\"worst_avg\"]).head(10))\n",
    "\n",
    "# ============================================================\n",
    "# FINAL forecast on FULL data\n",
    "# ============================================================\n",
    "F3, T3 = simulate_stage3(monthly, H)\n",
    "F4, T4 = simulate_stage4v30(monthly, H)\n",
    "\n",
    "F = (1-wF_best)*F3 + wF_best*F4\n",
    "T = (1-wT_best)*T3 + wT_best*T4\n",
    "S = T / np.clip(F, 1.0, np.inf)\n",
    "\n",
    "pred_map = {}\n",
    "preview = []\n",
    "for i, period in enumerate(future_periods):\n",
    "    key = f\"{period.year}_{str(period.month).zfill(2)}\"\n",
    "    pred_map[f\"{key}_Claim_Frequency\"] = float(F[i])\n",
    "    pred_map[f\"{key}_Total_Claim\"]     = float(T[i])\n",
    "    pred_map[f\"{key}_Claim_Severity\"]  = float(S[i])\n",
    "    preview.append([str(period), float(F[i]), float(T[i]), float(S[i])])\n",
    "\n",
    "sub = sample_sub.copy()\n",
    "sub[\"value\"] = sub[\"id\"].map(pred_map)\n",
    "missing = int(sub[\"value\"].isna().sum())\n",
    "print(\"NaN in submission:\", missing)\n",
    "assert missing == 0, \"Ada id belum terisi. Cek key format.\"\n",
    "\n",
    "sub = sub[[\"id\",\"value\"]]\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"\\nPreview future predictions:\")\n",
    "print(pd.DataFrame(preview, columns=[\"period\",\"pred_freq\",\"pred_total\",\"pred_sev\"]))\n",
    "print(\"\\nSaved: submission.csv\")\n",
    "print(sub.head(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "972f8e51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T05:20:39.708082Z",
     "iopub.status.busy": "2026-02-22T05:20:39.707687Z",
     "iopub.status.idle": "2026-02-22T05:20:39.718527Z",
     "shell.execute_reply": "2026-02-22T05:20:39.716449Z"
    },
    "papermill": {
     "duration": 0.02553,
     "end_time": "2026-02-22T05:20:39.721472",
     "exception": false,
     "start_time": "2026-02-22T05:20:39.695942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         id         value\n",
      "0   2025_08_Claim_Frequency  2.232146e+02\n",
      "1    2025_08_Claim_Severity  5.929055e+07\n",
      "2       2025_08_Total_Claim  1.323452e+10\n",
      "3   2025_09_Claim_Frequency  2.041827e+02\n",
      "4    2025_09_Claim_Severity  5.933691e+07\n",
      "5       2025_09_Total_Claim  1.211557e+10\n",
      "6   2025_10_Claim_Frequency  2.703015e+02\n",
      "7    2025_10_Claim_Severity  4.604846e+07\n",
      "8       2025_10_Total_Claim  1.244697e+10\n",
      "9   2025_11_Claim_Frequency  2.655660e+02\n",
      "10   2025_11_Claim_Severity  5.043058e+07\n",
      "11      2025_11_Total_Claim  1.339265e+10\n"
     ]
    }
   ],
   "source": [
    "print(sub.head(12)) ##  19,23433"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15694757,
     "datasetId": 9488145,
     "sourceId": 14836320,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1135.51371,
   "end_time": "2026-02-22T05:20:42.927109",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-22T05:01:47.413399",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03853f4d70244450be8ffc0d414f288f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0d44fdd39fa744caaa995f8215b3abc3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "24f979e09a7c4d9fb775876e2007eefa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "361ea9322cc248ac8fe8cba08d09ab81": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "63660ba9e8934ad5a3558e765d1bd34f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_361ea9322cc248ac8fe8cba08d09ab81",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_03853f4d70244450be8ffc0d414f288f",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡200/200â€‡[03:12&lt;00:00,â€‡â€‡1.02s/it]"
      }
     },
     "83ea90e760fc416b9fba23bfd16e14bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a01d33f082fd4c44b2ff2fc73c1dd765",
        "IPY_MODEL_fa1b8a3b28e94992a4405118f071d0bb",
        "IPY_MODEL_63660ba9e8934ad5a3558e765d1bd34f"
       ],
       "layout": "IPY_MODEL_24f979e09a7c4d9fb775876e2007eefa",
       "tabbable": null,
       "tooltip": null
      }
     },
     "84b19fbc124f4fc7aad64413a8bba26f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a01d33f082fd4c44b2ff2fc73c1dd765": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a2875907f4514678bd8f5cd9c8a079cc",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_0d44fdd39fa744caaa995f8215b3abc3",
       "tabbable": null,
       "tooltip": null,
       "value": "Bestâ€‡trial:â€‡171.â€‡Bestâ€‡value:â€‡0.0271526:â€‡100%"
      }
     },
     "a2875907f4514678bd8f5cd9c8a079cc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b4a18385dff54197ac69f0bbc0d35d0e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fa1b8a3b28e94992a4405118f071d0bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b4a18385dff54197ac69f0bbc0d35d0e",
       "max": 200.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_84b19fbc124f4fc7aad64413a8bba26f",
       "tabbable": null,
       "tooltip": null,
       "value": 200.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
