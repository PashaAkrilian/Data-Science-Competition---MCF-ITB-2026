{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "843592b1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-16T02:27:27.134532Z",
     "iopub.status.busy": "2026-02-16T02:27:27.133849Z",
     "iopub.status.idle": "2026-02-16T02:27:28.253353Z",
     "shell.execute_reply": "2026-02-16T02:27:28.252668Z"
    },
    "papermill": {
     "duration": 1.126731,
     "end_time": "2026-02-16T02:27:28.255596",
     "exception": false,
     "start_time": "2026-02-16T02:27:27.128865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/Data_Klaim.csv\n",
      "/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/sample_submission.csv\n",
      "/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/Data_Polis.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae130e01",
   "metadata": {
    "papermill": {
     "duration": 0.003035,
     "end_time": "2026-02-16T02:27:28.263358",
     "exception": false,
     "start_time": "2026-02-16T02:27:28.260323",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DATA FOUNDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "362641b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T02:27:28.271145Z",
     "iopub.status.busy": "2026-02-16T02:27:28.270531Z",
     "iopub.status.idle": "2026-02-16T02:27:28.507744Z",
     "shell.execute_reply": "2026-02-16T02:27:28.506473Z"
    },
    "papermill": {
     "duration": 0.243853,
     "end_time": "2026-02-16T02:27:28.510119",
     "exception": false,
     "start_time": "2026-02-16T02:27:28.266266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape: (4627, 38)\n",
      "Unique months: 19\n",
      "ICD groups: 36\n",
      "Total missing rate: 0.0\n",
      "\n",
      "STAGE 1 v2 — ULTRA CLEAN & FORECAST OPTIMIZED\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\n",
    "\n",
    "klaim = pd.read_csv(BASE_PATH + \"Data_Klaim.csv\")\n",
    "polis = pd.read_csv(BASE_PATH + \"Data_Polis.csv\")\n",
    "\n",
    "# ============================================================\n",
    "# CLEAN COLUMN NAMES\n",
    "# ============================================================\n",
    "\n",
    "def clean_columns(df):\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"_\", regex=False)\n",
    "        .str.replace(\"/\", \"_\", regex=False)\n",
    "        .str.replace(\"-\", \"_\", regex=False)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "klaim = clean_columns(klaim)\n",
    "polis = clean_columns(polis)\n",
    "\n",
    "klaim = klaim.drop_duplicates().reset_index(drop=True)\n",
    "polis = polis.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# ============================================================\n",
    "# DATE PARSING\n",
    "# ============================================================\n",
    "\n",
    "for col in klaim.columns:\n",
    "    if \"tanggal\" in col:\n",
    "        klaim[col] = pd.to_datetime(klaim[col], errors=\"coerce\")\n",
    "\n",
    "for col in polis.columns:\n",
    "    if \"tanggal\" in col:\n",
    "        polis[col] = pd.to_datetime(polis[col], errors=\"coerce\")\n",
    "\n",
    "# ============================================================\n",
    "# BASIC CLEANING (NO TARGET DISTORTION)\n",
    "# ============================================================\n",
    "\n",
    "klaim = klaim.dropna(subset=[\"nomor_polis\", \"tanggal_pasien_masuk_rs\"])\n",
    "klaim[\"nominal_klaim_yang_disetujui\"] = klaim[\"nominal_klaim_yang_disetujui\"].fillna(0)\n",
    "\n",
    "# ⛔ NO WINSORIZE (do NOT distort total ground truth)\n",
    "\n",
    "# ============================================================\n",
    "# MERGE POLIS\n",
    "# ============================================================\n",
    "\n",
    "df = klaim.merge(polis, on=\"nomor_polis\", how=\"left\")\n",
    "\n",
    "for col in [\"plan_code\", \"gender\", \"domisili\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(\"UNKNOWN\")\n",
    "\n",
    "# ============================================================\n",
    "# SERVICE MONTH (NO LEAKAGE)\n",
    "# ============================================================\n",
    "\n",
    "df[\"year_month\"] = df[\"tanggal_pasien_masuk_rs\"].dt.to_period(\"M\")\n",
    "\n",
    "# ============================================================\n",
    "# DEMOGRAPHICS (SAFE FEATURES)\n",
    "# ============================================================\n",
    "\n",
    "if \"tanggal_lahir\" in df.columns:\n",
    "    df[\"age\"] = (\n",
    "        (df[\"tanggal_pasien_masuk_rs\"] - df[\"tanggal_lahir\"]).dt.days / 365\n",
    "    ).clip(0, 100)\n",
    "    df[\"age\"] = df[\"age\"].fillna(df[\"age\"].median())\n",
    "\n",
    "if \"tanggal_efektif_polis\" in df.columns:\n",
    "    df[\"tenure_days\"] = (\n",
    "        df[\"tanggal_pasien_masuk_rs\"] -\n",
    "        df[\"tanggal_efektif_polis\"]\n",
    "    ).dt.days.clip(lower=0)\n",
    "    df[\"tenure_days\"] = df[\"tenure_days\"].fillna(0)\n",
    "\n",
    "if \"tanggal_pasien_keluar_rs\" in df.columns:\n",
    "    df[\"los\"] = (\n",
    "        df[\"tanggal_pasien_keluar_rs\"] -\n",
    "        df[\"tanggal_pasien_masuk_rs\"]\n",
    "    ).dt.days.clip(lower=0)\n",
    "    df[\"los\"] = df[\"los\"].fillna(0)\n",
    "\n",
    "# ============================================================\n",
    "# SEGMENT FEATURES\n",
    "# ============================================================\n",
    "\n",
    "df[\"care_type\"] = (\n",
    "    df[\"inpatient_outpatient\"]\n",
    "    .astype(str).str.upper().str.strip()\n",
    ")\n",
    "\n",
    "df[\"care_type\"] = df[\"care_type\"].replace([\"NAN\",\"NONE\"],\"UNKNOWN\")\n",
    "\n",
    "df[\"is_inpatient\"] = df[\"care_type\"].eq(\"IP\").astype(int)\n",
    "\n",
    "rc = df[\"reimburse_cashless\"].astype(str).str.upper().str.strip()\n",
    "df[\"is_cashless\"] = rc.eq(\"C\").astype(int)\n",
    "\n",
    "loc = df[\"lokasi_rs\"].astype(str).str.upper().str.strip()\n",
    "\n",
    "df[\"rs_bucket\"] = np.select(\n",
    "    [\n",
    "        loc.eq(\"INDONESIA\"),\n",
    "        loc.eq(\"SINGAPORE\"),\n",
    "        loc.eq(\"MALAYSIA\")\n",
    "    ],\n",
    "    [\"ID\",\"SG\",\"MY\"],\n",
    "    default=\"OTHER\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# ICD REDUCTION (ROBUST GROUPING)\n",
    "# ============================================================\n",
    "\n",
    "df[\"icd_group_raw\"] = (\n",
    "    df[\"icd_diagnosis\"]\n",
    "    .astype(str)\n",
    "    .str.split(\".\").str[0]\n",
    "    .str[:3]\n",
    ")\n",
    "\n",
    "top_icd = df[\"icd_group_raw\"].value_counts().head(35).index\n",
    "\n",
    "df[\"icd_group\"] = np.where(\n",
    "    df[\"icd_group_raw\"].isin(top_icd),\n",
    "    df[\"icd_group_raw\"],\n",
    "    \"OTHER\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# LOG TARGET (ROBUST MODELING)\n",
    "# ============================================================\n",
    "\n",
    "df[\"log_claim\"] = np.log1p(df[\"nominal_klaim_yang_disetujui\"])\n",
    "\n",
    "# ============================================================\n",
    "# MONTHLY PORTFOLIO PANEL (CORE SIGNAL)\n",
    "# ============================================================\n",
    "\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(\n",
    "          frequency=(\"nomor_polis\",\"count\"),\n",
    "          total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\")\n",
    "      )\n",
    "      .sort_index()\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "monthly[\"severity\"] = (\n",
    "    monthly[\"total_claim\"] /\n",
    "    monthly[\"frequency\"].replace(0,np.nan)\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# CORE FORECAST FEATURES (NO LEAKAGE)\n",
    "# ============================================================\n",
    "\n",
    "monthly[\"month_index\"] = np.arange(len(monthly))\n",
    "\n",
    "monthly[\"month\"] = monthly[\"year_month\"].dt.month\n",
    "monthly[\"month_sin\"] = np.sin(2*np.pi*monthly[\"month\"]/12)\n",
    "monthly[\"month_cos\"] = np.cos(2*np.pi*monthly[\"month\"]/12)\n",
    "\n",
    "# LAGS (critical for <5% score)\n",
    "for lag in [1,2,3,6,12]:\n",
    "    monthly[f\"freq_lag{lag}\"] = monthly[\"frequency\"].shift(lag)\n",
    "    monthly[f\"total_lag{lag}\"] = monthly[\"total_claim\"].shift(lag)\n",
    "\n",
    "# Rolling (no leakage)\n",
    "monthly[\"freq_roll3\"] = monthly[\"frequency\"].rolling(3).mean()\n",
    "monthly[\"freq_roll6\"] = monthly[\"frequency\"].rolling(6).mean()\n",
    "\n",
    "monthly[\"total_roll3\"] = monthly[\"total_claim\"].rolling(3).mean()\n",
    "monthly[\"total_roll6\"] = monthly[\"total_claim\"].rolling(6).mean()\n",
    "\n",
    "# YoY robust (avoid explosion)\n",
    "monthly[\"freq_yoy\"] = (\n",
    "    monthly[\"frequency\"] /\n",
    "    monthly[\"freq_lag12\"]\n",
    ")\n",
    "\n",
    "monthly[\"total_yoy\"] = (\n",
    "    monthly[\"total_claim\"] /\n",
    "    monthly[\"total_lag12\"]\n",
    ")\n",
    "\n",
    "# expanding fallback instead of bfill\n",
    "monthly = monthly.fillna(method=\"ffill\")\n",
    "\n",
    "# ============================================================\n",
    "# MERGE CONTEXT BACK\n",
    "# ============================================================\n",
    "\n",
    "context_cols = [\n",
    "    \"year_month\",\n",
    "    \"freq_roll3\",\"freq_roll6\",\n",
    "    \"total_roll3\",\"total_roll6\",\n",
    "    \"freq_yoy\",\"total_yoy\",\n",
    "    \"month_index\",\"month_sin\",\"month_cos\"\n",
    "]\n",
    "\n",
    "df = df.merge(monthly[context_cols], on=\"year_month\", how=\"left\")\n",
    "\n",
    "df = df.fillna(0)\n",
    "\n",
    "# ============================================================\n",
    "# FINAL CHECK\n",
    "# ============================================================\n",
    "\n",
    "print(\"Final shape:\", df.shape)\n",
    "print(\"Unique months:\", df[\"year_month\"].nunique())\n",
    "print(\"ICD groups:\", df[\"icd_group\"].nunique())\n",
    "print(\"Total missing rate:\", df.isna().mean().mean())\n",
    "print(\"\\nSTAGE 1 v2 — ULTRA CLEAN & FORECAST OPTIMIZED\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8b64d3",
   "metadata": {
    "papermill": {
     "duration": 0.002714,
     "end_time": "2026-02-16T02:27:28.515809",
     "exception": false,
     "start_time": "2026-02-16T02:27:28.513095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TIME-SERIES DATASET ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49d12120",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T02:27:28.523310Z",
     "iopub.status.busy": "2026-02-16T02:27:28.522331Z",
     "iopub.status.idle": "2026-02-16T02:27:28.734826Z",
     "shell.execute_reply": "2026-02-16T02:27:28.733844Z"
    },
    "papermill": {
     "duration": 0.218325,
     "end_time": "2026-02-16T02:27:28.736758",
     "exception": false,
     "start_time": "2026-02-16T02:27:28.518433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPACT PANEL SHAPE: (129, 34)\n",
      "Columns: 34\n",
      "\n",
      "STAGE 2 v2 — HIERARCHICAL YOY PANEL READY\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "seg_cols = [\n",
    "    \"plan_code\",\n",
    "    \"care_type\",\n",
    "    \"is_cashless\",\n",
    "    \"rs_bucket\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# 1. BUILD SEGMENT MONTHLY\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly = (\n",
    "    df.groupby([\"year_month\"] + seg_cols)\n",
    "      .agg(\n",
    "          frequency=(\"claim_id\",\"count\"),\n",
    "          total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\")\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(seg_cols + [\"year_month\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 2. TARGET TRANSFORM\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"log_total\"] = np.log1p(seg_monthly[\"total_claim\"])\n",
    "seg_monthly[\"log_freq\"]  = np.log1p(seg_monthly[\"frequency\"])\n",
    "\n",
    "# ============================================================\n",
    "# 3. CALENDAR FEATURES\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"month\"] = seg_monthly[\"year_month\"].dt.month\n",
    "seg_monthly[\"month_sin\"] = np.sin(2*np.pi*seg_monthly[\"month\"]/12)\n",
    "seg_monthly[\"month_cos\"] = np.cos(2*np.pi*seg_monthly[\"month\"]/12)\n",
    "\n",
    "# Trend index per segment\n",
    "seg_monthly[\"seg_month_index\"] = (\n",
    "    seg_monthly.groupby(seg_cols).cumcount()\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 4. CORE LAGS (INCLUDING LAG12 — CRITICAL)\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly = seg_monthly.sort_values(seg_cols + [\"year_month\"])\n",
    "\n",
    "for col in [\"log_total\",\"log_freq\"]:\n",
    "\n",
    "    for lag in [1,2,3,6,12]:\n",
    "        seg_monthly[f\"{col}_lag{lag}\"] = \\\n",
    "            seg_monthly.groupby(seg_cols)[col].shift(lag)\n",
    "\n",
    "    seg_monthly[f\"{col}_roll3\"] = \\\n",
    "        seg_monthly.groupby(seg_cols)[col] \\\n",
    "        .transform(lambda x: x.shift(1).rolling(3).mean())\n",
    "\n",
    "    seg_monthly[f\"{col}_roll6\"] = \\\n",
    "        seg_monthly.groupby(seg_cols)[col] \\\n",
    "        .transform(lambda x: x.shift(1).rolling(6).mean())\n",
    "\n",
    "# ============================================================\n",
    "# 5. YOY RATIO (VERY IMPORTANT FOR AUG–DEC FORECAST)\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"yoy_total\"] = (\n",
    "    seg_monthly[\"log_total_lag1\"] -\n",
    "    seg_monthly[\"log_total_lag12\"]\n",
    ")\n",
    "\n",
    "seg_monthly[\"yoy_freq\"] = (\n",
    "    seg_monthly[\"log_freq_lag1\"] -\n",
    "    seg_monthly[\"log_freq_lag12\"]\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 6. MOMENTUM (SMOOTHER VERSION)\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"momentum_total\"] = (\n",
    "    seg_monthly[\"log_total_roll3\"] -\n",
    "    seg_monthly[\"log_total_roll6\"]\n",
    ")\n",
    "\n",
    "seg_monthly[\"momentum_freq\"] = (\n",
    "    seg_monthly[\"log_freq_roll3\"] -\n",
    "    seg_monthly[\"log_freq_roll6\"]\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 7. SEGMENT SIZE & SHRINKAGE WEIGHT\n",
    "# ============================================================\n",
    "\n",
    "# segment size stability\n",
    "seg_monthly[\"seg_size_roll6\"] = (\n",
    "    seg_monthly.groupby(seg_cols)[\"frequency\"]\n",
    "    .transform(lambda x: x.shift(1).rolling(6).mean())\n",
    ")\n",
    "\n",
    "# portfolio monthly total (for shrink reference)\n",
    "portfolio_month = (\n",
    "    seg_monthly.groupby(\"year_month\")[\"total_claim\"]\n",
    "    .transform(\"sum\")\n",
    ")\n",
    "\n",
    "seg_monthly[\"seg_weight\"] = (\n",
    "    seg_monthly[\"total_claim\"] / portfolio_month\n",
    ").fillna(0)\n",
    "\n",
    "# shrink small segments\n",
    "seg_monthly[\"stability_weight\"] = (\n",
    "    seg_monthly[\"seg_size_roll6\"] /\n",
    "    (seg_monthly[\"seg_size_roll6\"] + 20)\n",
    ").fillna(0)\n",
    "\n",
    "# ============================================================\n",
    "# 8. FILTER EARLY MONTHS (SAFE TRAIN WINDOW)\n",
    "# ============================================================\n",
    "\n",
    "seg_model = seg_monthly[\n",
    "    seg_monthly[\"log_total_lag12\"].notna()\n",
    "].reset_index(drop=True)\n",
    "\n",
    "seg_model = seg_model.fillna(0)\n",
    "\n",
    "print(\"COMPACT PANEL SHAPE:\", seg_model.shape)\n",
    "print(\"Columns:\", len(seg_model.columns))\n",
    "print(\"\\nSTAGE 2 v2 — HIERARCHICAL YOY PANEL READY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039d08db",
   "metadata": {
    "papermill": {
     "duration": 0.002874,
     "end_time": "2026-02-16T02:27:28.742758",
     "exception": false,
     "start_time": "2026-02-16T02:27:28.739884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MODEL DEVELOPMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34c420d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T02:27:28.750217Z",
     "iopub.status.busy": "2026-02-16T02:27:28.749717Z",
     "iopub.status.idle": "2026-02-16T02:27:36.633429Z",
     "shell.execute_reply": "2026-02-16T02:27:36.632446Z"
    },
    "papermill": {
     "duration": 7.88979,
     "end_time": "2026-02-16T02:27:36.635274",
     "exception": false,
     "start_time": "2026-02-16T02:27:28.745484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "MAPE Frequency : 5.79\n",
      "MAPE Total     : 6.35\n",
      "MAPE Severity  : 5.9\n",
      "Estimated Score: 6.35\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FINAL PUSH v2 — DIRECT 15-OUTPUT OPTIMIZATION\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "def weighted_mape(y_true, y_pred):\n",
    "    return np.sum(np.abs(y_true - y_pred)) / np.sum(y_true)\n",
    "\n",
    "# ============================================================\n",
    "# BUILD MONTHLY\n",
    "# ============================================================\n",
    "\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(\n",
    "          frequency=(\"claim_id\",\"count\"),\n",
    "          total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\")\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(\"year_month\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "monthly[\"severity\"] = monthly[\"total_claim\"] / monthly[\"frequency\"]\n",
    "\n",
    "# FEATURE ENGINEERING\n",
    "monthly[\"log_total\"] = np.log1p(monthly[\"total_claim\"])\n",
    "monthly[\"log_freq\"]  = np.log1p(monthly[\"frequency\"])\n",
    "monthly[\"log_sev\"]   = np.log1p(monthly[\"severity\"])\n",
    "\n",
    "monthly[\"month\"] = monthly[\"year_month\"].dt.month\n",
    "monthly[\"month_sin\"] = np.sin(2*np.pi*monthly[\"month\"]/12)\n",
    "monthly[\"month_cos\"] = np.cos(2*np.pi*monthly[\"month\"]/12)\n",
    "\n",
    "for lag in [1,2,3,6,12]:\n",
    "    monthly[f\"log_total_lag{lag}\"] = monthly[\"log_total\"].shift(lag)\n",
    "    monthly[f\"log_freq_lag{lag}\"]  = monthly[\"log_freq\"].shift(lag)\n",
    "    monthly[f\"log_sev_lag{lag}\"]   = monthly[\"log_sev\"].shift(lag)\n",
    "\n",
    "monthly = monthly.dropna().reset_index(drop=True)\n",
    "\n",
    "features = [c for c in monthly.columns if \"lag\" in c] + [\"month_sin\",\"month_cos\"]\n",
    "\n",
    "train = monthly.iloc[:-5]\n",
    "valid = monthly.iloc[-5:]\n",
    "\n",
    "# ============================================================\n",
    "# MODELS\n",
    "# ============================================================\n",
    "\n",
    "model_total = lgb.LGBMRegressor(\n",
    "    objective=\"tweedie\",\n",
    "    tweedie_variance_power=1.3,\n",
    "    n_estimators=800,\n",
    "    learning_rate=0.02,\n",
    "    num_leaves=12,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    random_state=42,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "model_freq = lgb.LGBMRegressor(\n",
    "    objective=\"poisson\",\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.02,\n",
    "    num_leaves=12,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    random_state=42,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "model_sev = lgb.LGBMRegressor(\n",
    "    objective=\"regression\",\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.02,\n",
    "    num_leaves=12,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    random_state=42,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "model_total.fit(train[features], train[\"total_claim\"])\n",
    "model_freq.fit(train[features], train[\"frequency\"])\n",
    "model_sev.fit(train[features], train[\"log_sev\"])\n",
    "\n",
    "pred_total = model_total.predict(valid[features])\n",
    "pred_freq  = model_freq.predict(valid[features])\n",
    "pred_freq  = np.clip(pred_freq,1,None)\n",
    "pred_sev   = np.expm1(model_sev.predict(valid[features]))\n",
    "\n",
    "# small stabilization\n",
    "pred_total = 0.85*pred_total + 0.15*(pred_freq*pred_sev)\n",
    "\n",
    "# ============================================================\n",
    "# TRUE VALUES\n",
    "# ============================================================\n",
    "\n",
    "true_total = valid[\"total_claim\"].values\n",
    "true_freq  = valid[\"frequency\"].values\n",
    "true_sev   = valid[\"severity\"].values\n",
    "\n",
    "# 15-output vector\n",
    "true_vec = np.concatenate([true_freq,true_sev,true_total])\n",
    "pred_vec = np.concatenate([pred_freq,pred_sev,pred_total])\n",
    "\n",
    "score = weighted_mape(true_vec, pred_vec)\n",
    "\n",
    "freq_m  = weighted_mape(true_freq, pred_freq)*100\n",
    "total_m = weighted_mape(true_total, pred_total)*100\n",
    "sev_m   = weighted_mape(true_sev, pred_sev)*100\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"MAPE Frequency :\", round(freq_m,2))\n",
    "print(\"MAPE Total     :\", round(total_m,2))\n",
    "print(\"MAPE Severity  :\", round(sev_m,2))\n",
    "print(\"Estimated Score:\", round(score*100,2))\n",
    "print(\"==============================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd38d93",
   "metadata": {
    "papermill": {
     "duration": 0.003228,
     "end_time": "2026-02-16T02:27:36.641543",
     "exception": false,
     "start_time": "2026-02-16T02:27:36.638315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TOTAL CLAIM OPTIMIZATION & VALIDATION, OPTUNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e839521f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T02:27:36.649477Z",
     "iopub.status.busy": "2026-02-16T02:27:36.648817Z",
     "iopub.status.idle": "2026-02-16T02:27:36.836918Z",
     "shell.execute_reply": "2026-02-16T02:27:36.836283Z"
    },
    "papermill": {
     "duration": 0.19436,
     "end_time": "2026-02-16T02:27:36.838743",
     "exception": false,
     "start_time": "2026-02-16T02:27:36.644383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Final 5M Weighted MAPE: 0.064567\n",
      "==============================\n",
      "STAGE 4 TRIPLE ENSEMBLE COMPLETE\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 4 — TRIPLE MODEL ENSEMBLE PUSH\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "def weighted_mape(y_true, y_pred):\n",
    "    return np.sum(np.abs(y_true - y_pred)) / np.sum(y_true)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# BUILD MONTHLY\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\"))\n",
    "      .reset_index()\n",
    "      .sort_values(\"year_month\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "monthly[\"month\"] = monthly[\"year_month\"].dt.month\n",
    "monthly[\"month_sin\"] = np.sin(2*np.pi*monthly[\"month\"]/12)\n",
    "monthly[\"month_cos\"] = np.cos(2*np.pi*monthly[\"month\"]/12)\n",
    "\n",
    "for lag in [1,2,3,6]:\n",
    "    monthly[f\"lag{lag}\"] = monthly[\"total_claim\"].shift(lag)\n",
    "\n",
    "monthly[\"roll3\"] = monthly[\"total_claim\"].shift(1).rolling(3).mean()\n",
    "\n",
    "monthly = monthly.dropna().reset_index(drop=True)\n",
    "\n",
    "features = [\n",
    "    \"month_sin\",\"month_cos\",\n",
    "    \"lag1\",\"lag2\",\"lag3\",\"lag6\",\"roll3\"\n",
    "]\n",
    "\n",
    "H = 5\n",
    "\n",
    "train = monthly.iloc[:-H].copy()\n",
    "valid = monthly.iloc[-H:].copy()\n",
    "\n",
    "preds_all = []\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3 MODELS WITH DIFFERENT BIAS\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "configs = [\n",
    "    {\"lr\":0.02, \"leaves\":8},\n",
    "    {\"lr\":0.015, \"leaves\":6},\n",
    "    {\"lr\":0.025, \"leaves\":10}\n",
    "]\n",
    "\n",
    "for cfg in configs:\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    for h in range(1, H+1):\n",
    "\n",
    "        train_h = train.copy()\n",
    "        train_h[\"target\"] = train_h[\"total_claim\"].shift(-h)\n",
    "        train_h = train_h.dropna()\n",
    "\n",
    "        if len(train_h) < 3:\n",
    "            preds.append(train[\"total_claim\"].iloc[-1])\n",
    "            continue\n",
    "\n",
    "        model = lgb.LGBMRegressor(\n",
    "            objective=\"tweedie\",\n",
    "            tweedie_variance_power=1.3,\n",
    "            n_estimators=700,\n",
    "            learning_rate=cfg[\"lr\"],\n",
    "            num_leaves=cfg[\"leaves\"],\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            random_state=42,\n",
    "            verbosity=-1\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            train_h[features].astype(float),\n",
    "            train_h[\"target\"].astype(float)\n",
    "        )\n",
    "\n",
    "        X_valid = valid.iloc[[h-1]][features].astype(float)\n",
    "        pred = model.predict(X_valid)[0]\n",
    "\n",
    "        preds.append(pred)\n",
    "\n",
    "    preds_all.append(preds)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ENSEMBLE (OPTIMAL SIMPLE AVERAGE)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "preds_all = np.array(preds_all)\n",
    "\n",
    "final_preds = preds_all.mean(axis=0)\n",
    "\n",
    "score = weighted_mape(valid[\"total_claim\"], final_preds)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"Final 5M Weighted MAPE:\", round(score,6))\n",
    "print(\"==============================\")\n",
    "print(\"STAGE 4 TRIPLE ENSEMBLE COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67d8cc0",
   "metadata": {
    "papermill": {
     "duration": 0.002911,
     "end_time": "2026-02-16T02:27:36.844675",
     "exception": false,
     "start_time": "2026-02-16T02:27:36.841764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TEST PREDICTION & KAGGLE SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55eed16a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T02:27:36.852677Z",
     "iopub.status.busy": "2026-02-16T02:27:36.851960Z",
     "iopub.status.idle": "2026-02-16T02:27:37.022489Z",
     "shell.execute_reply": "2026-02-16T02:27:37.021359Z"
    },
    "papermill": {
     "duration": 0.176843,
     "end_time": "2026-02-16T02:27:37.024371",
     "exception": false,
     "start_time": "2026-02-16T02:27:36.847528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Submission Created (Total + Frequency Modeled)\n",
      "                        id         value\n",
      "0  2025_08_Claim_Frequency  2.393846e+02\n",
      "1   2025_08_Claim_Severity  5.475876e+07\n",
      "2      2025_08_Total_Claim  1.310840e+10\n",
      "3  2025_09_Claim_Frequency  2.393846e+02\n",
      "4   2025_09_Claim_Severity  5.475876e+07\n",
      "5      2025_09_Total_Claim  1.310840e+10\n",
      "6  2025_10_Claim_Frequency  2.393846e+02\n",
      "7   2025_10_Claim_Severity  5.475876e+07\n",
      "8      2025_10_Total_Claim  1.310840e+10\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 5 — FINAL CONSISTENT SUBMISSION (FIXED VERSION)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\n",
    "sample_sub = pd.read_csv(BASE_PATH + \"sample_submission.csv\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# BUILD MONTHLY\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(\n",
    "          frequency=(\"claim_id\",\"count\"),\n",
    "          total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\")\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(\"year_month\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "monthly[\"severity\"] = monthly[\"total_claim\"] / monthly[\"frequency\"]\n",
    "\n",
    "monthly[\"month\"] = monthly[\"year_month\"].dt.month\n",
    "monthly[\"month_sin\"] = np.sin(2*np.pi*monthly[\"month\"]/12)\n",
    "monthly[\"month_cos\"] = np.cos(2*np.pi*monthly[\"month\"]/12)\n",
    "\n",
    "# Use stable lag memory (NO lag12)\n",
    "for lag in [1,2,3,6]:\n",
    "    monthly[f\"lag_total_{lag}\"] = monthly[\"total_claim\"].shift(lag)\n",
    "    monthly[f\"lag_freq_{lag}\"] = monthly[\"frequency\"].shift(lag)\n",
    "\n",
    "monthly[\"roll3_total\"] = monthly[\"total_claim\"].shift(1).rolling(3).mean()\n",
    "monthly[\"roll3_freq\"] = monthly[\"frequency\"].shift(1).rolling(3).mean()\n",
    "\n",
    "monthly = monthly.dropna().reset_index(drop=True)\n",
    "\n",
    "features_total = [\n",
    "    \"month_sin\",\"month_cos\",\n",
    "    \"lag_total_1\",\"lag_total_2\",\"lag_total_3\",\"lag_total_6\",\n",
    "    \"roll3_total\"\n",
    "]\n",
    "\n",
    "features_freq = [\n",
    "    \"month_sin\",\"month_cos\",\n",
    "    \"lag_freq_1\",\"lag_freq_2\",\"lag_freq_3\",\"lag_freq_6\",\n",
    "    \"roll3_freq\"\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# FUTURE PERIODS\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "sample_sub[\"year\"]  = sample_sub[\"id\"].str.split(\"_\").str[0]\n",
    "sample_sub[\"month\"] = sample_sub[\"id\"].str.split(\"_\").str[1]\n",
    "sample_sub[\"month_key\"] = sample_sub[\"year\"] + \"-\" + sample_sub[\"month\"]\n",
    "\n",
    "future_periods = (\n",
    "    pd.PeriodIndex(sample_sub[\"month_key\"], freq=\"M\")\n",
    "      .unique()\n",
    "      .sort_values()\n",
    ")\n",
    "\n",
    "sim_df = monthly.copy()\n",
    "predictions = {}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# RECURSIVE FORECAST\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "for period in future_periods:\n",
    "\n",
    "    new_row = {}\n",
    "    new_row[\"year_month\"] = period\n",
    "    new_row[\"month\"] = period.month\n",
    "    new_row[\"month_sin\"] = np.sin(2*np.pi*period.month/12)\n",
    "    new_row[\"month_cos\"] = np.cos(2*np.pi*period.month/12)\n",
    "\n",
    "    # -------------------------\n",
    "    # Build lag features\n",
    "    # -------------------------\n",
    "    for lag in [1,2,3,6]:\n",
    "        new_row[f\"lag_total_{lag}\"] = sim_df[\"total_claim\"].iloc[-lag]\n",
    "        new_row[f\"lag_freq_{lag}\"] = sim_df[\"frequency\"].iloc[-lag]\n",
    "\n",
    "    new_row[\"roll3_total\"] = sim_df[\"total_claim\"].tail(3).mean()\n",
    "    new_row[\"roll3_freq\"] = sim_df[\"frequency\"].tail(3).mean()\n",
    "\n",
    "    X_total = pd.DataFrame([new_row])[features_total].astype(float)\n",
    "    X_freq  = pd.DataFrame([new_row])[features_freq].astype(float)\n",
    "\n",
    "    # -------------------------\n",
    "    # TOTAL MODEL\n",
    "    # -------------------------\n",
    "    model_total = lgb.LGBMRegressor(\n",
    "        objective=\"tweedie\",\n",
    "        tweedie_variance_power=1.3,\n",
    "        n_estimators=700,\n",
    "        learning_rate=0.02,\n",
    "        num_leaves=8,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        random_state=42,\n",
    "        verbosity=-1\n",
    "    )\n",
    "\n",
    "    model_total.fit(\n",
    "        sim_df[features_total].astype(float),\n",
    "        sim_df[\"total_claim\"].astype(float)\n",
    "    )\n",
    "\n",
    "    pred_total = model_total.predict(X_total)[0]\n",
    "\n",
    "    # -------------------------\n",
    "    # FREQUENCY MODEL\n",
    "    # -------------------------\n",
    "    model_freq = lgb.LGBMRegressor(\n",
    "        objective=\"poisson\",\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.02,\n",
    "        num_leaves=8,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        random_state=42,\n",
    "        verbosity=-1\n",
    "    )\n",
    "\n",
    "    model_freq.fit(\n",
    "        sim_df[features_freq].astype(float),\n",
    "        sim_df[\"frequency\"].astype(float)\n",
    "    )\n",
    "\n",
    "    pred_freq = model_freq.predict(X_freq)[0]\n",
    "    pred_freq = max(pred_freq, 1)\n",
    "\n",
    "    # -------------------------\n",
    "    # SEVERITY (CONSISTENT)\n",
    "    # -------------------------\n",
    "    pred_sev = pred_total / pred_freq\n",
    "\n",
    "    # -------------------------\n",
    "    # UPDATE SIMULATION\n",
    "    # -------------------------\n",
    "    new_row[\"total_claim\"] = pred_total\n",
    "    new_row[\"frequency\"] = pred_freq\n",
    "    new_row[\"severity\"] = pred_sev\n",
    "\n",
    "    sim_df = pd.concat([sim_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    key = f\"{period.year}_{str(period.month).zfill(2)}\"\n",
    "\n",
    "    predictions[f\"{key}_Total_Claim\"] = pred_total\n",
    "    predictions[f\"{key}_Claim_Frequency\"] = pred_freq\n",
    "    predictions[f\"{key}_Claim_Severity\"] = pred_sev\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# BUILD SUBMISSION\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "submission = sample_sub.copy()\n",
    "submission[\"value\"] = submission[\"id\"].map(predictions)\n",
    "submission = submission[[\"id\",\"value\"]]\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"Final Submission Created (Total + Frequency Modeled)\")\n",
    "print(submission.head(9))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15694139,
     "sourceId": 130882,
     "sourceType": "competition"
    },
    {
     "datasetId": 9488145,
     "sourceId": 14836320,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13.945264,
   "end_time": "2026-02-16T02:27:37.948347",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-16T02:27:24.003083",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
