{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e66e827",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-21T01:49:28.267303Z",
     "iopub.status.busy": "2026-02-21T01:49:28.266940Z",
     "iopub.status.idle": "2026-02-21T01:49:29.375721Z",
     "shell.execute_reply": "2026-02-21T01:49:29.374583Z"
    },
    "papermill": {
     "duration": 1.117404,
     "end_time": "2026-02-21T01:49:29.377811",
     "exception": false,
     "start_time": "2026-02-21T01:49:28.260407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/Data_Klaim.csv\n",
      "/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/sample_submission.csv\n",
      "/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/Data_Polis.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b64d43d",
   "metadata": {
    "papermill": {
     "duration": 0.004077,
     "end_time": "2026-02-21T01:49:29.386389",
     "exception": false,
     "start_time": "2026-02-21T01:49:29.382312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DATA FOUNDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a75d6fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T01:49:29.396977Z",
     "iopub.status.busy": "2026-02-21T01:49:29.396548Z",
     "iopub.status.idle": "2026-02-21T01:49:29.641402Z",
     "shell.execute_reply": "2026-02-21T01:49:29.640158Z"
    },
    "papermill": {
     "duration": 0.252975,
     "end_time": "2026-02-21T01:49:29.643524",
     "exception": false,
     "start_time": "2026-02-21T01:49:29.390549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVICE_COL: tanggal_pasien_masuk_rs\n",
      "EXPOSURE_MODE: inforce\n",
      "Policy start col: tanggal_efektif_polis\n",
      "Frequency source: claim_id\n",
      "Monthly shape: (16, 34)\n",
      "Unique months: 16\n",
      "Exposure min/max: 4096.0 4096.0\n",
      "Total_claim min/max: 9610379678.55 17480540371.87\n",
      "\n",
      "STAGE 1 v4 â€” READY\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 1 v4 â€” FOUNDATION (DATASET-AWARE + NO TARGET DISTORTION)\n",
    "# - Fix YYYYMMDD parsing\n",
    "# - Keep RAW nominal for target (total_claim)\n",
    "# - Put winsorization into separate column (optional features)\n",
    "# - Build monthly with complete month range (fill missing months)\n",
    "# - Exposure: claimant / inforce (optional)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\n",
    "klaim = pd.read_csv(BASE_PATH + \"Data_Klaim.csv\")\n",
    "polis = pd.read_csv(BASE_PATH + \"Data_Polis.csv\")\n",
    "\n",
    "# =============================\n",
    "# CLEAN COLUMN NAMES\n",
    "# =============================\n",
    "def clean_columns(df):\n",
    "    df = df.copy()\n",
    "    df.columns = (\n",
    "        df.columns.astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"_\", regex=False)\n",
    "        .str.replace(\"/\", \"_\", regex=False)\n",
    "        .str.replace(\"-\", \"_\", regex=False)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "klaim = clean_columns(klaim)\n",
    "polis = clean_columns(polis)\n",
    "\n",
    "# =============================\n",
    "# DATE PARSING (handle YYYYMMDD int + dd/mm/yyyy)\n",
    "# =============================\n",
    "def parse_mixed_date(s: pd.Series) -> pd.Series:\n",
    "    s = s.copy()\n",
    "    idx = s.index\n",
    "\n",
    "    # normalize to string for pattern checks\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        ss = s.astype(\"Int64\").astype(str)\n",
    "    else:\n",
    "        ss = s.astype(str).str.strip()\n",
    "\n",
    "    ss = ss.replace({\"<NA>\": np.nan, \"nan\": np.nan, \"None\": np.nan, \"NaT\": np.nan})\n",
    "\n",
    "    out = pd.Series(pd.NaT, index=idx, dtype=\"datetime64[ns]\")\n",
    "\n",
    "    # YYYYMMDD (8 digits)\n",
    "    m8 = ss.str.fullmatch(r\"\\d{8}\", na=False)\n",
    "    if m8.any():\n",
    "        out.loc[m8] = pd.to_datetime(ss.loc[m8], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "\n",
    "    # remaining\n",
    "    rest = ~m8 & ss.notna()\n",
    "    if rest.any():\n",
    "        has_slash = ss.loc[rest].str.contains(\"/\", na=False)\n",
    "        if has_slash.any():\n",
    "            out.loc[rest[rest].index.intersection(ss.loc[rest][has_slash].index)] = pd.to_datetime(\n",
    "                ss.loc[rest][has_slash], errors=\"coerce\", dayfirst=True\n",
    "            )\n",
    "        if (~has_slash).any():\n",
    "            out.loc[rest[rest].index.intersection(ss.loc[rest][~has_slash].index)] = pd.to_datetime(\n",
    "                ss.loc[rest][~has_slash], errors=\"coerce\"\n",
    "            )\n",
    "\n",
    "    return out\n",
    "\n",
    "for col in klaim.columns:\n",
    "    if \"tanggal\" in col:\n",
    "        klaim[col] = parse_mixed_date(klaim[col])\n",
    "\n",
    "for col in polis.columns:\n",
    "    if \"tanggal\" in col:\n",
    "        polis[col] = parse_mixed_date(polis[col])\n",
    "\n",
    "# =============================\n",
    "# SAFE DEDUP\n",
    "# =============================\n",
    "claim_id_col = None\n",
    "for c in [\"claim_id\", \"id_klaim\", \"klaim_id\"]:\n",
    "    if c in klaim.columns:\n",
    "        claim_id_col = c\n",
    "        break\n",
    "\n",
    "if claim_id_col is not None:\n",
    "    klaim = klaim.drop_duplicates(subset=[claim_id_col]).reset_index(drop=True)\n",
    "else:\n",
    "    klaim = klaim.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "polis = polis.drop_duplicates(subset=[\"nomor_polis\"]).reset_index(drop=True)\n",
    "\n",
    "# =============================\n",
    "# BASIC CLEANING\n",
    "# =============================\n",
    "# choose service date column\n",
    "service_col = \"tanggal_pasien_masuk_rs\" if \"tanggal_pasien_masuk_rs\" in klaim.columns else None\n",
    "if service_col is None:\n",
    "    # fallback: first tanggal* column\n",
    "    tcols = [c for c in klaim.columns if \"tanggal\" in c]\n",
    "    service_col = tcols[0] if len(tcols) else None\n",
    "\n",
    "if service_col is None:\n",
    "    raise ValueError(\"No tanggal column found in klaim for building year_month.\")\n",
    "\n",
    "klaim = klaim.dropna(subset=[\"nomor_polis\", service_col]).copy()\n",
    "\n",
    "# nominal column\n",
    "nom_col = \"nominal_klaim_yang_disetujui\"\n",
    "if nom_col not in klaim.columns:\n",
    "    # fallback: try find 'nominal' column\n",
    "    cand = [c for c in klaim.columns if \"nominal\" in c]\n",
    "    if len(cand) == 0:\n",
    "        raise ValueError(\"No nominal column found in klaim.\")\n",
    "    nom_col = cand[0]\n",
    "\n",
    "# IMPORTANT: keep RAW nominal for target\n",
    "raw_nom = pd.to_numeric(klaim[nom_col], errors=\"coerce\").fillna(0).clip(lower=0)\n",
    "klaim[nom_col] = raw_nom\n",
    "\n",
    "# OPTIONAL: winsorized copy for feature engineering (NOT for target)\n",
    "klaim[\"nominal_klaim_clip\"] = raw_nom.copy()\n",
    "pos = klaim[\"nominal_klaim_clip\"] > 0\n",
    "if pos.any():\n",
    "    low_q  = klaim.loc[pos, \"nominal_klaim_clip\"].quantile(0.005)\n",
    "    high_q = klaim.loc[pos, \"nominal_klaim_clip\"].quantile(0.995)\n",
    "    klaim.loc[pos, \"nominal_klaim_clip\"] = klaim.loc[pos, \"nominal_klaim_clip\"].clip(low_q, high_q)\n",
    "\n",
    "# =============================\n",
    "# MERGE\n",
    "# =============================\n",
    "df = klaim.merge(polis, on=\"nomor_polis\", how=\"left\")\n",
    "\n",
    "# =============================\n",
    "# SERVICE MONTH\n",
    "# =============================\n",
    "df[\"year_month\"] = df[service_col].dt.to_period(\"M\")\n",
    "\n",
    "min_m = df[\"year_month\"].min()\n",
    "max_m = df[\"year_month\"].max()\n",
    "all_months = pd.period_range(min_m, max_m, freq=\"M\")\n",
    "\n",
    "# ============================================================\n",
    "# EXPOSURE OPTIONS\n",
    "# ============================================================\n",
    "EXPOSURE_MODE = \"inforce\"  # \"claimant\" or \"inforce\"\n",
    "\n",
    "# claimant exposure: unique policies that claim in that month\n",
    "expo_claimant = (\n",
    "    df.groupby(\"year_month\")[\"nomor_polis\"].nunique()\n",
    "      .reindex(all_months, fill_value=0)\n",
    "      .rename(\"exposure_claimant\")\n",
    "      .rename_axis(\"year_month\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# inforce exposure: cumulative started policies (no end date available)\n",
    "start_col = None\n",
    "for c in [\"tanggal_efektif_polis\", \"tanggal_mulai_polis\", \"tanggal_mulai\"]:\n",
    "    if c in polis.columns:\n",
    "        start_col = c\n",
    "        break\n",
    "\n",
    "if start_col is not None:\n",
    "    p = polis[[\"nomor_polis\", start_col]].dropna(subset=[start_col]).copy()\n",
    "    p[\"start_m\"] = p[start_col].dt.to_period(\"M\")\n",
    "\n",
    "    base = p.loc[p[\"start_m\"] < min_m, \"nomor_polis\"].nunique()\n",
    "    inc = p.loc[p[\"start_m\"] >= min_m].groupby(\"start_m\")[\"nomor_polis\"].nunique()\n",
    "\n",
    "    expo_inforce = (\n",
    "        (base + inc.reindex(all_months, fill_value=0).cumsum())\n",
    "        .rename(\"exposure_inforce\")\n",
    "        .rename_axis(\"year_month\")\n",
    "        .reset_index()\n",
    "    )\n",
    "else:\n",
    "    expo_inforce = expo_claimant[[\"year_month\"]].copy()\n",
    "    expo_inforce[\"exposure_inforce\"] = 0\n",
    "\n",
    "expo = expo_claimant.merge(expo_inforce, on=\"year_month\", how=\"left\")\n",
    "\n",
    "# choose exposure with fallback safety\n",
    "expo[\"exposure\"] = np.where(EXPOSURE_MODE == \"inforce\", expo[\"exposure_inforce\"], expo[\"exposure_claimant\"])\n",
    "# if inforce is mostly 0 (bad parsing / missing), fallback to claimant\n",
    "if (EXPOSURE_MODE == \"inforce\") and (expo[\"exposure\"].sum() == 0):\n",
    "    expo[\"exposure\"] = expo[\"exposure_claimant\"]\n",
    "\n",
    "# merge exposure into df (keperluan stage lain)\n",
    "df = df.merge(expo[[\"year_month\", \"exposure\"]], on=\"year_month\", how=\"left\")\n",
    "df[\"active_policies\"] = df[\"exposure\"]\n",
    "\n",
    "# ============================================================\n",
    "# MONTHLY CORE TABLE (complete months)\n",
    "# target total_claim MUST be RAW nominal\n",
    "# ============================================================\n",
    "freq_col = claim_id_col if claim_id_col is not None else \"nomor_polis\"\n",
    "\n",
    "monthly_core = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(\n",
    "          frequency=(freq_col, \"count\"),\n",
    "          total_claim=(nom_col, \"sum\")\n",
    "      )\n",
    "      .reindex(all_months, fill_value=0)\n",
    "      .rename_axis(\"year_month\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "monthly = monthly_core.merge(expo[[\"year_month\", \"exposure\"]], on=\"year_month\", how=\"left\")\n",
    "\n",
    "monthly[\"severity\"] = monthly[\"total_claim\"] / monthly[\"frequency\"].replace(0, np.nan)\n",
    "monthly[\"claim_rate\"] = monthly[\"frequency\"] / monthly[\"exposure\"].replace(0, np.nan)\n",
    "\n",
    "# ============================================================\n",
    "# LOG FEATURES\n",
    "# ============================================================\n",
    "monthly[\"log_total\"] = np.log1p(monthly[\"total_claim\"])\n",
    "monthly[\"log_freq\"]  = np.log1p(monthly[\"frequency\"])\n",
    "monthly[\"log_sev\"]   = np.log1p(monthly[\"severity\"])\n",
    "monthly[\"log_rate\"]  = np.log1p(monthly[\"claim_rate\"])\n",
    "\n",
    "# ============================================================\n",
    "# VOLATILITY\n",
    "# ============================================================\n",
    "monthly[\"roll6\"] = monthly[\"total_claim\"].rolling(6, min_periods=3).mean()\n",
    "monthly[\"std6\"]  = monthly[\"total_claim\"].rolling(6, min_periods=3).std()\n",
    "monthly[\"vol_ratio\"] = monthly[\"std6\"] / monthly[\"roll6\"]\n",
    "monthly[\"high_vol_regime\"] = (monthly[\"vol_ratio\"] > monthly[\"vol_ratio\"].median()).astype(int)\n",
    "\n",
    "# ============================================================\n",
    "# TIME FEATURES\n",
    "# ============================================================\n",
    "monthly[\"month\"] = monthly[\"year_month\"].dt.month\n",
    "monthly[\"month_sin\"] = np.sin(2*np.pi*monthly[\"month\"]/12)\n",
    "monthly[\"month_cos\"] = np.cos(2*np.pi*monthly[\"month\"]/12)\n",
    "monthly[\"month_index\"] = np.arange(len(monthly))\n",
    "\n",
    "# ============================================================\n",
    "# SAFE LAGS\n",
    "# ============================================================\n",
    "for col in [\"log_total\", \"log_freq\", \"log_sev\", \"log_rate\"]:\n",
    "    monthly[f\"{col}_lag1\"] = monthly[col].shift(1)\n",
    "    monthly[f\"{col}_lag2\"] = monthly[col].shift(2)\n",
    "    monthly[f\"{col}_lag3\"] = monthly[col].shift(3)\n",
    "    monthly[f\"{col}_roll3\"] = monthly[col].shift(1).rolling(3).mean()\n",
    "\n",
    "monthly = monthly.dropna().reset_index(drop=True)\n",
    "\n",
    "# ============================================================\n",
    "# FINAL CHECK\n",
    "# ============================================================\n",
    "print(\"SERVICE_COL:\", service_col)\n",
    "print(\"EXPOSURE_MODE:\", EXPOSURE_MODE)\n",
    "print(\"Policy start col:\", start_col)\n",
    "print(\"Frequency source:\", freq_col)\n",
    "print(\"Monthly shape:\", monthly.shape)\n",
    "print(\"Unique months:\", monthly[\"year_month\"].nunique())\n",
    "print(\"Exposure min/max:\", float(monthly[\"exposure\"].min()), float(monthly[\"exposure\"].max()))\n",
    "print(\"Total_claim min/max:\", float(monthly[\"total_claim\"].min()), float(monthly[\"total_claim\"].max()))\n",
    "print(\"\\nSTAGE 1 v4 â€” READY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "750a0205",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T01:49:29.654292Z",
     "iopub.status.busy": "2026-02-21T01:49:29.653933Z",
     "iopub.status.idle": "2026-02-21T01:49:29.670192Z",
     "shell.execute_reply": "2026-02-21T01:49:29.668953Z"
    },
    "papermill": {
     "duration": 0.024122,
     "end_time": "2026-02-21T01:49:29.672350",
     "exception": false,
     "start_time": "2026-02-21T01:49:29.648228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year_month  frequency  exposure  freq_per_exposure\n",
      "6     2024-10        274      4096           0.066895\n",
      "7     2024-11        270      4096           0.065918\n",
      "8     2024-12        238      4096           0.058105\n",
      "9     2025-01        216      4096           0.052734\n",
      "10    2025-02        246      4096           0.060059\n",
      "11    2025-03        230      4096           0.056152\n",
      "12    2025-04        208      4096           0.050781\n",
      "13    2025-05        239      4096           0.058350\n",
      "14    2025-06        234      4096           0.057129\n",
      "15    2025-07        264      4096           0.064453\n",
      "freq_per_exposure min/max: 0.05078125 0.06689453125\n"
     ]
    }
   ],
   "source": [
    "tmp = monthly.copy()\n",
    "tmp[\"freq_per_exposure\"] = tmp[\"frequency\"] / tmp[\"exposure\"]\n",
    "print(tmp[[\"year_month\",\"frequency\",\"exposure\",\"freq_per_exposure\"]].tail(10))\n",
    "print(\"freq_per_exposure min/max:\",\n",
    "      tmp[\"freq_per_exposure\"].min(),\n",
    "      tmp[\"freq_per_exposure\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a417e27",
   "metadata": {
    "papermill": {
     "duration": 0.004402,
     "end_time": "2026-02-21T01:49:29.681247",
     "exception": false,
     "start_time": "2026-02-21T01:49:29.676845",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TIME-SERIES DATASET ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efe0328d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T01:49:29.691822Z",
     "iopub.status.busy": "2026-02-21T01:49:29.691531Z",
     "iopub.status.idle": "2026-02-21T01:49:29.806195Z",
     "shell.execute_reply": "2026-02-21T01:49:29.804950Z"
    },
    "papermill": {
     "duration": 0.122841,
     "end_time": "2026-02-21T01:49:29.808362",
     "exception": false,
     "start_time": "2026-02-21T01:49:29.685521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPACT PANEL SHAPE: (414, 29)\n",
      "Unique segments: 41\n",
      "Columns: 29\n",
      "\n",
      "STAGE 2 â€” ELITE SEGMENT PANEL READY\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 2 â€” ELITE SEGMENT PANEL (SAFE VERSION)\n",
    "# No KeyError â€¢ Auto-create missing columns â€¢ Short series safe\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ ENSURE REQUIRED SEGMENT COLUMNS EXIST\n",
    "# ============================================================\n",
    "\n",
    "# Care Type\n",
    "if \"care_type\" not in df.columns:\n",
    "    if \"inpatient_outpatient\" in df.columns:\n",
    "        df[\"care_type\"] = (\n",
    "            df[\"inpatient_outpatient\"]\n",
    "            .astype(str)\n",
    "            .str.upper()\n",
    "            .str.strip()\n",
    "        )\n",
    "    else:\n",
    "        df[\"care_type\"] = \"UNKNOWN\"\n",
    "\n",
    "df[\"care_type\"] = df[\"care_type\"].fillna(\"UNKNOWN\")\n",
    "\n",
    "\n",
    "# Cashless\n",
    "if \"is_cashless\" not in df.columns:\n",
    "    if \"reimburse_cashless\" in df.columns:\n",
    "        rc = df[\"reimburse_cashless\"].astype(str).str.upper().str.strip()\n",
    "        df[\"is_cashless\"] = rc.eq(\"C\").astype(int)\n",
    "    else:\n",
    "        df[\"is_cashless\"] = 0\n",
    "\n",
    "\n",
    "# RS Bucket\n",
    "if \"rs_bucket\" not in df.columns:\n",
    "    if \"lokasi_rs\" in df.columns:\n",
    "        loc = df[\"lokasi_rs\"].astype(str).str.upper().str.strip()\n",
    "        df[\"rs_bucket\"] = np.select(\n",
    "            [\n",
    "                loc.eq(\"INDONESIA\"),\n",
    "                loc.eq(\"SINGAPORE\"),\n",
    "                loc.eq(\"MALAYSIA\")\n",
    "            ],\n",
    "            [\"ID\",\"SG\",\"MY\"],\n",
    "            default=\"OTHER\"\n",
    "        )\n",
    "    else:\n",
    "        df[\"rs_bucket\"] = \"OTHER\"\n",
    "\n",
    "df[\"rs_bucket\"] = df[\"rs_bucket\"].fillna(\"OTHER\")\n",
    "\n",
    "\n",
    "# Plan Code\n",
    "if \"plan_code\" not in df.columns:\n",
    "    df[\"plan_code\"] = \"UNKNOWN\"\n",
    "\n",
    "df[\"plan_code\"] = df[\"plan_code\"].fillna(\"UNKNOWN\")\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ DEFINE SEGMENT COLUMNS\n",
    "# ============================================================\n",
    "\n",
    "seg_cols = [\"plan_code\",\"care_type\",\"is_cashless\",\"rs_bucket\"]\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ BUILD SEGMENT MONTHLY\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly = (\n",
    "    df.groupby([\"year_month\"] + seg_cols)\n",
    "      .agg(\n",
    "          frequency=(\"nomor_polis\",\"count\"),\n",
    "          total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\"),\n",
    "          exposure=(\"nomor_polis\",\"nunique\")\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(seg_cols + [\"year_month\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ TARGETS\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"severity\"] = (\n",
    "    seg_monthly[\"total_claim\"] /\n",
    "    seg_monthly[\"frequency\"].replace(0, np.nan)\n",
    ")\n",
    "\n",
    "seg_monthly[\"log_total\"] = np.log1p(seg_monthly[\"total_claim\"])\n",
    "seg_monthly[\"log_freq\"]  = np.log1p(seg_monthly[\"frequency\"])\n",
    "seg_monthly[\"log_sev\"]   = np.log1p(seg_monthly[\"severity\"])\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ CALENDAR\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"month\"] = seg_monthly[\"year_month\"].dt.month\n",
    "seg_monthly[\"month_sin\"] = np.sin(2*np.pi*seg_monthly[\"month\"]/12)\n",
    "seg_monthly[\"month_cos\"] = np.cos(2*np.pi*seg_monthly[\"month\"]/12)\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ LAGS (STRICT NO LEAKAGE)\n",
    "# ============================================================\n",
    "\n",
    "for col in [\"log_total\",\"log_freq\",\"log_sev\"]:\n",
    "    \n",
    "    seg_monthly[f\"{col}_lag1\"] = \\\n",
    "        seg_monthly.groupby(seg_cols)[col].shift(1)\n",
    "    \n",
    "    seg_monthly[f\"{col}_lag2\"] = \\\n",
    "        seg_monthly.groupby(seg_cols)[col].shift(2)\n",
    "    \n",
    "    seg_monthly[f\"{col}_lag3\"] = \\\n",
    "        seg_monthly.groupby(seg_cols)[col].shift(3)\n",
    "\n",
    "    seg_monthly[f\"{col}_roll3\"] = \\\n",
    "        seg_monthly.groupby(seg_cols)[col] \\\n",
    "        .transform(lambda x: x.shift(1).rolling(3).mean())\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ MOMENTUM\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"momentum_total\"] = (\n",
    "    seg_monthly[\"log_total_lag1\"] -\n",
    "    seg_monthly[\"log_total_lag2\"]\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ SEGMENT WEIGHT\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"seg_weight\"] = (\n",
    "    seg_monthly[\"frequency\"] /\n",
    "    seg_monthly.groupby(\"year_month\")[\"frequency\"].transform(\"sum\")\n",
    ").fillna(0)\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ SAFE TRAIN WINDOW\n",
    "# ============================================================\n",
    "\n",
    "seg_model = seg_monthly[\n",
    "    seg_monthly[\"log_total_lag3\"].notna()\n",
    "].reset_index(drop=True)\n",
    "\n",
    "seg_model = seg_model.fillna(0)\n",
    "\n",
    "# ============================================================\n",
    "# FINAL CHECK\n",
    "# ============================================================\n",
    "\n",
    "print(\"COMPACT PANEL SHAPE:\", seg_model.shape)\n",
    "print(\"Unique segments:\", seg_model[seg_cols].drop_duplicates().shape[0])\n",
    "print(\"Columns:\", len(seg_model.columns))\n",
    "print(\"\\nSTAGE 2 â€” ELITE SEGMENT PANEL READY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729cba83",
   "metadata": {
    "papermill": {
     "duration": 0.005035,
     "end_time": "2026-02-21T01:49:29.818123",
     "exception": false,
     "start_time": "2026-02-21T01:49:29.813088",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MODEL DEVELOPMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1226bb65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T01:49:29.829245Z",
     "iopub.status.busy": "2026-02-21T01:49:29.828893Z",
     "iopub.status.idle": "2026-02-21T01:50:30.036613Z",
     "shell.execute_reply": "2026-02-21T01:50:30.035648Z"
    },
    "papermill": {
     "duration": 60.22368,
     "end_time": "2026-02-21T01:50:30.046454",
     "exception": false,
     "start_time": "2026-02-21T01:49:29.822774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Horizon months used : 5\n",
      "Best Config:\n",
      "  wt_total=0.85 (ETS weight), anchor_total=median\n",
      "  wt_freq =0.2 (ETS weight), anchor_freq =mean\n",
      "------------------------------\n",
      "STAGE 3 v17 MAPE Frequency : 5.1557\n",
      "STAGE 3 v17 MAPE Total     : 7.9753\n",
      "STAGE 3 v17 MAPE Severity  : 4.7684\n",
      "Estimated Score            : 5.9665\n",
      "==============================\n",
      "\n",
      "Preview last horizon months:\n",
      "   year_month  frequency   total_claim      severity  pred_frequency  \\\n",
      "14    2025-03        230  1.367924e+10  5.947496e+07      234.031716   \n",
      "15    2025-04        208  1.116425e+10  5.367427e+07      232.851773   \n",
      "16    2025-05        239  1.222680e+10  5.115814e+07      237.225688   \n",
      "17    2025-06        234  1.337312e+10  5.715008e+07      234.888808   \n",
      "18    2025-07        264  1.369923e+10  5.189101e+07      235.077202   \n",
      "\n",
      "      pred_total  pred_severity  \n",
      "14  1.224504e+10   5.232214e+07  \n",
      "15  1.224868e+10   5.260289e+07  \n",
      "16  1.222798e+10   5.154577e+07  \n",
      "17  1.221086e+10   5.198572e+07  \n",
      "18  1.219531e+10   5.187790e+07  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 3 v17 â€” KAGGLE-MATCH VALIDATION (AUTO-TUNED SHRINK)\n",
    "# - Horizon = unique months in sample_submission (usually 5)\n",
    "# - Predict TOTAL & FREQ directly (ETS on log1p), derive SEVERITY\n",
    "# - True recursive (refit each step on simulated history)\n",
    "# - Auto grid-search shrink weights + anchor type (mean/median)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\n",
    "sample_sub = pd.read_csv(BASE_PATH + \"sample_submission.csv\")\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "# ==============================\n",
    "# BUILD MONTHLY (consistent with Stage 1 v3)\n",
    "# ==============================\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(\n",
    "          frequency=(\"claim_id\",\"count\"),\n",
    "          total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\"),\n",
    "          exposure=(\"active_policies\",\"first\")\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(\"year_month\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "monthly[\"severity\"]   = monthly[\"total_claim\"] / monthly[\"frequency\"].replace(0, np.nan)\n",
    "monthly[\"claim_rate\"] = monthly[\"frequency\"] / monthly[\"exposure\"].replace(0, np.nan)\n",
    "\n",
    "# ==============================\n",
    "# HORIZON = months in sample_submission (Kaggle behavior)\n",
    "# ==============================\n",
    "sample_sub[\"year\"]  = sample_sub[\"id\"].str.split(\"_\").str[0]\n",
    "sample_sub[\"month\"] = sample_sub[\"id\"].str.split(\"_\").str[1]\n",
    "sample_sub[\"month_key\"] = sample_sub[\"year\"] + \"-\" + sample_sub[\"month\"]\n",
    "\n",
    "future_periods = (\n",
    "    pd.PeriodIndex(sample_sub[\"month_key\"], freq=\"M\")\n",
    "      .unique()\n",
    "      .sort_values()\n",
    ")\n",
    "\n",
    "H = int(len(future_periods))\n",
    "H = min(H, max(1, len(monthly) - 6))  # safety\n",
    "\n",
    "# ==============================\n",
    "# SIMULATOR (true recursive)\n",
    "# ==============================\n",
    "def simulate(train_df, H, wt_total, wt_freq, anchor_total=\"mean\", anchor_freq=\"mean\"):\n",
    "    sim_df = train_df.copy()\n",
    "\n",
    "    pred_total, pred_freq, pred_sev = [], [], []\n",
    "\n",
    "    for step in range(H):\n",
    "        hist = sim_df.copy()\n",
    "\n",
    "        # ---- TOTAL ETS on log1p ----\n",
    "        try:\n",
    "            mdl_t = ExponentialSmoothing(\n",
    "                np.log1p(hist[\"total_claim\"].astype(float)),\n",
    "                trend=\"add\",\n",
    "                damped_trend=True,\n",
    "                seasonal=None\n",
    "            ).fit()\n",
    "            total_fc = float(np.expm1(mdl_t.forecast(1).iloc[0]))\n",
    "        except:\n",
    "            total_fc = float(hist[\"total_claim\"].iloc[-1])\n",
    "\n",
    "        # anchor total\n",
    "        if anchor_total == \"median\":\n",
    "            total_anchor = float(hist[\"total_claim\"].tail(3).median())\n",
    "        else:\n",
    "            total_anchor = float(hist[\"total_claim\"].tail(3).mean())\n",
    "\n",
    "        total_pred = wt_total * total_fc + (1 - wt_total) * total_anchor\n",
    "        total_pred = max(float(total_pred), 1.0)\n",
    "\n",
    "        # ---- FREQ ETS on log1p ----\n",
    "        try:\n",
    "            mdl_f = ExponentialSmoothing(\n",
    "                np.log1p(hist[\"frequency\"].astype(float)),\n",
    "                trend=\"add\",\n",
    "                damped_trend=True,\n",
    "                seasonal=None\n",
    "            ).fit()\n",
    "            freq_fc = float(np.expm1(mdl_f.forecast(1).iloc[0]))\n",
    "        except:\n",
    "            freq_fc = float(hist[\"frequency\"].iloc[-1])\n",
    "\n",
    "        # anchor freq\n",
    "        if anchor_freq == \"median\":\n",
    "            freq_anchor = float(hist[\"frequency\"].tail(3).median())\n",
    "        else:\n",
    "            freq_anchor = float(hist[\"frequency\"].tail(3).mean())\n",
    "\n",
    "        freq_pred = wt_freq * freq_fc + (1 - wt_freq) * freq_anchor\n",
    "        freq_pred = max(float(freq_pred), 1.0)\n",
    "\n",
    "        sev_pred = total_pred / freq_pred\n",
    "\n",
    "        pred_total.append(total_pred)\n",
    "        pred_freq.append(freq_pred)\n",
    "        pred_sev.append(sev_pred)\n",
    "\n",
    "        # ---- append recursive row (keep year_month progressing) ----\n",
    "        last_period = hist[\"year_month\"].iloc[-1]\n",
    "        next_period = last_period + 1\n",
    "        exposure_next = float(hist[\"exposure\"].iloc[-1]) if \"exposure\" in hist.columns else np.nan\n",
    "\n",
    "        sim_df = pd.concat([sim_df, pd.DataFrame([{\n",
    "            \"year_month\": next_period,\n",
    "            \"frequency\": freq_pred,\n",
    "            \"total_claim\": total_pred,\n",
    "            \"exposure\": exposure_next,\n",
    "            \"severity\": sev_pred,\n",
    "            \"claim_rate\": (freq_pred / exposure_next) if (exposure_next and exposure_next > 0) else np.nan\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    return pred_total, pred_freq, pred_sev\n",
    "\n",
    "# ==============================\n",
    "# SPLIT (Kaggle-match horizon)\n",
    "# ==============================\n",
    "train = monthly.iloc[:-H].copy()\n",
    "valid = monthly.iloc[-H:].copy()\n",
    "\n",
    "# ==============================\n",
    "# AUTO SEARCH (small grid, fast)\n",
    "# ==============================\n",
    "wt_total_grid = [0.35, 0.45, 0.55, 0.60, 0.65, 0.75, 0.85]\n",
    "wt_freq_grid  = [0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80]\n",
    "\n",
    "best = {\n",
    "    \"score\": 1e18,\n",
    "    \"params\": None,\n",
    "    \"detail\": None\n",
    "}\n",
    "\n",
    "for wt_t in wt_total_grid:\n",
    "    for wt_f in wt_freq_grid:\n",
    "        for a_t in [\"mean\", \"median\"]:\n",
    "            for a_f in [\"mean\", \"median\"]:\n",
    "\n",
    "                pt, pf, ps = simulate(train, H, wt_t, wt_f, a_t, a_f)\n",
    "\n",
    "                mf = mape(valid[\"frequency\"], pf)\n",
    "                mt = mape(valid[\"total_claim\"], pt)\n",
    "                ms = mape(valid[\"severity\"], ps)\n",
    "                avg = float(np.nanmean([mf, mt, ms]))\n",
    "\n",
    "                if avg < best[\"score\"]:\n",
    "                    best[\"score\"] = avg\n",
    "                    best[\"params\"] = (wt_t, wt_f, a_t, a_f)\n",
    "                    best[\"detail\"] = (mf, mt, ms)\n",
    "\n",
    "# ==============================\n",
    "# RUN BEST + REPORT\n",
    "# ==============================\n",
    "wt_t, wt_f, a_t, a_f = best[\"params\"]\n",
    "pt, pf, ps = simulate(train, H, wt_t, wt_f, a_t, a_f)\n",
    "\n",
    "mf, mt, ms = best[\"detail\"]\n",
    "avg = best[\"score\"]\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(f\"Horizon months used : {H}\")\n",
    "print(\"Best Config:\")\n",
    "print(f\"  wt_total={wt_t} (ETS weight), anchor_total={a_t}\")\n",
    "print(f\"  wt_freq ={wt_f} (ETS weight), anchor_freq ={a_f}\")\n",
    "print(\"------------------------------\")\n",
    "print(\"STAGE 3 v17 MAPE Frequency :\", round(mf, 4))\n",
    "print(\"STAGE 3 v17 MAPE Total     :\", round(mt, 4))\n",
    "print(\"STAGE 3 v17 MAPE Severity  :\", round(ms, 4))\n",
    "print(\"Estimated Score            :\", round(avg, 4))\n",
    "print(\"==============================\")\n",
    "\n",
    "check = valid[[\"year_month\",\"frequency\",\"total_claim\",\"severity\"]].copy()\n",
    "check[\"pred_frequency\"] = pf\n",
    "check[\"pred_total\"] = pt\n",
    "check[\"pred_severity\"] = ps\n",
    "print(\"\\nPreview last horizon months:\")\n",
    "print(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbf1d60",
   "metadata": {
    "papermill": {
     "duration": 0.006791,
     "end_time": "2026-02-21T01:50:30.060157",
     "exception": false,
     "start_time": "2026-02-21T01:50:30.053366",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TOTAL CLAIM OPTIMIZATION & VALIDATION, OPTUNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e95979e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T01:50:30.076685Z",
     "iopub.status.busy": "2026-02-21T01:50:30.076320Z",
     "iopub.status.idle": "2026-02-21T02:04:03.877049Z",
     "shell.execute_reply": "2026-02-21T02:04:03.876070Z"
    },
    "papermill": {
     "duration": 813.812353,
     "end_time": "2026-02-21T02:04:03.879553",
     "exception": false,
     "start_time": "2026-02-21T01:50:30.067200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N months: 19 | Horizon H: 5 | Has exposure: True\n",
      "CV train_ends: [12, 13, 14] | #splits: 3\n",
      "Split weights: [0.05, 0.15, 0.8]\n",
      "Baseline CV %: 7.2792\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6177aa484f9745e4bf4fb29fca4eae6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Horizon: 5 | Splits: [12, 13, 14] | weights: [0.05, 0.15, 0.8]\n",
      "Best Params: {'k_anchor': 4, 'anchor_rate': 'mean', 'anchor_sev': 'mean', 'wt_rate': 0.3332890181752667, 'wt_sev': 0.5501072463530577, 'beta': 0.8005225975816733, 'damped': True, 'init_method': 'estimated', 'capR_low': 0.7818972835090717, 'capR_high': 1.3058960367487729, 'capS_low': 0.6976922342421228, 'capS_high': 1.1536381686265984}\n",
      "CV Best %  : 7.0121\n",
      "==============================\n",
      "\n",
      "Per-split metrics (%):\n",
      "               avg  mape_total  mape_freq  mape_sev  stab_pen\n",
      "train_end                                                    \n",
      "12         12.7698     15.7394     9.1449   13.4252    1.0781\n",
      "13         11.5313     13.9288     4.4085   16.2565    0.9767\n",
      "14          5.7701      6.4797     6.2969    4.5335    1.4895\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 4 v23 â€” RATE+SEVERITY (EXPOSURE-AWARE) + ETS ENSEMBLE + HEAVY RECENT CV\n",
    "# Target: turunin Total & Severity tanpa overfit (series cuma 19 bulan)\n",
    "# - Forecast claim_rate = freq/exposure  (lebih stabil)\n",
    "# - Forecast severity = total/freq       (lebih stabil)\n",
    "# - Reconstruct: freq = rate*exposure, total = freq*sev\n",
    "# - ETS ensemble: trend add + trend none\n",
    "# - Anchor shrink + clamp ratio vs anchor\n",
    "# - Walk-forward CV: bobot lebih berat split terbaru (Kaggle-like)\n",
    "# ============================================================\n",
    "\n",
    "!pip install -q optuna statsmodels\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\n",
    "sample_sub = pd.read_csv(BASE_PATH + \"sample_submission.csv\")\n",
    "\n",
    "# ------------------------------\n",
    "# MAPE (fraction)\n",
    "# ------------------------------\n",
    "def mape_frac(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred) & (y_true != 0)\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return float(np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])))\n",
    "\n",
    "# ------------------------------\n",
    "# BUILD MONTHLY\n",
    "# ------------------------------\n",
    "assert \"df\" in globals(), \"Variabel df belum ada.\"\n",
    "assert \"year_month\" in df.columns, \"df['year_month'] belum ada. Buat dulu: df['year_month']=tanggal.dt.to_period('M')\"\n",
    "\n",
    "has_exposure = \"active_policies\" in df.columns\n",
    "\n",
    "agg_dict = {\n",
    "    \"frequency\": (\"claim_id\",\"count\"),\n",
    "    \"total_claim\": (\"nominal_klaim_yang_disetujui\",\"sum\"),\n",
    "}\n",
    "if has_exposure:\n",
    "    agg_dict[\"exposure\"] = (\"active_policies\",\"first\")\n",
    "\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(**agg_dict)\n",
    "      .reset_index()\n",
    "      .sort_values(\"year_month\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ensure Period[M]\n",
    "if not isinstance(monthly.loc[0, \"year_month\"], pd.Period):\n",
    "    monthly[\"year_month\"] = pd.PeriodIndex(monthly[\"year_month\"], freq=\"M\")\n",
    "\n",
    "monthly[\"frequency\"]   = monthly[\"frequency\"].astype(float).clip(lower=1.0)\n",
    "monthly[\"total_claim\"] = monthly[\"total_claim\"].astype(float).clip(lower=1.0)\n",
    "\n",
    "if not has_exposure:\n",
    "    # fallback exposure: pakai rata-rata frequency * 10 (sekadar scaling stabil)\n",
    "    monthly[\"exposure\"] = float(np.nanmean(monthly[\"frequency\"])) * 10.0\n",
    "\n",
    "monthly[\"exposure\"] = monthly[\"exposure\"].astype(float).clip(lower=1.0)\n",
    "\n",
    "# components\n",
    "monthly[\"severity\"]   = (monthly[\"total_claim\"] / monthly[\"frequency\"]).astype(float).clip(lower=1e-9)\n",
    "monthly[\"claim_rate\"] = (monthly[\"frequency\"]   / monthly[\"exposure\"]).astype(float).clip(lower=1e-12)\n",
    "\n",
    "# ------------------------------\n",
    "# Horizon from sample_submission\n",
    "# ------------------------------\n",
    "sample_sub[\"year\"]  = sample_sub[\"id\"].str.split(\"_\").str[0]\n",
    "sample_sub[\"month\"] = sample_sub[\"id\"].str.split(\"_\").str[1]\n",
    "sample_sub[\"month_key\"] = sample_sub[\"year\"] + \"-\" + sample_sub[\"month\"]\n",
    "\n",
    "future_periods = (\n",
    "    pd.PeriodIndex(sample_sub[\"month_key\"], freq=\"M\")\n",
    "      .unique()\n",
    "      .sort_values()\n",
    ")\n",
    "\n",
    "H = int(len(future_periods))\n",
    "N = len(monthly)\n",
    "H = min(H, max(1, N - 12))  # N=19 -> H=5 tetap\n",
    "print(\"N months:\", N, \"| Horizon H:\", H, \"| Has exposure:\", has_exposure)\n",
    "\n",
    "# ------------------------------\n",
    "# Walk-forward splits\n",
    "# ------------------------------\n",
    "train_end_last = N - H\n",
    "train_ends = list(range(max(12, train_end_last - 2), train_end_last + 1))\n",
    "train_ends = [te for te in train_ends if (te + H) <= N and te >= 12]\n",
    "print(\"CV train_ends:\", train_ends, \"| #splits:\", len(train_ends))\n",
    "\n",
    "# Bobot super fokus split terbaru (ini yang bikin mendekati Stage 3)\n",
    "if len(train_ends) == 1:\n",
    "    split_w = np.array([1.0])\n",
    "elif len(train_ends) == 2:\n",
    "    split_w = np.array([0.15, 0.85], dtype=float)\n",
    "else:\n",
    "    # 3 split: makin ekstrem ke yang terakhir\n",
    "    split_w = np.array([0.05, 0.15, 0.80], dtype=float)\n",
    "split_w = split_w / split_w.sum()\n",
    "print(\"Split weights:\", split_w.round(3).tolist())\n",
    "\n",
    "# ------------------------------\n",
    "# ETS 1-step on LOG series\n",
    "# ------------------------------\n",
    "def ets_1step_log(x_log: pd.Series, trend, damped, init_method):\n",
    "    try:\n",
    "        m = ExponentialSmoothing(\n",
    "            x_log,\n",
    "            trend=trend,\n",
    "            damped_trend=(damped if trend is not None else False),\n",
    "            seasonal=None,\n",
    "            initialization_method=init_method\n",
    "        ).fit()\n",
    "        return float(m.forecast(1).iloc[0])\n",
    "    except:\n",
    "        return float(x_log.iloc[-1])\n",
    "\n",
    "def anchor_level(x_level: pd.Series, k: int, how: str):\n",
    "    tail = np.asarray(x_level.tail(k), dtype=float)\n",
    "    if how == \"median\":\n",
    "        return float(np.median(tail))\n",
    "    return float(np.mean(tail))\n",
    "\n",
    "# ------------------------------\n",
    "# One split TRUE RECURSIVE\n",
    "# ------------------------------\n",
    "def run_split(monthly_all: pd.DataFrame, train_end: int, H: int, P: dict):\n",
    "    train = monthly_all.iloc[:train_end].copy().reset_index(drop=True)\n",
    "    valid = monthly_all.iloc[train_end:train_end+H].copy().reset_index(drop=True)\n",
    "    if len(valid) < H or len(train) < 12:\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    sim = train.copy()\n",
    "\n",
    "    pred_F, pred_T, pred_S = [], [], []\n",
    "    pen = []\n",
    "\n",
    "    for step in range(H):\n",
    "        k = int(P[\"k_anchor\"])\n",
    "\n",
    "        # exposure next: anggap stabil = last known\n",
    "        exp_next = float(sim[\"exposure\"].iloc[-1])\n",
    "        exp_next = max(1.0, exp_next)\n",
    "\n",
    "        # anchors (LEVEL)\n",
    "        aR = anchor_level(sim[\"claim_rate\"], k, P[\"anchor_rate\"])\n",
    "        aS = anchor_level(sim[\"severity\"],   k, P[\"anchor_sev\"])\n",
    "\n",
    "        # build log series for ETS\n",
    "        lr = np.log(sim[\"claim_rate\"].astype(float).clip(lower=1e-12))\n",
    "        ls = np.log(sim[\"severity\"].astype(float).clip(lower=1e-12))\n",
    "\n",
    "        # ETS ensemble for rate\n",
    "        lr_add  = ets_1step_log(lr, trend=\"add\",  damped=P[\"damped\"], init_method=P[\"init_method\"])\n",
    "        lr_none = ets_1step_log(lr, trend=None,   damped=False,       init_method=P[\"init_method\"])\n",
    "        lr_hat  = P[\"beta\"]*lr_add + (1-P[\"beta\"])*lr_none\n",
    "        r_fc    = float(np.exp(lr_hat))\n",
    "\n",
    "        # ETS ensemble for severity\n",
    "        ls_add  = ets_1step_log(ls, trend=\"add\",  damped=P[\"damped\"], init_method=P[\"init_method\"])\n",
    "        ls_none = ets_1step_log(ls, trend=None,   damped=False,       init_method=P[\"init_method\"])\n",
    "        ls_hat  = P[\"beta\"]*ls_add + (1-P[\"beta\"])*ls_none\n",
    "        s_fc    = float(np.exp(ls_hat))\n",
    "\n",
    "        # shrink to anchor (LEVEL) â€” gaya Stage 3\n",
    "        r_pred = P[\"wt_rate\"]*r_fc + (1-P[\"wt_rate\"])*aR\n",
    "        s_pred = P[\"wt_sev\"] *s_fc + (1-P[\"wt_sev\"] )*aS\n",
    "\n",
    "        # clamp ratio vs anchor (anti spike)\n",
    "        r_pred = float(np.clip(r_pred, aR*P[\"capR_low\"], aR*P[\"capR_high\"]))\n",
    "        s_pred = float(np.clip(s_pred, aS*P[\"capS_low\"], aS*P[\"capS_high\"]))\n",
    "\n",
    "        # reconstruct\n",
    "        f_pred = float(max(1.0, np.round(r_pred * exp_next)))\n",
    "        t_pred = float(max(1.0, f_pred * s_pred))\n",
    "        s_pred = float(max(1e-9, t_pred / f_pred))\n",
    "\n",
    "        pred_F.append(f_pred)\n",
    "        pred_T.append(t_pred)\n",
    "        pred_S.append(s_pred)\n",
    "\n",
    "        # stability penalty ringan: drift vs anchor\n",
    "        # (anchor level for freq/total implied)\n",
    "        aF = float(max(1.0, np.round(aR * exp_next)))\n",
    "        aT = float(max(1.0, aF * aS))\n",
    "        pen_f = abs(f_pred - aF) / (abs(aF) + 1e-9)\n",
    "        pen_t = abs(t_pred - aT) / (abs(aT) + 1e-9)\n",
    "        pen.append(0.5*(pen_f + pen_t))\n",
    "\n",
    "        # append recursive\n",
    "        sim = pd.concat([sim, pd.DataFrame([{\n",
    "            \"year_month\": sim[\"year_month\"].iloc[-1] + 1,\n",
    "            \"exposure\": exp_next,\n",
    "            \"claim_rate\": float(max(1e-12, f_pred/exp_next)),\n",
    "            \"severity\": s_pred,\n",
    "            \"frequency\": f_pred,\n",
    "            \"total_claim\": t_pred\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    # targets\n",
    "    yF = valid[\"frequency\"].astype(float).values\n",
    "    yT = valid[\"total_claim\"].astype(float).values\n",
    "    yS = (valid[\"total_claim\"].astype(float).values /\n",
    "          np.clip(valid[\"frequency\"].astype(float).values, 1.0, np.inf))\n",
    "\n",
    "    mf = mape_frac(yF, pred_F)\n",
    "    mt = mape_frac(yT, pred_T)\n",
    "    ms = mape_frac(yS, pred_S)\n",
    "    avg = float(np.nanmean([mf, mt, ms]))\n",
    "    stab = float(np.mean(pen)) if len(pen) else np.nan\n",
    "    return avg, mt, mf, ms, stab\n",
    "\n",
    "# ------------------------------\n",
    "# Baseline: dekat Stage 3 (rate kuat anchor, sev ETS dominan)\n",
    "# ------------------------------\n",
    "P0 = dict(\n",
    "    k_anchor=3,\n",
    "    anchor_rate=\"mean\",\n",
    "    anchor_sev=\"median\",\n",
    "    wt_rate=0.25,\n",
    "    wt_sev=0.85,\n",
    "    beta=0.75,\n",
    "    damped=True,\n",
    "    init_method=\"estimated\",\n",
    "    capR_low=0.80, capR_high=1.25,\n",
    "    capS_low=0.75, capS_high=1.35,\n",
    ")\n",
    "\n",
    "def baseline_score(P):\n",
    "    s = 0.0\n",
    "    for te, w in zip(train_ends, split_w):\n",
    "        avg, *_ = run_split(monthly, te, H, P)\n",
    "        s += w*avg\n",
    "    return float(s)\n",
    "\n",
    "print(\"Baseline CV %:\", round(baseline_score(P0)*100, 4))\n",
    "\n",
    "# ------------------------------\n",
    "# OPTUNA objective (weighted CV + very small penalty)\n",
    "# ------------------------------\n",
    "PEN_W = 0.02\n",
    "\n",
    "def objective(trial):\n",
    "    P = dict(\n",
    "        k_anchor=trial.suggest_int(\"k_anchor\", 2, 5),\n",
    "        anchor_rate=trial.suggest_categorical(\"anchor_rate\", [\"mean\",\"median\"]),\n",
    "        anchor_sev=trial.suggest_categorical(\"anchor_sev\", [\"mean\",\"median\"]),\n",
    "\n",
    "        wt_rate=trial.suggest_float(\"wt_rate\", 0.05, 0.55),\n",
    "        wt_sev=trial.suggest_float(\"wt_sev\",  0.55, 0.98),\n",
    "\n",
    "        beta=trial.suggest_float(\"beta\", 0.30, 0.95),  # ensemble weight (add vs none)\n",
    "        damped=trial.suggest_categorical(\"damped\", [False, True]),\n",
    "        init_method=trial.suggest_categorical(\"init_method\", [\"estimated\",\"heuristic\"]),\n",
    "\n",
    "        capR_low=trial.suggest_float(\"capR_low\", 0.70, 0.92),\n",
    "        capR_high=trial.suggest_float(\"capR_high\", 1.10, 1.35),\n",
    "        capS_low=trial.suggest_float(\"capS_low\", 0.65, 0.90),\n",
    "        capS_high=trial.suggest_float(\"capS_high\", 1.10, 1.50),\n",
    "    )\n",
    "\n",
    "    cv = 0.0\n",
    "    pen = 0.0\n",
    "    ok = 0\n",
    "    for te, w in zip(train_ends, split_w):\n",
    "        avg, mt, mf, ms, stab = run_split(monthly, te, H, P)\n",
    "        if np.isfinite(avg):\n",
    "            cv += w*avg\n",
    "            ok += 1\n",
    "        if np.isfinite(stab):\n",
    "            pen += w*stab\n",
    "    if ok == 0:\n",
    "        return 1e9\n",
    "    return float(cv + PEN_W*pen)\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "\n",
    "# enqueue baseline\n",
    "study.enqueue_trial(P0)\n",
    "\n",
    "study.optimize(objective, n_trials=700, show_progress_bar=True)\n",
    "\n",
    "bestP = study.best_params\n",
    "print(\"\\n==============================\")\n",
    "print(\"Horizon:\", H, \"| Splits:\", train_ends, \"| weights:\", split_w.round(3).tolist())\n",
    "print(\"Best Params:\", bestP)\n",
    "print(\"CV Best %  :\", round(study.best_value*100, 4))\n",
    "print(\"==============================\")\n",
    "\n",
    "# per-split\n",
    "rows = []\n",
    "for te in train_ends:\n",
    "    avg, mt, mf, ms, stab = run_split(monthly, te, H, bestP)\n",
    "    rows.append([te, avg, mt, mf, ms, stab])\n",
    "\n",
    "spl = pd.DataFrame(rows, columns=[\"train_end\",\"avg\",\"mape_total\",\"mape_freq\",\"mape_sev\",\"stab_pen\"])\n",
    "print(\"\\nPer-split metrics (%):\")\n",
    "print((spl.set_index(\"train_end\")[[\"avg\",\"mape_total\",\"mape_freq\",\"mape_sev\",\"stab_pen\"]]*100).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd249c82",
   "metadata": {
    "papermill": {
     "duration": 0.007588,
     "end_time": "2026-02-21T02:04:03.895253",
     "exception": false,
     "start_time": "2026-02-21T02:04:03.887665",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TEST PREDICTION & KAGGLE SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffa8e7d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T02:04:03.913404Z",
     "iopub.status.busy": "2026-02-21T02:04:03.912988Z",
     "iopub.status.idle": "2026-02-21T02:04:09.532071Z",
     "shell.execute_reply": "2026-02-21T02:04:09.531149Z"
    },
    "papermill": {
     "duration": 5.634777,
     "end_time": "2026-02-21T02:04:09.537580",
     "exception": false,
     "start_time": "2026-02-21T02:04:03.902803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monthly_full N: 19 | has_exposure: True | H: 5\n",
      "\n",
      "Backtest last-H:\n",
      "Best blend w(Stage4) = 1.0 | avg % = 5.7701\n",
      "NaN in submission: 0\n",
      "\n",
      "Preview future preds:\n",
      "    period  pred_freq    pred_total      pred_sev\n",
      "0  2025-08      236.0  1.258380e+10  5.332120e+07\n",
      "1  2025-09      241.0  1.284121e+10  5.328303e+07\n",
      "2  2025-10      241.0  1.289896e+10  5.352265e+07\n",
      "3  2025-11      243.0  1.290934e+10  5.312485e+07\n",
      "4  2025-12      239.0  1.272892e+10  5.325907e+07\n",
      "\n",
      "Saved: submission.csv\n",
      "                         id         value\n",
      "0   2025_08_Claim_Frequency  2.360000e+02\n",
      "1    2025_08_Claim_Severity  5.332120e+07\n",
      "2       2025_08_Total_Claim  1.258380e+10\n",
      "3   2025_09_Claim_Frequency  2.410000e+02\n",
      "4    2025_09_Claim_Severity  5.328303e+07\n",
      "5       2025_09_Total_Claim  1.284121e+10\n",
      "6   2025_10_Claim_Frequency  2.410000e+02\n",
      "7    2025_10_Claim_Severity  5.352265e+07\n",
      "8       2025_10_Total_Claim  1.289896e+10\n",
      "9   2025_11_Claim_Frequency  2.430000e+02\n",
      "10   2025_11_Claim_Severity  5.312485e+07\n",
      "11      2025_11_Total_Claim  1.290934e+10\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 5 v25 â€” BLEND Stage4v23 (rate+sev) + Stage3v17 (direct)\n",
    "# - Auto-pick blend weight on last-H backtest\n",
    "# - Then forecast future months from sample_submission\n",
    "# ============================================================\n",
    "\n",
    "!pip install -q statsmodels\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\n",
    "sample_sub = pd.read_csv(BASE_PATH + \"sample_submission.csv\")\n",
    "\n",
    "assert \"df\" in globals(), \"Variabel df belum ada.\"\n",
    "assert \"year_month\" in df.columns, \"df['year_month'] belum ada. Buat dulu: df['year_month']=tanggal.dt.to_period('M')\"\n",
    "assert \"claim_id\" in df.columns, \"Kolom claim_id tidak ada.\"\n",
    "\n",
    "# ------------------------------\n",
    "# FUTURE PERIODS\n",
    "# ------------------------------\n",
    "sample_sub[\"year\"]  = sample_sub[\"id\"].str.split(\"_\").str[0]\n",
    "sample_sub[\"month\"] = sample_sub[\"id\"].str.split(\"_\").str[1]\n",
    "sample_sub[\"month_key\"] = sample_sub[\"year\"] + \"-\" + sample_sub[\"month\"]\n",
    "\n",
    "future_periods = (\n",
    "    pd.PeriodIndex(sample_sub[\"month_key\"], freq=\"M\")\n",
    "      .unique()\n",
    "      .sort_values()\n",
    ")\n",
    "H = int(len(future_periods))\n",
    "\n",
    "# ------------------------------\n",
    "# BUILD monthly_full (KONSISTEN, TANPA dropna lag)\n",
    "# ------------------------------\n",
    "has_exposure = \"active_policies\" in df.columns\n",
    "\n",
    "agg_dict = {\n",
    "    \"frequency\": (\"claim_id\",\"count\"),\n",
    "    \"total_claim\": (\"nominal_klaim_yang_disetujui\",\"sum\"),\n",
    "}\n",
    "if has_exposure:\n",
    "    agg_dict[\"exposure\"] = (\"active_policies\",\"first\")\n",
    "\n",
    "monthly_full = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(**agg_dict)\n",
    "      .reset_index()\n",
    "      .sort_values(\"year_month\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "if not isinstance(monthly_full.loc[0, \"year_month\"], pd.Period):\n",
    "    monthly_full[\"year_month\"] = pd.PeriodIndex(monthly_full[\"year_month\"], freq=\"M\")\n",
    "\n",
    "monthly_full[\"frequency\"]   = monthly_full[\"frequency\"].astype(float).clip(lower=1.0)\n",
    "monthly_full[\"total_claim\"] = monthly_full[\"total_claim\"].astype(float).clip(lower=1.0)\n",
    "\n",
    "if not has_exposure:\n",
    "    monthly_full[\"exposure\"] = float(np.nanmean(monthly_full[\"frequency\"])) * 10.0\n",
    "\n",
    "monthly_full[\"exposure\"] = monthly_full[\"exposure\"].astype(float).clip(lower=1.0)\n",
    "monthly_full[\"severity\"] = (monthly_full[\"total_claim\"] / monthly_full[\"frequency\"]).astype(float).clip(lower=1e-9)\n",
    "monthly_full[\"claim_rate\"] = (monthly_full[\"frequency\"] / monthly_full[\"exposure\"]).astype(float).clip(lower=1e-12)\n",
    "\n",
    "print(\"monthly_full N:\", len(monthly_full), \"| has_exposure:\", has_exposure, \"| H:\", H)\n",
    "\n",
    "# ------------------------------\n",
    "# METRIC\n",
    "# ------------------------------\n",
    "def mape_frac(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred) & (y_true != 0)\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return float(np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])))\n",
    "\n",
    "# ============================================================\n",
    "# MODEL A: Stage 4 v23 (RATE + SEVERITY) â€” exact logic\n",
    "# ============================================================\n",
    "def ets_1step_log(x_log: pd.Series, trend, damped, init_method):\n",
    "    try:\n",
    "        m = ExponentialSmoothing(\n",
    "            x_log,\n",
    "            trend=trend,\n",
    "            damped_trend=(damped if trend is not None else False),\n",
    "            seasonal=None,\n",
    "            initialization_method=init_method\n",
    "        ).fit()\n",
    "        return float(m.forecast(1).iloc[0])\n",
    "    except:\n",
    "        return float(x_log.iloc[-1])\n",
    "\n",
    "def anchor_level(x_level: pd.Series, k: int, how: str):\n",
    "    tail = np.asarray(x_level.tail(k), dtype=float)\n",
    "    if len(tail) == 0:\n",
    "        return float(np.asarray(x_level, dtype=float)[-1])\n",
    "    return float(np.median(tail)) if how == \"median\" else float(np.mean(tail))\n",
    "\n",
    "def predict_stage4_rate_sev(sim_start: pd.DataFrame, periods: pd.PeriodIndex, P: dict):\n",
    "    sim = sim_start.copy().reset_index(drop=True)\n",
    "    out = []\n",
    "\n",
    "    for per in periods:\n",
    "        k = int(P[\"k_anchor\"])\n",
    "        exp_next = float(sim[\"exposure\"].iloc[-1])\n",
    "        exp_next = max(1.0, exp_next)\n",
    "\n",
    "        aR = anchor_level(sim[\"claim_rate\"], k, P[\"anchor_rate\"])\n",
    "        aS = anchor_level(sim[\"severity\"],   k, P[\"anchor_sev\"])\n",
    "\n",
    "        lr = np.log(sim[\"claim_rate\"].astype(float).clip(lower=1e-12))\n",
    "        ls = np.log(sim[\"severity\"].astype(float).clip(lower=1e-12))\n",
    "\n",
    "        lr_add  = ets_1step_log(lr, trend=\"add\",  damped=bool(P[\"damped\"]), init_method=P[\"init_method\"])\n",
    "        lr_none = ets_1step_log(lr, trend=None,   damped=False,             init_method=P[\"init_method\"])\n",
    "        r_fc = float(np.exp(float(P[\"beta\"])*lr_add + (1-float(P[\"beta\"]))*lr_none))\n",
    "\n",
    "        ls_add  = ets_1step_log(ls, trend=\"add\",  damped=bool(P[\"damped\"]), init_method=P[\"init_method\"])\n",
    "        ls_none = ets_1step_log(ls, trend=None,   damped=False,             init_method=P[\"init_method\"])\n",
    "        s_fc = float(np.exp(float(P[\"beta\"])*ls_add + (1-float(P[\"beta\"]))*ls_none))\n",
    "\n",
    "        r_pred = float(P[\"wt_rate\"])*r_fc + (1-float(P[\"wt_rate\"]))*aR\n",
    "        s_pred = float(P[\"wt_sev\"]) *s_fc + (1-float(P[\"wt_sev\"])) *aS\n",
    "\n",
    "        r_pred = float(np.clip(r_pred, aR*float(P[\"capR_low\"]), aR*float(P[\"capR_high\"])))\n",
    "        s_pred = float(np.clip(s_pred, aS*float(P[\"capS_low\"]), aS*float(P[\"capS_high\"])))\n",
    "\n",
    "        f_pred = float(max(1.0, np.round(r_pred * exp_next)))\n",
    "        t_pred = float(max(1.0, f_pred * s_pred))\n",
    "        s_pred = float(max(1e-9, t_pred / f_pred))\n",
    "\n",
    "        out.append((f_pred, t_pred, s_pred))\n",
    "\n",
    "        sim = pd.concat([sim, pd.DataFrame([{\n",
    "            \"year_month\": per,\n",
    "            \"exposure\": exp_next,\n",
    "            \"claim_rate\": float(max(1e-12, f_pred/exp_next)),\n",
    "            \"severity\": s_pred,\n",
    "            \"frequency\": f_pred,\n",
    "            \"total_claim\": t_pred\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# MODEL B: Stage 3 v17 (DIRECT total + freq)\n",
    "# ============================================================\n",
    "def ets_1step_log1p(level_series: pd.Series):\n",
    "    y = np.log1p(level_series.astype(float).clip(lower=1e-12))\n",
    "    try:\n",
    "        m = ExponentialSmoothing(y, trend=\"add\", damped_trend=True, seasonal=None).fit()\n",
    "        return float(np.expm1(m.forecast(1).iloc[0]))\n",
    "    except:\n",
    "        return float(level_series.iloc[-1])\n",
    "\n",
    "def predict_stage3_direct(sim_start: pd.DataFrame, periods: pd.PeriodIndex, CFG: dict):\n",
    "    sim = sim_start.copy().reset_index(drop=True)\n",
    "    out = []\n",
    "    for per in periods:\n",
    "        # total\n",
    "        total_fc = ets_1step_log1p(sim[\"total_claim\"])\n",
    "        total_anchor = float(sim[\"total_claim\"].tail(3).median() if CFG[\"anchor_total\"]==\"median\"\n",
    "                             else sim[\"total_claim\"].tail(3).mean())\n",
    "        total_pred = float(CFG[\"wt_total\"])*total_fc + (1-float(CFG[\"wt_total\"]))*total_anchor\n",
    "        total_pred = max(total_pred, 1.0)\n",
    "\n",
    "        # freq\n",
    "        freq_fc = ets_1step_log1p(sim[\"frequency\"])\n",
    "        freq_anchor = float(sim[\"frequency\"].tail(3).median() if CFG[\"anchor_freq\"]==\"median\"\n",
    "                            else sim[\"frequency\"].tail(3).mean())\n",
    "        freq_pred = float(CFG[\"wt_freq\"])*freq_fc + (1-float(CFG[\"wt_freq\"]))*freq_anchor\n",
    "        freq_pred = max(freq_pred, 1.0)\n",
    "\n",
    "        sev_pred = float(total_pred / freq_pred)\n",
    "        out.append((freq_pred, total_pred, sev_pred))\n",
    "\n",
    "        sim = pd.concat([sim, pd.DataFrame([{\n",
    "            \"year_month\": per,\n",
    "            \"frequency\": freq_pred,\n",
    "            \"total_claim\": total_pred,\n",
    "            \"severity\": sev_pred,\n",
    "            \"exposure\": float(sim[\"exposure\"].iloc[-1]),\n",
    "            \"claim_rate\": float(freq_pred / max(float(sim[\"exposure\"].iloc[-1]), 1.0))\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "# ==============================\n",
    "# PASTE PARAMS (dari output kamu)\n",
    "# ==============================\n",
    "P_STAGE4 = {\n",
    "    'k_anchor': 4,\n",
    "    'anchor_rate': 'mean',\n",
    "    'anchor_sev': 'mean',\n",
    "    'wt_rate': 0.3332890181752667,\n",
    "    'wt_sev': 0.5501072463530577,\n",
    "    'beta': 0.8005225975816733,\n",
    "    'damped': True,\n",
    "    'init_method': 'estimated',\n",
    "    'capR_low': 0.7818972835090717,\n",
    "    'capR_high': 1.3058960367487729,\n",
    "    'capS_low': 0.6976922342421228,\n",
    "    'capS_high': 1.1536381686265984\n",
    "}\n",
    "\n",
    "CFG_STAGE3 = {\n",
    "    \"wt_total\": 0.85, \"anchor_total\": \"median\",\n",
    "    \"wt_freq\":  0.20, \"anchor_freq\":  \"mean\",\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# AUTO PICK BLEND WEIGHT on last-H backtest\n",
    "# ============================================================\n",
    "N = len(monthly_full)\n",
    "H_bt = min(H, max(1, N-6))          # safety kalau data kependekan\n",
    "train_bt = monthly_full.iloc[:-H_bt].copy()\n",
    "valid_bt = monthly_full.iloc[-H_bt:].copy()\n",
    "periods_bt = valid_bt[\"year_month\"].tolist()\n",
    "periods_bt = pd.PeriodIndex(periods_bt, freq=\"M\")\n",
    "\n",
    "predA = predict_stage4_rate_sev(train_bt, periods_bt, P_STAGE4)\n",
    "predB = predict_stage3_direct(train_bt, periods_bt, CFG_STAGE3)\n",
    "\n",
    "yF = valid_bt[\"frequency\"].values\n",
    "yT = valid_bt[\"total_claim\"].values\n",
    "yS = (valid_bt[\"total_claim\"].values / np.clip(valid_bt[\"frequency\"].values, 1.0, np.inf))\n",
    "\n",
    "best_w, best_score = None, 1e18\n",
    "for w in np.linspace(0.0, 1.0, 21):  # 0..1 step 0.05\n",
    "    Fp = []; Tp = []; Sp = []\n",
    "    for (Fa, Ta, _Sa), (Fb, Tb, _Sb) in zip(predA, predB):\n",
    "        f = w*Fa + (1-w)*Fb\n",
    "        t = w*Ta + (1-w)*Tb\n",
    "        f = max(1.0, float(np.round(f)))\n",
    "        t = max(1.0, float(t))\n",
    "        s = t / f\n",
    "        Fp.append(f); Tp.append(t); Sp.append(s)\n",
    "\n",
    "    mf = mape_frac(yF, Fp)\n",
    "    mt = mape_frac(yT, Tp)\n",
    "    ms = mape_frac(yS, Sp)\n",
    "    avg = float(np.nanmean([mf, mt, ms]))\n",
    "\n",
    "    if avg < best_score:\n",
    "        best_score = avg\n",
    "        best_w = float(w)\n",
    "\n",
    "print(\"\\nBacktest last-H:\")\n",
    "print(\"Best blend w(Stage4) =\", best_w, \"| avg % =\", round(best_score*100, 4))\n",
    "\n",
    "# ============================================================\n",
    "# FORECAST FUTURE with chosen blend\n",
    "# ============================================================\n",
    "predA_fut = predict_stage4_rate_sev(monthly_full, future_periods, P_STAGE4)\n",
    "predB_fut = predict_stage3_direct(monthly_full, future_periods, CFG_STAGE3)\n",
    "\n",
    "pred_map = {}\n",
    "preview = []\n",
    "\n",
    "for per, (Fa, Ta, _Sa), (Fb, Tb, _Sb) in zip(future_periods, predA_fut, predB_fut):\n",
    "    f = best_w*Fa + (1-best_w)*Fb\n",
    "    t = best_w*Ta + (1-best_w)*Tb\n",
    "    f = max(1.0, float(np.round(f)))\n",
    "    t = max(1.0, float(t))\n",
    "    s = float(t / f)\n",
    "\n",
    "    key = f\"{per.year}_{str(per.month).zfill(2)}\"\n",
    "    pred_map[f\"{key}_Claim_Frequency\"] = f\n",
    "    pred_map[f\"{key}_Total_Claim\"]     = t\n",
    "    pred_map[f\"{key}_Claim_Severity\"]  = s\n",
    "\n",
    "    preview.append([str(per), f, t, s])\n",
    "\n",
    "sub = sample_sub.copy()\n",
    "sub[\"value\"] = sub[\"id\"].map(pred_map)\n",
    "\n",
    "missing = int(sub[\"value\"].isna().sum())\n",
    "print(\"NaN in submission:\", missing)\n",
    "if missing:\n",
    "    print(sub.loc[sub[\"value\"].isna(), \"id\"].head(10).tolist())\n",
    "assert missing == 0\n",
    "\n",
    "sub = sub[[\"id\",\"value\"]]\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "pv = pd.DataFrame(preview, columns=[\"period\",\"pred_freq\",\"pred_total\",\"pred_sev\"])\n",
    "print(\"\\nPreview future preds:\")\n",
    "print(pv)\n",
    "\n",
    "print(\"\\nSaved: submission.csv\")\n",
    "print(sub.head(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cff49450",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T02:04:09.555891Z",
     "iopub.status.busy": "2026-02-21T02:04:09.555479Z",
     "iopub.status.idle": "2026-02-21T02:04:09.565858Z",
     "shell.execute_reply": "2026-02-21T02:04:09.565118Z"
    },
    "papermill": {
     "duration": 0.023245,
     "end_time": "2026-02-21T02:04:09.568867",
     "exception": false,
     "start_time": "2026-02-21T02:04:09.545622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         id         value\n",
      "0   2025_08_Claim_Frequency  2.360000e+02\n",
      "1    2025_08_Claim_Severity  5.332120e+07\n",
      "2       2025_08_Total_Claim  1.258380e+10\n",
      "3   2025_09_Claim_Frequency  2.410000e+02\n",
      "4    2025_09_Claim_Severity  5.328303e+07\n",
      "5       2025_09_Total_Claim  1.284121e+10\n",
      "6   2025_10_Claim_Frequency  2.410000e+02\n",
      "7    2025_10_Claim_Severity  5.352265e+07\n",
      "8       2025_10_Total_Claim  1.289896e+10\n",
      "9   2025_11_Claim_Frequency  2.430000e+02\n",
      "10   2025_11_Claim_Severity  5.312485e+07\n",
      "11      2025_11_Total_Claim  1.290934e+10\n"
     ]
    }
   ],
   "source": [
    "print(sub.head(12)) ##  13 persen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b456870",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T02:04:09.586264Z",
     "iopub.status.busy": "2026-02-21T02:04:09.585889Z",
     "iopub.status.idle": "2026-02-21T02:04:10.016759Z",
     "shell.execute_reply": "2026-02-21T02:04:10.014794Z"
    },
    "papermill": {
     "duration": 0.442823,
     "end_time": "2026-02-21T02:04:10.019488",
     "exception": false,
     "start_time": "2026-02-21T02:04:09.576665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backtest last-H (%): avg 5.7701 | total 6.4797 | freq 6.2969 | sev 4.5335\n",
      "        ym     yF     pF            yT            pT\n",
      "0  2025-03  230.0  241.0  1.367924e+10  1.282809e+10\n",
      "1  2025-04  208.0  236.0  1.116425e+10  1.263251e+10\n",
      "2  2025-05  239.0  236.0  1.222680e+10  1.272460e+10\n",
      "3  2025-06  234.0  239.0  1.337312e+10  1.315459e+10\n",
      "4  2025-07  264.0  238.0  1.369923e+10  1.269653e+10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "def mape_frac(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred) & (y_true != 0)\n",
    "    return float(np.mean(np.abs((y_true[mask]-y_pred[mask]) / y_true[mask])))\n",
    "\n",
    "# PASTE best params Stage4 v23 kamu\n",
    "BEST = {\n",
    "    'k_anchor': 4,\n",
    "    'anchor_rate': 'mean',\n",
    "    'anchor_sev': 'mean',\n",
    "    'wt_rate': 0.3332890181752667,\n",
    "    'wt_sev': 0.5501072463530577,\n",
    "    'beta': 0.8005225975816733,\n",
    "    'damped': True,\n",
    "    'init_method': 'estimated',\n",
    "    'capR_low': 0.7818972835090717,\n",
    "    'capR_high': 1.3058960367487729,\n",
    "    'capS_low': 0.6976922342421228,\n",
    "    'capS_high': 1.1536381686265984\n",
    "}\n",
    "\n",
    "# monthly harus yang sama dengan Stage4 v23 (19 bulan, ada exposure/claim_rate/severity)\n",
    "# Kalau kamu sudah punya `monthly` dari Stage4 v23, pakai itu. Jangan rebuild lagi.\n",
    "assert \"monthly\" in globals(), \"monthly belum ada (pakai monthly dari Stage4 v23)\"\n",
    "monthly_stage4 = monthly.copy().reset_index(drop=True)\n",
    "\n",
    "# horizon sama seperti Kaggle (5)\n",
    "H = 5\n",
    "N = len(monthly_stage4)\n",
    "train_end = N - H\n",
    "train = monthly_stage4.iloc[:train_end].copy().reset_index(drop=True)\n",
    "valid = monthly_stage4.iloc[train_end:train_end+H].copy().reset_index(drop=True)\n",
    "\n",
    "def ets_1step_log(x_log: pd.Series, trend, damped, init_method):\n",
    "    try:\n",
    "        m = ExponentialSmoothing(\n",
    "            x_log,\n",
    "            trend=trend,\n",
    "            damped_trend=(damped if trend is not None else False),\n",
    "            seasonal=None,\n",
    "            initialization_method=init_method\n",
    "        ).fit()\n",
    "        return float(m.forecast(1).iloc[0])\n",
    "    except:\n",
    "        return float(x_log.iloc[-1])\n",
    "\n",
    "def anchor_level(x_level: pd.Series, k: int, how: str):\n",
    "    tail = np.asarray(x_level.tail(k), dtype=float)\n",
    "    return float(np.median(tail)) if how == \"median\" else float(np.mean(tail))\n",
    "\n",
    "sim = train.copy()\n",
    "pred_F, pred_T, pred_S = [], [], []\n",
    "\n",
    "for _ in range(H):\n",
    "    k = int(BEST[\"k_anchor\"])\n",
    "    exp_next = float(sim[\"exposure\"].iloc[-1])\n",
    "\n",
    "    aR = anchor_level(sim[\"claim_rate\"], k, BEST[\"anchor_rate\"])\n",
    "    aS = anchor_level(sim[\"severity\"],   k, BEST[\"anchor_sev\"])\n",
    "\n",
    "    lr = np.log(sim[\"claim_rate\"].astype(float).clip(lower=1e-12))\n",
    "    ls = np.log(sim[\"severity\"].astype(float).clip(lower=1e-12))\n",
    "\n",
    "    lr_add  = ets_1step_log(lr, trend=\"add\",  damped=bool(BEST[\"damped\"]), init_method=BEST[\"init_method\"])\n",
    "    lr_none = ets_1step_log(lr, trend=None,   damped=False,                init_method=BEST[\"init_method\"])\n",
    "    r_fc = float(np.exp(float(BEST[\"beta\"])*lr_add + (1-float(BEST[\"beta\"]))*lr_none))\n",
    "\n",
    "    ls_add  = ets_1step_log(ls, trend=\"add\",  damped=bool(BEST[\"damped\"]), init_method=BEST[\"init_method\"])\n",
    "    ls_none = ets_1step_log(ls, trend=None,   damped=False,                init_method=BEST[\"init_method\"])\n",
    "    s_fc = float(np.exp(float(BEST[\"beta\"])*ls_add + (1-float(BEST[\"beta\"]))*ls_none))\n",
    "\n",
    "    r_pred = float(BEST[\"wt_rate\"])*r_fc + (1-float(BEST[\"wt_rate\"]))*aR\n",
    "    s_pred = float(BEST[\"wt_sev\"]) *s_fc + (1-float(BEST[\"wt_sev\"])) *aS\n",
    "\n",
    "    r_pred = float(np.clip(r_pred, aR*float(BEST[\"capR_low\"]), aR*float(BEST[\"capR_high\"])))\n",
    "    s_pred = float(np.clip(s_pred, aS*float(BEST[\"capS_low\"]), aS*float(BEST[\"capS_high\"])))\n",
    "\n",
    "    f_pred = float(max(1.0, np.round(r_pred * exp_next)))\n",
    "    t_pred = float(max(1.0, f_pred * s_pred))\n",
    "    s_pred = float(t_pred / f_pred)\n",
    "\n",
    "    pred_F.append(f_pred)\n",
    "    pred_T.append(t_pred)\n",
    "    pred_S.append(s_pred)\n",
    "\n",
    "    sim = pd.concat([sim, pd.DataFrame([{\n",
    "        \"year_month\": sim[\"year_month\"].iloc[-1] + 1,\n",
    "        \"exposure\": exp_next,\n",
    "        \"claim_rate\": float(max(1e-12, f_pred/exp_next)),\n",
    "        \"severity\": s_pred,\n",
    "        \"frequency\": f_pred,\n",
    "        \"total_claim\": t_pred\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "yF = valid[\"frequency\"].values\n",
    "yT = valid[\"total_claim\"].values\n",
    "yS = (valid[\"total_claim\"].values / np.clip(valid[\"frequency\"].values, 1.0, np.inf))\n",
    "\n",
    "mf = mape_frac(yF, pred_F)\n",
    "mt = mape_frac(yT, pred_T)\n",
    "ms = mape_frac(yS, pred_S)\n",
    "avg = float(np.nanmean([mf, mt, ms]))\n",
    "\n",
    "print(\"Backtest last-H (%):\",\n",
    "      \"avg\", round(avg*100,4),\n",
    "      \"| total\", round(mt*100,4),\n",
    "      \"| freq\", round(mf*100,4),\n",
    "      \"| sev\", round(ms*100,4))\n",
    "print(pd.DataFrame({\"ym\": valid[\"year_month\"], \"yF\": yF, \"pF\": pred_F, \"yT\": yT, \"pT\": pred_T}))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15694757,
     "datasetId": 9488145,
     "sourceId": 14836320,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 885.94978,
   "end_time": "2026-02-21T02:04:10.750366",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-21T01:49:24.800586",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0a84ca03c70344978465a5802918022b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e3fc704fd262480397a9848fa6c57af9",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_f5a5b02845d848afb4d15a8fb9391a25",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡700/700â€‡[13:25&lt;00:00,â€‡â€‡1.21s/it]"
      }
     },
     "5baec766d4ad4c3fabd4929074eee58c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6177aa484f9745e4bf4fb29fca4eae6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6a5b3dab64ea4bd99d077adb8c018e17",
        "IPY_MODEL_eb081ffba17c449787f73ab2e4eb9118",
        "IPY_MODEL_0a84ca03c70344978465a5802918022b"
       ],
       "layout": "IPY_MODEL_5baec766d4ad4c3fabd4929074eee58c",
       "tabbable": null,
       "tooltip": null
      }
     },
     "6a5b3dab64ea4bd99d077adb8c018e17": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ecdbbb0389944b25beffcb2fe88ed75d",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_8eec459057e54581a6c07cf350b025d6",
       "tabbable": null,
       "tooltip": null,
       "value": "Bestâ€‡trial:â€‡660.â€‡Bestâ€‡value:â€‡0.0701207:â€‡100%"
      }
     },
     "8eec459057e54581a6c07cf350b025d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c47db53485cb481db037ecab5193a27e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "dadcd022da5b4d8f8fb4bbba336ebe2c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e3fc704fd262480397a9848fa6c57af9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eb081ffba17c449787f73ab2e4eb9118": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_dadcd022da5b4d8f8fb4bbba336ebe2c",
       "max": 700.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c47db53485cb481db037ecab5193a27e",
       "tabbable": null,
       "tooltip": null,
       "value": 700.0
      }
     },
     "ecdbbb0389944b25beffcb2fe88ed75d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f5a5b02845d848afb4d15a8fb9391a25": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
