{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b03572ea",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-21T01:56:01.091989Z",
     "iopub.status.busy": "2026-02-21T01:56:01.091096Z",
     "iopub.status.idle": "2026-02-21T01:56:02.345494Z",
     "shell.execute_reply": "2026-02-21T01:56:02.344077Z"
    },
    "papermill": {
     "duration": 1.26471,
     "end_time": "2026-02-21T01:56:02.348114",
     "exception": false,
     "start_time": "2026-02-21T01:56:01.083404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/Data_Klaim.csv\n",
      "/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/sample_submission.csv\n",
      "/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/Data_Polis.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc8b6d1",
   "metadata": {
    "papermill": {
     "duration": 0.004108,
     "end_time": "2026-02-21T01:56:02.356826",
     "exception": false,
     "start_time": "2026-02-21T01:56:02.352718",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DATA FOUNDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afdd05a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T01:56:02.368626Z",
     "iopub.status.busy": "2026-02-21T01:56:02.367929Z",
     "iopub.status.idle": "2026-02-21T01:56:02.668586Z",
     "shell.execute_reply": "2026-02-21T01:56:02.667205Z"
    },
    "papermill": {
     "duration": 0.309998,
     "end_time": "2026-02-21T01:56:02.671206",
     "exception": false,
     "start_time": "2026-02-21T01:56:02.361208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVICE_COL: tanggal_pasien_masuk_rs\n",
      "EXPOSURE_MODE: inforce\n",
      "Policy start col: tanggal_efektif_polis\n",
      "Frequency source: claim_id\n",
      "Monthly shape: (16, 34)\n",
      "Unique months: 16\n",
      "Exposure min/max: 4096.0 4096.0\n",
      "Total_claim min/max: 9610379678.55 17480540371.87\n",
      "\n",
      "STAGE 1 v4 â€” READY\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 1 v4 â€” FOUNDATION (DATASET-AWARE + NO TARGET DISTORTION)\n",
    "# - Fix YYYYMMDD parsing\n",
    "# - Keep RAW nominal for target (total_claim)\n",
    "# - Put winsorization into separate column (optional features)\n",
    "# - Build monthly with complete month range (fill missing months)\n",
    "# - Exposure: claimant / inforce (optional)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\n",
    "klaim = pd.read_csv(BASE_PATH + \"Data_Klaim.csv\")\n",
    "polis = pd.read_csv(BASE_PATH + \"Data_Polis.csv\")\n",
    "\n",
    "# =============================\n",
    "# CLEAN COLUMN NAMES\n",
    "# =============================\n",
    "def clean_columns(df):\n",
    "    df = df.copy()\n",
    "    df.columns = (\n",
    "        df.columns.astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"_\", regex=False)\n",
    "        .str.replace(\"/\", \"_\", regex=False)\n",
    "        .str.replace(\"-\", \"_\", regex=False)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "klaim = clean_columns(klaim)\n",
    "polis = clean_columns(polis)\n",
    "\n",
    "# =============================\n",
    "# DATE PARSING (handle YYYYMMDD int + dd/mm/yyyy)\n",
    "# =============================\n",
    "def parse_mixed_date(s: pd.Series) -> pd.Series:\n",
    "    s = s.copy()\n",
    "    idx = s.index\n",
    "\n",
    "    # normalize to string for pattern checks\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        ss = s.astype(\"Int64\").astype(str)\n",
    "    else:\n",
    "        ss = s.astype(str).str.strip()\n",
    "\n",
    "    ss = ss.replace({\"<NA>\": np.nan, \"nan\": np.nan, \"None\": np.nan, \"NaT\": np.nan})\n",
    "\n",
    "    out = pd.Series(pd.NaT, index=idx, dtype=\"datetime64[ns]\")\n",
    "\n",
    "    # YYYYMMDD (8 digits)\n",
    "    m8 = ss.str.fullmatch(r\"\\d{8}\", na=False)\n",
    "    if m8.any():\n",
    "        out.loc[m8] = pd.to_datetime(ss.loc[m8], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "\n",
    "    # remaining\n",
    "    rest = ~m8 & ss.notna()\n",
    "    if rest.any():\n",
    "        has_slash = ss.loc[rest].str.contains(\"/\", na=False)\n",
    "        if has_slash.any():\n",
    "            out.loc[rest[rest].index.intersection(ss.loc[rest][has_slash].index)] = pd.to_datetime(\n",
    "                ss.loc[rest][has_slash], errors=\"coerce\", dayfirst=True\n",
    "            )\n",
    "        if (~has_slash).any():\n",
    "            out.loc[rest[rest].index.intersection(ss.loc[rest][~has_slash].index)] = pd.to_datetime(\n",
    "                ss.loc[rest][~has_slash], errors=\"coerce\"\n",
    "            )\n",
    "\n",
    "    return out\n",
    "\n",
    "for col in klaim.columns:\n",
    "    if \"tanggal\" in col:\n",
    "        klaim[col] = parse_mixed_date(klaim[col])\n",
    "\n",
    "for col in polis.columns:\n",
    "    if \"tanggal\" in col:\n",
    "        polis[col] = parse_mixed_date(polis[col])\n",
    "\n",
    "# =============================\n",
    "# SAFE DEDUP\n",
    "# =============================\n",
    "claim_id_col = None\n",
    "for c in [\"claim_id\", \"id_klaim\", \"klaim_id\"]:\n",
    "    if c in klaim.columns:\n",
    "        claim_id_col = c\n",
    "        break\n",
    "\n",
    "if claim_id_col is not None:\n",
    "    klaim = klaim.drop_duplicates(subset=[claim_id_col]).reset_index(drop=True)\n",
    "else:\n",
    "    klaim = klaim.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "polis = polis.drop_duplicates(subset=[\"nomor_polis\"]).reset_index(drop=True)\n",
    "\n",
    "# =============================\n",
    "# BASIC CLEANING\n",
    "# =============================\n",
    "# choose service date column\n",
    "service_col = \"tanggal_pasien_masuk_rs\" if \"tanggal_pasien_masuk_rs\" in klaim.columns else None\n",
    "if service_col is None:\n",
    "    # fallback: first tanggal* column\n",
    "    tcols = [c for c in klaim.columns if \"tanggal\" in c]\n",
    "    service_col = tcols[0] if len(tcols) else None\n",
    "\n",
    "if service_col is None:\n",
    "    raise ValueError(\"No tanggal column found in klaim for building year_month.\")\n",
    "\n",
    "klaim = klaim.dropna(subset=[\"nomor_polis\", service_col]).copy()\n",
    "\n",
    "# nominal column\n",
    "nom_col = \"nominal_klaim_yang_disetujui\"\n",
    "if nom_col not in klaim.columns:\n",
    "    # fallback: try find 'nominal' column\n",
    "    cand = [c for c in klaim.columns if \"nominal\" in c]\n",
    "    if len(cand) == 0:\n",
    "        raise ValueError(\"No nominal column found in klaim.\")\n",
    "    nom_col = cand[0]\n",
    "\n",
    "# IMPORTANT: keep RAW nominal for target\n",
    "raw_nom = pd.to_numeric(klaim[nom_col], errors=\"coerce\").fillna(0).clip(lower=0)\n",
    "klaim[nom_col] = raw_nom\n",
    "\n",
    "# OPTIONAL: winsorized copy for feature engineering (NOT for target)\n",
    "klaim[\"nominal_klaim_clip\"] = raw_nom.copy()\n",
    "pos = klaim[\"nominal_klaim_clip\"] > 0\n",
    "if pos.any():\n",
    "    low_q  = klaim.loc[pos, \"nominal_klaim_clip\"].quantile(0.005)\n",
    "    high_q = klaim.loc[pos, \"nominal_klaim_clip\"].quantile(0.995)\n",
    "    klaim.loc[pos, \"nominal_klaim_clip\"] = klaim.loc[pos, \"nominal_klaim_clip\"].clip(low_q, high_q)\n",
    "\n",
    "# =============================\n",
    "# MERGE\n",
    "# =============================\n",
    "df = klaim.merge(polis, on=\"nomor_polis\", how=\"left\")\n",
    "\n",
    "# =============================\n",
    "# SERVICE MONTH\n",
    "# =============================\n",
    "df[\"year_month\"] = df[service_col].dt.to_period(\"M\")\n",
    "\n",
    "min_m = df[\"year_month\"].min()\n",
    "max_m = df[\"year_month\"].max()\n",
    "all_months = pd.period_range(min_m, max_m, freq=\"M\")\n",
    "\n",
    "# ============================================================\n",
    "# EXPOSURE OPTIONS\n",
    "# ============================================================\n",
    "EXPOSURE_MODE = \"inforce\"  # \"claimant\" or \"inforce\"\n",
    "\n",
    "# claimant exposure: unique policies that claim in that month\n",
    "expo_claimant = (\n",
    "    df.groupby(\"year_month\")[\"nomor_polis\"].nunique()\n",
    "      .reindex(all_months, fill_value=0)\n",
    "      .rename(\"exposure_claimant\")\n",
    "      .rename_axis(\"year_month\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# inforce exposure: cumulative started policies (no end date available)\n",
    "start_col = None\n",
    "for c in [\"tanggal_efektif_polis\", \"tanggal_mulai_polis\", \"tanggal_mulai\"]:\n",
    "    if c in polis.columns:\n",
    "        start_col = c\n",
    "        break\n",
    "\n",
    "if start_col is not None:\n",
    "    p = polis[[\"nomor_polis\", start_col]].dropna(subset=[start_col]).copy()\n",
    "    p[\"start_m\"] = p[start_col].dt.to_period(\"M\")\n",
    "\n",
    "    base = p.loc[p[\"start_m\"] < min_m, \"nomor_polis\"].nunique()\n",
    "    inc = p.loc[p[\"start_m\"] >= min_m].groupby(\"start_m\")[\"nomor_polis\"].nunique()\n",
    "\n",
    "    expo_inforce = (\n",
    "        (base + inc.reindex(all_months, fill_value=0).cumsum())\n",
    "        .rename(\"exposure_inforce\")\n",
    "        .rename_axis(\"year_month\")\n",
    "        .reset_index()\n",
    "    )\n",
    "else:\n",
    "    expo_inforce = expo_claimant[[\"year_month\"]].copy()\n",
    "    expo_inforce[\"exposure_inforce\"] = 0\n",
    "\n",
    "expo = expo_claimant.merge(expo_inforce, on=\"year_month\", how=\"left\")\n",
    "\n",
    "# choose exposure with fallback safety\n",
    "expo[\"exposure\"] = np.where(EXPOSURE_MODE == \"inforce\", expo[\"exposure_inforce\"], expo[\"exposure_claimant\"])\n",
    "# if inforce is mostly 0 (bad parsing / missing), fallback to claimant\n",
    "if (EXPOSURE_MODE == \"inforce\") and (expo[\"exposure\"].sum() == 0):\n",
    "    expo[\"exposure\"] = expo[\"exposure_claimant\"]\n",
    "\n",
    "# merge exposure into df (keperluan stage lain)\n",
    "df = df.merge(expo[[\"year_month\", \"exposure\"]], on=\"year_month\", how=\"left\")\n",
    "df[\"active_policies\"] = df[\"exposure\"]\n",
    "\n",
    "# ============================================================\n",
    "# MONTHLY CORE TABLE (complete months)\n",
    "# target total_claim MUST be RAW nominal\n",
    "# ============================================================\n",
    "freq_col = claim_id_col if claim_id_col is not None else \"nomor_polis\"\n",
    "\n",
    "monthly_core = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(\n",
    "          frequency=(freq_col, \"count\"),\n",
    "          total_claim=(nom_col, \"sum\")\n",
    "      )\n",
    "      .reindex(all_months, fill_value=0)\n",
    "      .rename_axis(\"year_month\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "monthly = monthly_core.merge(expo[[\"year_month\", \"exposure\"]], on=\"year_month\", how=\"left\")\n",
    "\n",
    "monthly[\"severity\"] = monthly[\"total_claim\"] / monthly[\"frequency\"].replace(0, np.nan)\n",
    "monthly[\"claim_rate\"] = monthly[\"frequency\"] / monthly[\"exposure\"].replace(0, np.nan)\n",
    "\n",
    "# ============================================================\n",
    "# LOG FEATURES\n",
    "# ============================================================\n",
    "monthly[\"log_total\"] = np.log1p(monthly[\"total_claim\"])\n",
    "monthly[\"log_freq\"]  = np.log1p(monthly[\"frequency\"])\n",
    "monthly[\"log_sev\"]   = np.log1p(monthly[\"severity\"])\n",
    "monthly[\"log_rate\"]  = np.log1p(monthly[\"claim_rate\"])\n",
    "\n",
    "# ============================================================\n",
    "# VOLATILITY\n",
    "# ============================================================\n",
    "monthly[\"roll6\"] = monthly[\"total_claim\"].rolling(6, min_periods=3).mean()\n",
    "monthly[\"std6\"]  = monthly[\"total_claim\"].rolling(6, min_periods=3).std()\n",
    "monthly[\"vol_ratio\"] = monthly[\"std6\"] / monthly[\"roll6\"]\n",
    "monthly[\"high_vol_regime\"] = (monthly[\"vol_ratio\"] > monthly[\"vol_ratio\"].median()).astype(int)\n",
    "\n",
    "# ============================================================\n",
    "# TIME FEATURES\n",
    "# ============================================================\n",
    "monthly[\"month\"] = monthly[\"year_month\"].dt.month\n",
    "monthly[\"month_sin\"] = np.sin(2*np.pi*monthly[\"month\"]/12)\n",
    "monthly[\"month_cos\"] = np.cos(2*np.pi*monthly[\"month\"]/12)\n",
    "monthly[\"month_index\"] = np.arange(len(monthly))\n",
    "\n",
    "# ============================================================\n",
    "# SAFE LAGS\n",
    "# ============================================================\n",
    "for col in [\"log_total\", \"log_freq\", \"log_sev\", \"log_rate\"]:\n",
    "    monthly[f\"{col}_lag1\"] = monthly[col].shift(1)\n",
    "    monthly[f\"{col}_lag2\"] = monthly[col].shift(2)\n",
    "    monthly[f\"{col}_lag3\"] = monthly[col].shift(3)\n",
    "    monthly[f\"{col}_roll3\"] = monthly[col].shift(1).rolling(3).mean()\n",
    "\n",
    "monthly = monthly.dropna().reset_index(drop=True)\n",
    "\n",
    "# ============================================================\n",
    "# FINAL CHECK\n",
    "# ============================================================\n",
    "print(\"SERVICE_COL:\", service_col)\n",
    "print(\"EXPOSURE_MODE:\", EXPOSURE_MODE)\n",
    "print(\"Policy start col:\", start_col)\n",
    "print(\"Frequency source:\", freq_col)\n",
    "print(\"Monthly shape:\", monthly.shape)\n",
    "print(\"Unique months:\", monthly[\"year_month\"].nunique())\n",
    "print(\"Exposure min/max:\", float(monthly[\"exposure\"].min()), float(monthly[\"exposure\"].max()))\n",
    "print(\"Total_claim min/max:\", float(monthly[\"total_claim\"].min()), float(monthly[\"total_claim\"].max()))\n",
    "print(\"\\nSTAGE 1 v4 â€” READY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cee73e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T01:56:02.682327Z",
     "iopub.status.busy": "2026-02-21T01:56:02.681881Z",
     "iopub.status.idle": "2026-02-21T01:56:02.699991Z",
     "shell.execute_reply": "2026-02-21T01:56:02.698680Z"
    },
    "papermill": {
     "duration": 0.026923,
     "end_time": "2026-02-21T01:56:02.702608",
     "exception": false,
     "start_time": "2026-02-21T01:56:02.675685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year_month  frequency  exposure  freq_per_exposure\n",
      "6     2024-10        274      4096           0.066895\n",
      "7     2024-11        270      4096           0.065918\n",
      "8     2024-12        238      4096           0.058105\n",
      "9     2025-01        216      4096           0.052734\n",
      "10    2025-02        246      4096           0.060059\n",
      "11    2025-03        230      4096           0.056152\n",
      "12    2025-04        208      4096           0.050781\n",
      "13    2025-05        239      4096           0.058350\n",
      "14    2025-06        234      4096           0.057129\n",
      "15    2025-07        264      4096           0.064453\n",
      "freq_per_exposure min/max: 0.05078125 0.06689453125\n"
     ]
    }
   ],
   "source": [
    "tmp = monthly.copy()\n",
    "tmp[\"freq_per_exposure\"] = tmp[\"frequency\"] / tmp[\"exposure\"]\n",
    "print(tmp[[\"year_month\",\"frequency\",\"exposure\",\"freq_per_exposure\"]].tail(10))\n",
    "print(\"freq_per_exposure min/max:\",\n",
    "      tmp[\"freq_per_exposure\"].min(),\n",
    "      tmp[\"freq_per_exposure\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a118955f",
   "metadata": {
    "papermill": {
     "duration": 0.004724,
     "end_time": "2026-02-21T01:56:02.711848",
     "exception": false,
     "start_time": "2026-02-21T01:56:02.707124",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TIME-SERIES DATASET ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eeaac81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T01:56:02.723404Z",
     "iopub.status.busy": "2026-02-21T01:56:02.722286Z",
     "iopub.status.idle": "2026-02-21T01:56:02.851247Z",
     "shell.execute_reply": "2026-02-21T01:56:02.849818Z"
    },
    "papermill": {
     "duration": 0.137741,
     "end_time": "2026-02-21T01:56:02.853836",
     "exception": false,
     "start_time": "2026-02-21T01:56:02.716095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPACT PANEL SHAPE: (414, 29)\n",
      "Unique segments: 41\n",
      "Columns: 29\n",
      "\n",
      "STAGE 2 â€” ELITE SEGMENT PANEL READY\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 2 â€” ELITE SEGMENT PANEL (SAFE VERSION)\n",
    "# No KeyError â€¢ Auto-create missing columns â€¢ Short series safe\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ ENSURE REQUIRED SEGMENT COLUMNS EXIST\n",
    "# ============================================================\n",
    "\n",
    "# Care Type\n",
    "if \"care_type\" not in df.columns:\n",
    "    if \"inpatient_outpatient\" in df.columns:\n",
    "        df[\"care_type\"] = (\n",
    "            df[\"inpatient_outpatient\"]\n",
    "            .astype(str)\n",
    "            .str.upper()\n",
    "            .str.strip()\n",
    "        )\n",
    "    else:\n",
    "        df[\"care_type\"] = \"UNKNOWN\"\n",
    "\n",
    "df[\"care_type\"] = df[\"care_type\"].fillna(\"UNKNOWN\")\n",
    "\n",
    "\n",
    "# Cashless\n",
    "if \"is_cashless\" not in df.columns:\n",
    "    if \"reimburse_cashless\" in df.columns:\n",
    "        rc = df[\"reimburse_cashless\"].astype(str).str.upper().str.strip()\n",
    "        df[\"is_cashless\"] = rc.eq(\"C\").astype(int)\n",
    "    else:\n",
    "        df[\"is_cashless\"] = 0\n",
    "\n",
    "\n",
    "# RS Bucket\n",
    "if \"rs_bucket\" not in df.columns:\n",
    "    if \"lokasi_rs\" in df.columns:\n",
    "        loc = df[\"lokasi_rs\"].astype(str).str.upper().str.strip()\n",
    "        df[\"rs_bucket\"] = np.select(\n",
    "            [\n",
    "                loc.eq(\"INDONESIA\"),\n",
    "                loc.eq(\"SINGAPORE\"),\n",
    "                loc.eq(\"MALAYSIA\")\n",
    "            ],\n",
    "            [\"ID\",\"SG\",\"MY\"],\n",
    "            default=\"OTHER\"\n",
    "        )\n",
    "    else:\n",
    "        df[\"rs_bucket\"] = \"OTHER\"\n",
    "\n",
    "df[\"rs_bucket\"] = df[\"rs_bucket\"].fillna(\"OTHER\")\n",
    "\n",
    "\n",
    "# Plan Code\n",
    "if \"plan_code\" not in df.columns:\n",
    "    df[\"plan_code\"] = \"UNKNOWN\"\n",
    "\n",
    "df[\"plan_code\"] = df[\"plan_code\"].fillna(\"UNKNOWN\")\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ DEFINE SEGMENT COLUMNS\n",
    "# ============================================================\n",
    "\n",
    "seg_cols = [\"plan_code\",\"care_type\",\"is_cashless\",\"rs_bucket\"]\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ BUILD SEGMENT MONTHLY\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly = (\n",
    "    df.groupby([\"year_month\"] + seg_cols)\n",
    "      .agg(\n",
    "          frequency=(\"nomor_polis\",\"count\"),\n",
    "          total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\"),\n",
    "          exposure=(\"nomor_polis\",\"nunique\")\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(seg_cols + [\"year_month\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ TARGETS\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"severity\"] = (\n",
    "    seg_monthly[\"total_claim\"] /\n",
    "    seg_monthly[\"frequency\"].replace(0, np.nan)\n",
    ")\n",
    "\n",
    "seg_monthly[\"log_total\"] = np.log1p(seg_monthly[\"total_claim\"])\n",
    "seg_monthly[\"log_freq\"]  = np.log1p(seg_monthly[\"frequency\"])\n",
    "seg_monthly[\"log_sev\"]   = np.log1p(seg_monthly[\"severity\"])\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ CALENDAR\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"month\"] = seg_monthly[\"year_month\"].dt.month\n",
    "seg_monthly[\"month_sin\"] = np.sin(2*np.pi*seg_monthly[\"month\"]/12)\n",
    "seg_monthly[\"month_cos\"] = np.cos(2*np.pi*seg_monthly[\"month\"]/12)\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ LAGS (STRICT NO LEAKAGE)\n",
    "# ============================================================\n",
    "\n",
    "for col in [\"log_total\",\"log_freq\",\"log_sev\"]:\n",
    "    \n",
    "    seg_monthly[f\"{col}_lag1\"] = \\\n",
    "        seg_monthly.groupby(seg_cols)[col].shift(1)\n",
    "    \n",
    "    seg_monthly[f\"{col}_lag2\"] = \\\n",
    "        seg_monthly.groupby(seg_cols)[col].shift(2)\n",
    "    \n",
    "    seg_monthly[f\"{col}_lag3\"] = \\\n",
    "        seg_monthly.groupby(seg_cols)[col].shift(3)\n",
    "\n",
    "    seg_monthly[f\"{col}_roll3\"] = \\\n",
    "        seg_monthly.groupby(seg_cols)[col] \\\n",
    "        .transform(lambda x: x.shift(1).rolling(3).mean())\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ MOMENTUM\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"momentum_total\"] = (\n",
    "    seg_monthly[\"log_total_lag1\"] -\n",
    "    seg_monthly[\"log_total_lag2\"]\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ SEGMENT WEIGHT\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"seg_weight\"] = (\n",
    "    seg_monthly[\"frequency\"] /\n",
    "    seg_monthly.groupby(\"year_month\")[\"frequency\"].transform(\"sum\")\n",
    ").fillna(0)\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ SAFE TRAIN WINDOW\n",
    "# ============================================================\n",
    "\n",
    "seg_model = seg_monthly[\n",
    "    seg_monthly[\"log_total_lag3\"].notna()\n",
    "].reset_index(drop=True)\n",
    "\n",
    "seg_model = seg_model.fillna(0)\n",
    "\n",
    "# ============================================================\n",
    "# FINAL CHECK\n",
    "# ============================================================\n",
    "\n",
    "print(\"COMPACT PANEL SHAPE:\", seg_model.shape)\n",
    "print(\"Unique segments:\", seg_model[seg_cols].drop_duplicates().shape[0])\n",
    "print(\"Columns:\", len(seg_model.columns))\n",
    "print(\"\\nSTAGE 2 â€” ELITE SEGMENT PANEL READY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d84d3e1",
   "metadata": {
    "papermill": {
     "duration": 0.004596,
     "end_time": "2026-02-21T01:56:02.863041",
     "exception": false,
     "start_time": "2026-02-21T01:56:02.858445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MODEL DEVELOPMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfd5ada2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T01:56:02.874966Z",
     "iopub.status.busy": "2026-02-21T01:56:02.873998Z",
     "iopub.status.idle": "2026-02-21T01:57:00.213774Z",
     "shell.execute_reply": "2026-02-21T01:57:00.212586Z"
    },
    "papermill": {
     "duration": 57.354148,
     "end_time": "2026-02-21T01:57:00.221592",
     "exception": false,
     "start_time": "2026-02-21T01:56:02.867444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Horizon months used : 5\n",
      "Best Config:\n",
      "  wt_total=0.85 (ETS weight), anchor_total=median\n",
      "  wt_freq =0.2 (ETS weight), anchor_freq =mean\n",
      "------------------------------\n",
      "STAGE 3 v17 MAPE Frequency : 5.1557\n",
      "STAGE 3 v17 MAPE Total     : 7.9753\n",
      "STAGE 3 v17 MAPE Severity  : 4.7684\n",
      "Estimated Score            : 5.9665\n",
      "==============================\n",
      "\n",
      "Preview last horizon months:\n",
      "   year_month  frequency   total_claim      severity  pred_frequency  \\\n",
      "14    2025-03        230  1.367924e+10  5.947496e+07      234.031716   \n",
      "15    2025-04        208  1.116425e+10  5.367427e+07      232.851773   \n",
      "16    2025-05        239  1.222680e+10  5.115814e+07      237.225688   \n",
      "17    2025-06        234  1.337312e+10  5.715008e+07      234.888808   \n",
      "18    2025-07        264  1.369923e+10  5.189101e+07      235.077202   \n",
      "\n",
      "      pred_total  pred_severity  \n",
      "14  1.224504e+10   5.232214e+07  \n",
      "15  1.224868e+10   5.260289e+07  \n",
      "16  1.222798e+10   5.154577e+07  \n",
      "17  1.221086e+10   5.198572e+07  \n",
      "18  1.219531e+10   5.187790e+07  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 3 v17 â€” KAGGLE-MATCH VALIDATION (AUTO-TUNED SHRINK)\n",
    "# - Horizon = unique months in sample_submission (usually 5)\n",
    "# - Predict TOTAL & FREQ directly (ETS on log1p), derive SEVERITY\n",
    "# - True recursive (refit each step on simulated history)\n",
    "# - Auto grid-search shrink weights + anchor type (mean/median)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\n",
    "sample_sub = pd.read_csv(BASE_PATH + \"sample_submission.csv\")\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "# ==============================\n",
    "# BUILD MONTHLY (consistent with Stage 1 v3)\n",
    "# ==============================\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(\n",
    "          frequency=(\"claim_id\",\"count\"),\n",
    "          total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\"),\n",
    "          exposure=(\"active_policies\",\"first\")\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(\"year_month\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "monthly[\"severity\"]   = monthly[\"total_claim\"] / monthly[\"frequency\"].replace(0, np.nan)\n",
    "monthly[\"claim_rate\"] = monthly[\"frequency\"] / monthly[\"exposure\"].replace(0, np.nan)\n",
    "\n",
    "# ==============================\n",
    "# HORIZON = months in sample_submission (Kaggle behavior)\n",
    "# ==============================\n",
    "sample_sub[\"year\"]  = sample_sub[\"id\"].str.split(\"_\").str[0]\n",
    "sample_sub[\"month\"] = sample_sub[\"id\"].str.split(\"_\").str[1]\n",
    "sample_sub[\"month_key\"] = sample_sub[\"year\"] + \"-\" + sample_sub[\"month\"]\n",
    "\n",
    "future_periods = (\n",
    "    pd.PeriodIndex(sample_sub[\"month_key\"], freq=\"M\")\n",
    "      .unique()\n",
    "      .sort_values()\n",
    ")\n",
    "\n",
    "H = int(len(future_periods))\n",
    "H = min(H, max(1, len(monthly) - 6))  # safety\n",
    "\n",
    "# ==============================\n",
    "# SIMULATOR (true recursive)\n",
    "# ==============================\n",
    "def simulate(train_df, H, wt_total, wt_freq, anchor_total=\"mean\", anchor_freq=\"mean\"):\n",
    "    sim_df = train_df.copy()\n",
    "\n",
    "    pred_total, pred_freq, pred_sev = [], [], []\n",
    "\n",
    "    for step in range(H):\n",
    "        hist = sim_df.copy()\n",
    "\n",
    "        # ---- TOTAL ETS on log1p ----\n",
    "        try:\n",
    "            mdl_t = ExponentialSmoothing(\n",
    "                np.log1p(hist[\"total_claim\"].astype(float)),\n",
    "                trend=\"add\",\n",
    "                damped_trend=True,\n",
    "                seasonal=None\n",
    "            ).fit()\n",
    "            total_fc = float(np.expm1(mdl_t.forecast(1).iloc[0]))\n",
    "        except:\n",
    "            total_fc = float(hist[\"total_claim\"].iloc[-1])\n",
    "\n",
    "        # anchor total\n",
    "        if anchor_total == \"median\":\n",
    "            total_anchor = float(hist[\"total_claim\"].tail(3).median())\n",
    "        else:\n",
    "            total_anchor = float(hist[\"total_claim\"].tail(3).mean())\n",
    "\n",
    "        total_pred = wt_total * total_fc + (1 - wt_total) * total_anchor\n",
    "        total_pred = max(float(total_pred), 1.0)\n",
    "\n",
    "        # ---- FREQ ETS on log1p ----\n",
    "        try:\n",
    "            mdl_f = ExponentialSmoothing(\n",
    "                np.log1p(hist[\"frequency\"].astype(float)),\n",
    "                trend=\"add\",\n",
    "                damped_trend=True,\n",
    "                seasonal=None\n",
    "            ).fit()\n",
    "            freq_fc = float(np.expm1(mdl_f.forecast(1).iloc[0]))\n",
    "        except:\n",
    "            freq_fc = float(hist[\"frequency\"].iloc[-1])\n",
    "\n",
    "        # anchor freq\n",
    "        if anchor_freq == \"median\":\n",
    "            freq_anchor = float(hist[\"frequency\"].tail(3).median())\n",
    "        else:\n",
    "            freq_anchor = float(hist[\"frequency\"].tail(3).mean())\n",
    "\n",
    "        freq_pred = wt_freq * freq_fc + (1 - wt_freq) * freq_anchor\n",
    "        freq_pred = max(float(freq_pred), 1.0)\n",
    "\n",
    "        sev_pred = total_pred / freq_pred\n",
    "\n",
    "        pred_total.append(total_pred)\n",
    "        pred_freq.append(freq_pred)\n",
    "        pred_sev.append(sev_pred)\n",
    "\n",
    "        # ---- append recursive row (keep year_month progressing) ----\n",
    "        last_period = hist[\"year_month\"].iloc[-1]\n",
    "        next_period = last_period + 1\n",
    "        exposure_next = float(hist[\"exposure\"].iloc[-1]) if \"exposure\" in hist.columns else np.nan\n",
    "\n",
    "        sim_df = pd.concat([sim_df, pd.DataFrame([{\n",
    "            \"year_month\": next_period,\n",
    "            \"frequency\": freq_pred,\n",
    "            \"total_claim\": total_pred,\n",
    "            \"exposure\": exposure_next,\n",
    "            \"severity\": sev_pred,\n",
    "            \"claim_rate\": (freq_pred / exposure_next) if (exposure_next and exposure_next > 0) else np.nan\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    return pred_total, pred_freq, pred_sev\n",
    "\n",
    "# ==============================\n",
    "# SPLIT (Kaggle-match horizon)\n",
    "# ==============================\n",
    "train = monthly.iloc[:-H].copy()\n",
    "valid = monthly.iloc[-H:].copy()\n",
    "\n",
    "# ==============================\n",
    "# AUTO SEARCH (small grid, fast)\n",
    "# ==============================\n",
    "wt_total_grid = [0.35, 0.45, 0.55, 0.60, 0.65, 0.75, 0.85]\n",
    "wt_freq_grid  = [0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80]\n",
    "\n",
    "best = {\n",
    "    \"score\": 1e18,\n",
    "    \"params\": None,\n",
    "    \"detail\": None\n",
    "}\n",
    "\n",
    "for wt_t in wt_total_grid:\n",
    "    for wt_f in wt_freq_grid:\n",
    "        for a_t in [\"mean\", \"median\"]:\n",
    "            for a_f in [\"mean\", \"median\"]:\n",
    "\n",
    "                pt, pf, ps = simulate(train, H, wt_t, wt_f, a_t, a_f)\n",
    "\n",
    "                mf = mape(valid[\"frequency\"], pf)\n",
    "                mt = mape(valid[\"total_claim\"], pt)\n",
    "                ms = mape(valid[\"severity\"], ps)\n",
    "                avg = float(np.nanmean([mf, mt, ms]))\n",
    "\n",
    "                if avg < best[\"score\"]:\n",
    "                    best[\"score\"] = avg\n",
    "                    best[\"params\"] = (wt_t, wt_f, a_t, a_f)\n",
    "                    best[\"detail\"] = (mf, mt, ms)\n",
    "\n",
    "# ==============================\n",
    "# RUN BEST + REPORT\n",
    "# ==============================\n",
    "wt_t, wt_f, a_t, a_f = best[\"params\"]\n",
    "pt, pf, ps = simulate(train, H, wt_t, wt_f, a_t, a_f)\n",
    "\n",
    "mf, mt, ms = best[\"detail\"]\n",
    "avg = best[\"score\"]\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(f\"Horizon months used : {H}\")\n",
    "print(\"Best Config:\")\n",
    "print(f\"  wt_total={wt_t} (ETS weight), anchor_total={a_t}\")\n",
    "print(f\"  wt_freq ={wt_f} (ETS weight), anchor_freq ={a_f}\")\n",
    "print(\"------------------------------\")\n",
    "print(\"STAGE 3 v17 MAPE Frequency :\", round(mf, 4))\n",
    "print(\"STAGE 3 v17 MAPE Total     :\", round(mt, 4))\n",
    "print(\"STAGE 3 v17 MAPE Severity  :\", round(ms, 4))\n",
    "print(\"Estimated Score            :\", round(avg, 4))\n",
    "print(\"==============================\")\n",
    "\n",
    "check = valid[[\"year_month\",\"frequency\",\"total_claim\",\"severity\"]].copy()\n",
    "check[\"pred_frequency\"] = pf\n",
    "check[\"pred_total\"] = pt\n",
    "check[\"pred_severity\"] = ps\n",
    "print(\"\\nPreview last horizon months:\")\n",
    "print(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159148f8",
   "metadata": {
    "papermill": {
     "duration": 0.007675,
     "end_time": "2026-02-21T01:57:00.236211",
     "exception": false,
     "start_time": "2026-02-21T01:57:00.228536",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TOTAL CLAIM OPTIMIZATION & VALIDATION, OPTUNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bf83a8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T01:57:00.255197Z",
     "iopub.status.busy": "2026-02-21T01:57:00.254290Z",
     "iopub.status.idle": "2026-02-21T02:09:55.117873Z",
     "shell.execute_reply": "2026-02-21T02:09:55.116820Z"
    },
    "papermill": {
     "duration": 774.876643,
     "end_time": "2026-02-21T02:09:55.120512",
     "exception": false,
     "start_time": "2026-02-21T01:57:00.243869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N months: 19 | Horizon H: 5 | Has exposure: True\n",
      "CV train_ends: [12, 13, 14] | #splits: 3\n",
      "Split weights: [0.05, 0.15, 0.8]\n",
      "Baseline CV %: 7.2792\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f356977b03439fa8283216a3840e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Horizon: 5 | Splits: [12, 13, 14] | weights: [0.05, 0.15, 0.8]\n",
      "Best Params: {'k_anchor': 4, 'anchor_rate': 'mean', 'anchor_sev': 'mean', 'wt_rate': 0.3332890181752667, 'wt_sev': 0.5501072463530577, 'beta': 0.8005225975816733, 'damped': True, 'init_method': 'estimated', 'capR_low': 0.7818972835090717, 'capR_high': 1.3058960367487729, 'capS_low': 0.6976922342421228, 'capS_high': 1.1536381686265984}\n",
      "CV Best %  : 7.0121\n",
      "==============================\n",
      "\n",
      "Per-split metrics (%):\n",
      "               avg  mape_total  mape_freq  mape_sev  stab_pen\n",
      "train_end                                                    \n",
      "12         12.7698     15.7394     9.1449   13.4252    1.0781\n",
      "13         11.5313     13.9288     4.4085   16.2565    0.9767\n",
      "14          5.7701      6.4797     6.2969    4.5335    1.4895\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 4 v23 â€” RATE+SEVERITY (EXPOSURE-AWARE) + ETS ENSEMBLE + HEAVY RECENT CV\n",
    "# Target: turunin Total & Severity tanpa overfit (series cuma 19 bulan)\n",
    "# - Forecast claim_rate = freq/exposure  (lebih stabil)\n",
    "# - Forecast severity = total/freq       (lebih stabil)\n",
    "# - Reconstruct: freq = rate*exposure, total = freq*sev\n",
    "# - ETS ensemble: trend add + trend none\n",
    "# - Anchor shrink + clamp ratio vs anchor\n",
    "# - Walk-forward CV: bobot lebih berat split terbaru (Kaggle-like)\n",
    "# ============================================================\n",
    "\n",
    "!pip install -q optuna statsmodels\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\n",
    "sample_sub = pd.read_csv(BASE_PATH + \"sample_submission.csv\")\n",
    "\n",
    "# ------------------------------\n",
    "# MAPE (fraction)\n",
    "# ------------------------------\n",
    "def mape_frac(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred) & (y_true != 0)\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return float(np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])))\n",
    "\n",
    "# ------------------------------\n",
    "# BUILD MONTHLY\n",
    "# ------------------------------\n",
    "assert \"df\" in globals(), \"Variabel df belum ada.\"\n",
    "assert \"year_month\" in df.columns, \"df['year_month'] belum ada. Buat dulu: df['year_month']=tanggal.dt.to_period('M')\"\n",
    "\n",
    "has_exposure = \"active_policies\" in df.columns\n",
    "\n",
    "agg_dict = {\n",
    "    \"frequency\": (\"claim_id\",\"count\"),\n",
    "    \"total_claim\": (\"nominal_klaim_yang_disetujui\",\"sum\"),\n",
    "}\n",
    "if has_exposure:\n",
    "    agg_dict[\"exposure\"] = (\"active_policies\",\"first\")\n",
    "\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(**agg_dict)\n",
    "      .reset_index()\n",
    "      .sort_values(\"year_month\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ensure Period[M]\n",
    "if not isinstance(monthly.loc[0, \"year_month\"], pd.Period):\n",
    "    monthly[\"year_month\"] = pd.PeriodIndex(monthly[\"year_month\"], freq=\"M\")\n",
    "\n",
    "monthly[\"frequency\"]   = monthly[\"frequency\"].astype(float).clip(lower=1.0)\n",
    "monthly[\"total_claim\"] = monthly[\"total_claim\"].astype(float).clip(lower=1.0)\n",
    "\n",
    "if not has_exposure:\n",
    "    # fallback exposure: pakai rata-rata frequency * 10 (sekadar scaling stabil)\n",
    "    monthly[\"exposure\"] = float(np.nanmean(monthly[\"frequency\"])) * 10.0\n",
    "\n",
    "monthly[\"exposure\"] = monthly[\"exposure\"].astype(float).clip(lower=1.0)\n",
    "\n",
    "# components\n",
    "monthly[\"severity\"]   = (monthly[\"total_claim\"] / monthly[\"frequency\"]).astype(float).clip(lower=1e-9)\n",
    "monthly[\"claim_rate\"] = (monthly[\"frequency\"]   / monthly[\"exposure\"]).astype(float).clip(lower=1e-12)\n",
    "\n",
    "# ------------------------------\n",
    "# Horizon from sample_submission\n",
    "# ------------------------------\n",
    "sample_sub[\"year\"]  = sample_sub[\"id\"].str.split(\"_\").str[0]\n",
    "sample_sub[\"month\"] = sample_sub[\"id\"].str.split(\"_\").str[1]\n",
    "sample_sub[\"month_key\"] = sample_sub[\"year\"] + \"-\" + sample_sub[\"month\"]\n",
    "\n",
    "future_periods = (\n",
    "    pd.PeriodIndex(sample_sub[\"month_key\"], freq=\"M\")\n",
    "      .unique()\n",
    "      .sort_values()\n",
    ")\n",
    "\n",
    "H = int(len(future_periods))\n",
    "N = len(monthly)\n",
    "H = min(H, max(1, N - 12))  # N=19 -> H=5 tetap\n",
    "print(\"N months:\", N, \"| Horizon H:\", H, \"| Has exposure:\", has_exposure)\n",
    "\n",
    "# ------------------------------\n",
    "# Walk-forward splits\n",
    "# ------------------------------\n",
    "train_end_last = N - H\n",
    "train_ends = list(range(max(12, train_end_last - 2), train_end_last + 1))\n",
    "train_ends = [te for te in train_ends if (te + H) <= N and te >= 12]\n",
    "print(\"CV train_ends:\", train_ends, \"| #splits:\", len(train_ends))\n",
    "\n",
    "# Bobot super fokus split terbaru (ini yang bikin mendekati Stage 3)\n",
    "if len(train_ends) == 1:\n",
    "    split_w = np.array([1.0])\n",
    "elif len(train_ends) == 2:\n",
    "    split_w = np.array([0.15, 0.85], dtype=float)\n",
    "else:\n",
    "    # 3 split: makin ekstrem ke yang terakhir\n",
    "    split_w = np.array([0.05, 0.15, 0.80], dtype=float)\n",
    "split_w = split_w / split_w.sum()\n",
    "print(\"Split weights:\", split_w.round(3).tolist())\n",
    "\n",
    "# ------------------------------\n",
    "# ETS 1-step on LOG series\n",
    "# ------------------------------\n",
    "def ets_1step_log(x_log: pd.Series, trend, damped, init_method):\n",
    "    try:\n",
    "        m = ExponentialSmoothing(\n",
    "            x_log,\n",
    "            trend=trend,\n",
    "            damped_trend=(damped if trend is not None else False),\n",
    "            seasonal=None,\n",
    "            initialization_method=init_method\n",
    "        ).fit()\n",
    "        return float(m.forecast(1).iloc[0])\n",
    "    except:\n",
    "        return float(x_log.iloc[-1])\n",
    "\n",
    "def anchor_level(x_level: pd.Series, k: int, how: str):\n",
    "    tail = np.asarray(x_level.tail(k), dtype=float)\n",
    "    if how == \"median\":\n",
    "        return float(np.median(tail))\n",
    "    return float(np.mean(tail))\n",
    "\n",
    "# ------------------------------\n",
    "# One split TRUE RECURSIVE\n",
    "# ------------------------------\n",
    "def run_split(monthly_all: pd.DataFrame, train_end: int, H: int, P: dict):\n",
    "    train = monthly_all.iloc[:train_end].copy().reset_index(drop=True)\n",
    "    valid = monthly_all.iloc[train_end:train_end+H].copy().reset_index(drop=True)\n",
    "    if len(valid) < H or len(train) < 12:\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    sim = train.copy()\n",
    "\n",
    "    pred_F, pred_T, pred_S = [], [], []\n",
    "    pen = []\n",
    "\n",
    "    for step in range(H):\n",
    "        k = int(P[\"k_anchor\"])\n",
    "\n",
    "        # exposure next: anggap stabil = last known\n",
    "        exp_next = float(sim[\"exposure\"].iloc[-1])\n",
    "        exp_next = max(1.0, exp_next)\n",
    "\n",
    "        # anchors (LEVEL)\n",
    "        aR = anchor_level(sim[\"claim_rate\"], k, P[\"anchor_rate\"])\n",
    "        aS = anchor_level(sim[\"severity\"],   k, P[\"anchor_sev\"])\n",
    "\n",
    "        # build log series for ETS\n",
    "        lr = np.log(sim[\"claim_rate\"].astype(float).clip(lower=1e-12))\n",
    "        ls = np.log(sim[\"severity\"].astype(float).clip(lower=1e-12))\n",
    "\n",
    "        # ETS ensemble for rate\n",
    "        lr_add  = ets_1step_log(lr, trend=\"add\",  damped=P[\"damped\"], init_method=P[\"init_method\"])\n",
    "        lr_none = ets_1step_log(lr, trend=None,   damped=False,       init_method=P[\"init_method\"])\n",
    "        lr_hat  = P[\"beta\"]*lr_add + (1-P[\"beta\"])*lr_none\n",
    "        r_fc    = float(np.exp(lr_hat))\n",
    "\n",
    "        # ETS ensemble for severity\n",
    "        ls_add  = ets_1step_log(ls, trend=\"add\",  damped=P[\"damped\"], init_method=P[\"init_method\"])\n",
    "        ls_none = ets_1step_log(ls, trend=None,   damped=False,       init_method=P[\"init_method\"])\n",
    "        ls_hat  = P[\"beta\"]*ls_add + (1-P[\"beta\"])*ls_none\n",
    "        s_fc    = float(np.exp(ls_hat))\n",
    "\n",
    "        # shrink to anchor (LEVEL) â€” gaya Stage 3\n",
    "        r_pred = P[\"wt_rate\"]*r_fc + (1-P[\"wt_rate\"])*aR\n",
    "        s_pred = P[\"wt_sev\"] *s_fc + (1-P[\"wt_sev\"] )*aS\n",
    "\n",
    "        # clamp ratio vs anchor (anti spike)\n",
    "        r_pred = float(np.clip(r_pred, aR*P[\"capR_low\"], aR*P[\"capR_high\"]))\n",
    "        s_pred = float(np.clip(s_pred, aS*P[\"capS_low\"], aS*P[\"capS_high\"]))\n",
    "\n",
    "        # reconstruct\n",
    "        f_pred = float(max(1.0, np.round(r_pred * exp_next)))\n",
    "        t_pred = float(max(1.0, f_pred * s_pred))\n",
    "        s_pred = float(max(1e-9, t_pred / f_pred))\n",
    "\n",
    "        pred_F.append(f_pred)\n",
    "        pred_T.append(t_pred)\n",
    "        pred_S.append(s_pred)\n",
    "\n",
    "        # stability penalty ringan: drift vs anchor\n",
    "        # (anchor level for freq/total implied)\n",
    "        aF = float(max(1.0, np.round(aR * exp_next)))\n",
    "        aT = float(max(1.0, aF * aS))\n",
    "        pen_f = abs(f_pred - aF) / (abs(aF) + 1e-9)\n",
    "        pen_t = abs(t_pred - aT) / (abs(aT) + 1e-9)\n",
    "        pen.append(0.5*(pen_f + pen_t))\n",
    "\n",
    "        # append recursive\n",
    "        sim = pd.concat([sim, pd.DataFrame([{\n",
    "            \"year_month\": sim[\"year_month\"].iloc[-1] + 1,\n",
    "            \"exposure\": exp_next,\n",
    "            \"claim_rate\": float(max(1e-12, f_pred/exp_next)),\n",
    "            \"severity\": s_pred,\n",
    "            \"frequency\": f_pred,\n",
    "            \"total_claim\": t_pred\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    # targets\n",
    "    yF = valid[\"frequency\"].astype(float).values\n",
    "    yT = valid[\"total_claim\"].astype(float).values\n",
    "    yS = (valid[\"total_claim\"].astype(float).values /\n",
    "          np.clip(valid[\"frequency\"].astype(float).values, 1.0, np.inf))\n",
    "\n",
    "    mf = mape_frac(yF, pred_F)\n",
    "    mt = mape_frac(yT, pred_T)\n",
    "    ms = mape_frac(yS, pred_S)\n",
    "    avg = float(np.nanmean([mf, mt, ms]))\n",
    "    stab = float(np.mean(pen)) if len(pen) else np.nan\n",
    "    return avg, mt, mf, ms, stab\n",
    "\n",
    "# ------------------------------\n",
    "# Baseline: dekat Stage 3 (rate kuat anchor, sev ETS dominan)\n",
    "# ------------------------------\n",
    "P0 = dict(\n",
    "    k_anchor=3,\n",
    "    anchor_rate=\"mean\",\n",
    "    anchor_sev=\"median\",\n",
    "    wt_rate=0.25,\n",
    "    wt_sev=0.85,\n",
    "    beta=0.75,\n",
    "    damped=True,\n",
    "    init_method=\"estimated\",\n",
    "    capR_low=0.80, capR_high=1.25,\n",
    "    capS_low=0.75, capS_high=1.35,\n",
    ")\n",
    "\n",
    "def baseline_score(P):\n",
    "    s = 0.0\n",
    "    for te, w in zip(train_ends, split_w):\n",
    "        avg, *_ = run_split(monthly, te, H, P)\n",
    "        s += w*avg\n",
    "    return float(s)\n",
    "\n",
    "print(\"Baseline CV %:\", round(baseline_score(P0)*100, 4))\n",
    "\n",
    "# ------------------------------\n",
    "# OPTUNA objective (weighted CV + very small penalty)\n",
    "# ------------------------------\n",
    "PEN_W = 0.02\n",
    "\n",
    "def objective(trial):\n",
    "    P = dict(\n",
    "        k_anchor=trial.suggest_int(\"k_anchor\", 2, 5),\n",
    "        anchor_rate=trial.suggest_categorical(\"anchor_rate\", [\"mean\",\"median\"]),\n",
    "        anchor_sev=trial.suggest_categorical(\"anchor_sev\", [\"mean\",\"median\"]),\n",
    "\n",
    "        wt_rate=trial.suggest_float(\"wt_rate\", 0.05, 0.55),\n",
    "        wt_sev=trial.suggest_float(\"wt_sev\",  0.55, 0.98),\n",
    "\n",
    "        beta=trial.suggest_float(\"beta\", 0.30, 0.95),  # ensemble weight (add vs none)\n",
    "        damped=trial.suggest_categorical(\"damped\", [False, True]),\n",
    "        init_method=trial.suggest_categorical(\"init_method\", [\"estimated\",\"heuristic\"]),\n",
    "\n",
    "        capR_low=trial.suggest_float(\"capR_low\", 0.70, 0.92),\n",
    "        capR_high=trial.suggest_float(\"capR_high\", 1.10, 1.35),\n",
    "        capS_low=trial.suggest_float(\"capS_low\", 0.65, 0.90),\n",
    "        capS_high=trial.suggest_float(\"capS_high\", 1.10, 1.50),\n",
    "    )\n",
    "\n",
    "    cv = 0.0\n",
    "    pen = 0.0\n",
    "    ok = 0\n",
    "    for te, w in zip(train_ends, split_w):\n",
    "        avg, mt, mf, ms, stab = run_split(monthly, te, H, P)\n",
    "        if np.isfinite(avg):\n",
    "            cv += w*avg\n",
    "            ok += 1\n",
    "        if np.isfinite(stab):\n",
    "            pen += w*stab\n",
    "    if ok == 0:\n",
    "        return 1e9\n",
    "    return float(cv + PEN_W*pen)\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "\n",
    "# enqueue baseline\n",
    "study.enqueue_trial(P0)\n",
    "\n",
    "study.optimize(objective, n_trials=700, show_progress_bar=True)\n",
    "\n",
    "bestP = study.best_params\n",
    "print(\"\\n==============================\")\n",
    "print(\"Horizon:\", H, \"| Splits:\", train_ends, \"| weights:\", split_w.round(3).tolist())\n",
    "print(\"Best Params:\", bestP)\n",
    "print(\"CV Best %  :\", round(study.best_value*100, 4))\n",
    "print(\"==============================\")\n",
    "\n",
    "# per-split\n",
    "rows = []\n",
    "for te in train_ends:\n",
    "    avg, mt, mf, ms, stab = run_split(monthly, te, H, bestP)\n",
    "    rows.append([te, avg, mt, mf, ms, stab])\n",
    "\n",
    "spl = pd.DataFrame(rows, columns=[\"train_end\",\"avg\",\"mape_total\",\"mape_freq\",\"mape_sev\",\"stab_pen\"])\n",
    "print(\"\\nPer-split metrics (%):\")\n",
    "print((spl.set_index(\"train_end\")[[\"avg\",\"mape_total\",\"mape_freq\",\"mape_sev\",\"stab_pen\"]]*100).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a8c94f",
   "metadata": {
    "papermill": {
     "duration": 0.007895,
     "end_time": "2026-02-21T02:09:55.136003",
     "exception": false,
     "start_time": "2026-02-21T02:09:55.128108",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TEST PREDICTION & KAGGLE SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7aae2b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T02:09:55.154094Z",
     "iopub.status.busy": "2026-02-21T02:09:55.153624Z",
     "iopub.status.idle": "2026-02-21T02:10:21.105165Z",
     "shell.execute_reply": "2026-02-21T02:10:21.104014Z"
    },
    "papermill": {
     "duration": 25.964382,
     "end_time": "2026-02-21T02:10:21.107828",
     "exception": false,
     "start_time": "2026-02-21T02:09:55.143446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N months: 19 | H: 5 | Has exposure: True\n",
      "CV train_ends: [12, 13, 14]\n",
      "\n",
      "Blend search results (fraction):\n",
      "    w_stage4  mean_avg  worst_avg\n",
      "10       1.0  0.100147   0.127236\n",
      "9        0.9  0.100175   0.128072\n",
      "8        0.8  0.100203   0.128911\n",
      "7        0.7  0.100232   0.129753\n",
      "6        0.6  0.100262   0.130598\n",
      "5        0.5  0.100385   0.131721\n",
      "4        0.4  0.100563   0.133011\n",
      "3        0.3  0.100741   0.134304\n",
      "\n",
      "Chosen w_stage4: 1.0 (w=1 pure Stage4, w=0 pure Stage3)\n",
      "NaN in submission: 0\n",
      "\n",
      "Preview future predictions:\n",
      "    period   pred_freq    pred_total      pred_sev\n",
      "0  2025-08  236.347344  1.260232e+10  5.332120e+07\n",
      "1  2025-09  241.058337  1.284432e+10  5.328303e+07\n",
      "2  2025-10  241.505029  1.292599e+10  5.352265e+07\n",
      "3  2025-11  242.857661  1.290178e+10  5.312485e+07\n",
      "4  2025-12  239.452450  1.275301e+10  5.325907e+07\n",
      "\n",
      "Saved: submission.csv\n",
      "                         id         value\n",
      "0   2025_08_Claim_Frequency  2.363473e+02\n",
      "1    2025_08_Claim_Severity  5.332120e+07\n",
      "2       2025_08_Total_Claim  1.260232e+10\n",
      "3   2025_09_Claim_Frequency  2.410583e+02\n",
      "4    2025_09_Claim_Severity  5.328303e+07\n",
      "5       2025_09_Total_Claim  1.284432e+10\n",
      "6   2025_10_Claim_Frequency  2.415050e+02\n",
      "7    2025_10_Claim_Severity  5.352265e+07\n",
      "8       2025_10_Total_Claim  1.292599e+10\n",
      "9   2025_11_Claim_Frequency  2.428577e+02\n",
      "10   2025_11_Claim_Severity  5.312485e+07\n",
      "11      2025_11_Total_Claim  1.290178e+10\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 5 v25 â€” ROBUST ENSEMBLE (Stage3 + Stage4v23) + AUTO BLEND\n",
    "# Goal: reduce public gap (LB) by minimizing worst-case across splits\n",
    "# ============================================================\n",
    "\n",
    "!pip install -q statsmodels\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\n",
    "sample_sub = pd.read_csv(BASE_PATH + \"sample_submission.csv\")\n",
    "\n",
    "assert \"df\" in globals(), \"df belum ada. Jalankan Stage 1 dulu.\"\n",
    "assert \"year_month\" in df.columns, \"df['year_month'] belum ada.\"\n",
    "\n",
    "# ------------------------------\n",
    "# Kaggle horizon from sample_sub\n",
    "# ------------------------------\n",
    "sample_sub[\"year\"]  = sample_sub[\"id\"].str.split(\"_\").str[0]\n",
    "sample_sub[\"month\"] = sample_sub[\"id\"].str.split(\"_\").str[1]\n",
    "sample_sub[\"month_key\"] = sample_sub[\"year\"] + \"-\" + sample_sub[\"month\"]\n",
    "\n",
    "future_periods = (\n",
    "    pd.PeriodIndex(sample_sub[\"month_key\"], freq=\"M\")\n",
    "      .unique()\n",
    "      .sort_values()\n",
    ")\n",
    "H = int(len(future_periods))\n",
    "\n",
    "# ------------------------------\n",
    "# BUILD monthly EXACT like Stage 4 v23 (rate+sev, exposure-aware)\n",
    "# ------------------------------\n",
    "has_exposure = \"active_policies\" in df.columns\n",
    "agg_dict = {\n",
    "    \"frequency\": (\"claim_id\",\"count\"),\n",
    "    \"total_claim\": (\"nominal_klaim_yang_disetujui\",\"sum\"),\n",
    "}\n",
    "if has_exposure:\n",
    "    agg_dict[\"exposure\"] = (\"active_policies\",\"first\")\n",
    "\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(**agg_dict)\n",
    "      .reset_index()\n",
    "      .sort_values(\"year_month\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "if not isinstance(monthly.loc[0, \"year_month\"], pd.Period):\n",
    "    monthly[\"year_month\"] = pd.PeriodIndex(monthly[\"year_month\"], freq=\"M\")\n",
    "\n",
    "monthly[\"frequency\"]   = monthly[\"frequency\"].astype(float).clip(lower=1.0)\n",
    "monthly[\"total_claim\"] = monthly[\"total_claim\"].astype(float).clip(lower=1.0)\n",
    "\n",
    "if not has_exposure:\n",
    "    monthly[\"exposure\"] = float(np.nanmean(monthly[\"frequency\"])) * 10.0\n",
    "monthly[\"exposure\"] = monthly[\"exposure\"].astype(float).clip(lower=1.0)\n",
    "\n",
    "monthly[\"severity\"]   = (monthly[\"total_claim\"] / monthly[\"frequency\"]).astype(float).clip(lower=1e-9)\n",
    "monthly[\"claim_rate\"] = (monthly[\"frequency\"] / monthly[\"exposure\"]).astype(float).clip(lower=1e-12)\n",
    "\n",
    "N = len(monthly)\n",
    "print(\"N months:\", N, \"| H:\", H, \"| Has exposure:\", has_exposure)\n",
    "\n",
    "# ------------------------------\n",
    "# Metrics\n",
    "# ------------------------------\n",
    "def mape_frac(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred) & (y_true != 0)\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return float(np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])))\n",
    "\n",
    "def avg_mape(yF, pF, yT, pT):\n",
    "    yS = yT / np.clip(yF, 1.0, np.inf)\n",
    "    pS = pT / np.clip(pF, 1.0, np.inf)\n",
    "    mf = mape_frac(yF, pF)\n",
    "    mt = mape_frac(yT, pT)\n",
    "    ms = mape_frac(yS, pS)\n",
    "    return float(np.nanmean([mf, mt, ms])), mt, mf, ms\n",
    "\n",
    "# ------------------------------\n",
    "# Stage 3 v17 config (fixed from your best)\n",
    "# ------------------------------\n",
    "S3 = dict(\n",
    "    wt_total=0.85, anchor_total=\"median\",\n",
    "    wt_freq=0.20,  anchor_freq=\"mean\",\n",
    "    k_anchor=3\n",
    ")\n",
    "\n",
    "def anchor_level(x: pd.Series, k: int, how: str):\n",
    "    tail = np.asarray(x.tail(k), dtype=float)\n",
    "    return float(np.median(tail)) if how == \"median\" else float(np.mean(tail))\n",
    "\n",
    "def ets_1step_log1p(level_series: pd.Series, trend=\"add\", damped=True):\n",
    "    y = np.log1p(level_series.astype(float).clip(lower=1e-12))\n",
    "    try:\n",
    "        m = ExponentialSmoothing(\n",
    "            y, trend=trend, damped_trend=(damped if trend is not None else False),\n",
    "            seasonal=None\n",
    "        ).fit()\n",
    "        return float(np.expm1(m.forecast(1).iloc[0]))\n",
    "    except:\n",
    "        return float(level_series.iloc[-1])\n",
    "\n",
    "def simulate_stage3(train_df: pd.DataFrame, H: int):\n",
    "    sim = train_df.copy().reset_index(drop=True)\n",
    "    pF, pT = [], []\n",
    "    for _ in range(H):\n",
    "        k = int(S3[\"k_anchor\"])\n",
    "\n",
    "        # TOTAL\n",
    "        tot_fc = ets_1step_log1p(sim[\"total_claim\"], trend=\"add\", damped=True)\n",
    "        tot_anchor = anchor_level(sim[\"total_claim\"], k, S3[\"anchor_total\"])\n",
    "        tot_pred = float(S3[\"wt_total\"])*tot_fc + (1-float(S3[\"wt_total\"]))*tot_anchor\n",
    "        tot_pred = max(1.0, tot_pred)\n",
    "\n",
    "        # FREQ\n",
    "        fre_fc = ets_1step_log1p(sim[\"frequency\"], trend=\"add\", damped=True)\n",
    "        fre_anchor = anchor_level(sim[\"frequency\"], k, S3[\"anchor_freq\"])\n",
    "        fre_pred = float(S3[\"wt_freq\"])*fre_fc + (1-float(S3[\"wt_freq\"]))*fre_anchor\n",
    "        fre_pred = max(1.0, fre_pred)\n",
    "\n",
    "        pF.append(fre_pred)\n",
    "        pT.append(tot_pred)\n",
    "\n",
    "        sim = pd.concat([sim, pd.DataFrame([{\n",
    "            \"year_month\": sim[\"year_month\"].iloc[-1] + 1,\n",
    "            \"frequency\": fre_pred,\n",
    "            \"total_claim\": tot_pred,\n",
    "            \"exposure\": float(sim[\"exposure\"].iloc[-1]),\n",
    "            \"severity\": float(tot_pred / fre_pred),\n",
    "            \"claim_rate\": float(fre_pred / float(sim[\"exposure\"].iloc[-1]))\n",
    "        }])], ignore_index=True)\n",
    "    return np.array(pF, float), np.array(pT, float)\n",
    "\n",
    "# ------------------------------\n",
    "# Stage 4 v23 best params (paste from your output)\n",
    "# ------------------------------\n",
    "S4 = {\n",
    "    'k_anchor': 4,\n",
    "    'anchor_rate': 'mean',\n",
    "    'anchor_sev': 'mean',\n",
    "    'wt_rate': 0.3332890181752667,\n",
    "    'wt_sev': 0.5501072463530577,\n",
    "    'beta': 0.8005225975816733,\n",
    "    'damped': True,\n",
    "    'init_method': 'estimated',\n",
    "    'capR_low': 0.7818972835090717,\n",
    "    'capR_high': 1.3058960367487729,\n",
    "    'capS_low': 0.6976922342421228,\n",
    "    'capS_high': 1.1536381686265984\n",
    "}\n",
    "\n",
    "ROUND_FREQ = False  # <- coba False untuk LB; set True kalau mau persis Stage4 objective\n",
    "\n",
    "def ets_1step_log(x_log: pd.Series, trend, damped, init_method):\n",
    "    try:\n",
    "        m = ExponentialSmoothing(\n",
    "            x_log, trend=trend,\n",
    "            damped_trend=(damped if trend is not None else False),\n",
    "            seasonal=None, initialization_method=init_method\n",
    "        ).fit()\n",
    "        return float(m.forecast(1).iloc[0])\n",
    "    except:\n",
    "        return float(x_log.iloc[-1])\n",
    "\n",
    "def simulate_stage4(train_df: pd.DataFrame, H: int):\n",
    "    sim = train_df.copy().reset_index(drop=True)\n",
    "    pF, pT = [], []\n",
    "    for _ in range(H):\n",
    "        k = int(S4[\"k_anchor\"])\n",
    "        exp_next = float(sim[\"exposure\"].iloc[-1])\n",
    "        exp_next = max(1.0, exp_next)\n",
    "\n",
    "        aR = anchor_level(sim[\"claim_rate\"], k, S4[\"anchor_rate\"])\n",
    "        aS = anchor_level(sim[\"severity\"],   k, S4[\"anchor_sev\"])\n",
    "\n",
    "        lr = np.log(sim[\"claim_rate\"].astype(float).clip(lower=1e-12))\n",
    "        ls = np.log(sim[\"severity\"].astype(float).clip(lower=1e-12))\n",
    "\n",
    "        lr_add  = ets_1step_log(lr, trend=\"add\",  damped=bool(S4[\"damped\"]), init_method=S4[\"init_method\"])\n",
    "        lr_none = ets_1step_log(lr, trend=None,   damped=False,              init_method=S4[\"init_method\"])\n",
    "        r_fc = float(np.exp(float(S4[\"beta\"])*lr_add + (1-float(S4[\"beta\"]))*lr_none))\n",
    "\n",
    "        ls_add  = ets_1step_log(ls, trend=\"add\",  damped=bool(S4[\"damped\"]), init_method=S4[\"init_method\"])\n",
    "        ls_none = ets_1step_log(ls, trend=None,   damped=False,              init_method=S4[\"init_method\"])\n",
    "        s_fc = float(np.exp(float(S4[\"beta\"])*ls_add + (1-float(S4[\"beta\"]))*ls_none))\n",
    "\n",
    "        r_pred = float(S4[\"wt_rate\"])*r_fc + (1-float(S4[\"wt_rate\"]))*aR\n",
    "        s_pred = float(S4[\"wt_sev\"]) *s_fc + (1-float(S4[\"wt_sev\"])) *aS\n",
    "\n",
    "        r_pred = float(np.clip(r_pred, aR*float(S4[\"capR_low\"]), aR*float(S4[\"capR_high\"])))\n",
    "        s_pred = float(np.clip(s_pred, aS*float(S4[\"capS_low\"]), aS*float(S4[\"capS_high\"])))\n",
    "\n",
    "        f_pred = float(max(1.0, r_pred * exp_next))\n",
    "        if ROUND_FREQ:\n",
    "            f_pred = float(max(1.0, np.round(f_pred)))\n",
    "\n",
    "        t_pred = float(max(1.0, f_pred * s_pred))\n",
    "\n",
    "        pF.append(f_pred)\n",
    "        pT.append(t_pred)\n",
    "\n",
    "        sim = pd.concat([sim, pd.DataFrame([{\n",
    "            \"year_month\": sim[\"year_month\"].iloc[-1] + 1,\n",
    "            \"exposure\": exp_next,\n",
    "            \"claim_rate\": float(max(1e-12, f_pred/exp_next)),\n",
    "            \"severity\": float(max(1e-9, t_pred/np.clip(f_pred, 1.0, np.inf))),\n",
    "            \"frequency\": f_pred,\n",
    "            \"total_claim\": t_pred\n",
    "        }])], ignore_index=True)\n",
    "    return np.array(pF, float), np.array(pT, float)\n",
    "\n",
    "# ------------------------------\n",
    "# Choose CV splits like Stage4 (12,13,14) and pick blend weight\n",
    "# Optimize worst-case avg MAPE across splits\n",
    "# ------------------------------\n",
    "train_end_last = N - H\n",
    "train_ends = list(range(max(12, train_end_last - 2), train_end_last + 1))\n",
    "train_ends = [te for te in train_ends if (te + H) <= N and te >= 12]\n",
    "print(\"CV train_ends:\", train_ends)\n",
    "\n",
    "w_grid = np.linspace(0.0, 1.0, 11)  # 0..1 step 0.1 ; w=1 means pure Stage4, w=0 pure Stage3\n",
    "\n",
    "best = None\n",
    "rows = []\n",
    "\n",
    "for w in w_grid:\n",
    "    split_scores = []\n",
    "    split_detail = []\n",
    "    for te in train_ends:\n",
    "        tr = monthly.iloc[:te].copy().reset_index(drop=True)\n",
    "        va = monthly.iloc[te:te+H].copy().reset_index(drop=True)\n",
    "\n",
    "        pF3, pT3 = simulate_stage3(tr, H)\n",
    "        pF4, pT4 = simulate_stage4(tr, H)\n",
    "\n",
    "        pF = w*pF4 + (1-w)*pF3\n",
    "        pT = w*pT4 + (1-w)*pT3\n",
    "\n",
    "        sc, mt, mf, ms = avg_mape(va[\"frequency\"].values, pF, va[\"total_claim\"].values, pT)\n",
    "        split_scores.append(sc)\n",
    "        split_detail.append((mt, mf, ms))\n",
    "\n",
    "    worst = float(np.max(split_scores))\n",
    "    meanv = float(np.mean(split_scores))\n",
    "    rows.append([w, meanv, worst])\n",
    "\n",
    "    cand = (worst, meanv, w, split_scores)\n",
    "    if (best is None) or (cand < best):\n",
    "        best = cand\n",
    "\n",
    "w_best = best[2]\n",
    "print(\"\\nBlend search results (fraction):\")\n",
    "print(pd.DataFrame(rows, columns=[\"w_stage4\",\"mean_avg\",\"worst_avg\"]).sort_values([\"worst_avg\",\"mean_avg\"]).head(8))\n",
    "print(\"\\nChosen w_stage4:\", w_best, \"(w=1 pure Stage4, w=0 pure Stage3)\")\n",
    "\n",
    "# ------------------------------\n",
    "# FINAL FORECAST on FULL data for Kaggle future months\n",
    "# ------------------------------\n",
    "pF3, pT3 = simulate_stage3(monthly, H)\n",
    "pF4, pT4 = simulate_stage4(monthly, H)\n",
    "\n",
    "pF = w_best*pF4 + (1-w_best)*pF3\n",
    "pT = w_best*pT4 + (1-w_best)*pT3\n",
    "pS = pT / np.clip(pF, 1.0, np.inf)\n",
    "\n",
    "pred_map = {}\n",
    "preview = []\n",
    "\n",
    "for i, period in enumerate(future_periods):\n",
    "    key = f\"{period.year}_{str(period.month).zfill(2)}\"\n",
    "    pred_map[f\"{key}_Claim_Frequency\"] = float(pF[i])\n",
    "    pred_map[f\"{key}_Total_Claim\"]     = float(pT[i])\n",
    "    pred_map[f\"{key}_Claim_Severity\"]  = float(pS[i])\n",
    "    preview.append([str(period), float(pF[i]), float(pT[i]), float(pS[i])])\n",
    "\n",
    "sub = sample_sub.copy()\n",
    "sub[\"value\"] = sub[\"id\"].map(pred_map)\n",
    "\n",
    "missing = int(sub[\"value\"].isna().sum())\n",
    "print(\"NaN in submission:\", missing)\n",
    "assert missing == 0, \"Ada id belum terisi. Cek key format.\"\n",
    "\n",
    "sub = sub[[\"id\",\"value\"]]\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"\\nPreview future predictions:\")\n",
    "print(pd.DataFrame(preview, columns=[\"period\",\"pred_freq\",\"pred_total\",\"pred_sev\"]))\n",
    "print(\"\\nSaved: submission.csv\")\n",
    "print(sub.head(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b183e519",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T02:10:21.127780Z",
     "iopub.status.busy": "2026-02-21T02:10:21.127284Z",
     "iopub.status.idle": "2026-02-21T02:10:21.139595Z",
     "shell.execute_reply": "2026-02-21T02:10:21.137376Z"
    },
    "papermill": {
     "duration": 0.026144,
     "end_time": "2026-02-21T02:10:21.142255",
     "exception": false,
     "start_time": "2026-02-21T02:10:21.116111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         id         value\n",
      "0   2025_08_Claim_Frequency  2.363473e+02\n",
      "1    2025_08_Claim_Severity  5.332120e+07\n",
      "2       2025_08_Total_Claim  1.260232e+10\n",
      "3   2025_09_Claim_Frequency  2.410583e+02\n",
      "4    2025_09_Claim_Severity  5.328303e+07\n",
      "5       2025_09_Total_Claim  1.284432e+10\n",
      "6   2025_10_Claim_Frequency  2.415050e+02\n",
      "7    2025_10_Claim_Severity  5.352265e+07\n",
      "8       2025_10_Total_Claim  1.292599e+10\n",
      "9   2025_11_Claim_Frequency  2.428577e+02\n",
      "10   2025_11_Claim_Severity  5.312485e+07\n",
      "11      2025_11_Total_Claim  1.290178e+10\n"
     ]
    }
   ],
   "source": [
    "print(sub.head(12)) ##  13,5 persen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eef75bec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T02:10:21.162375Z",
     "iopub.status.busy": "2026-02-21T02:10:21.160443Z",
     "iopub.status.idle": "2026-02-21T02:10:21.561590Z",
     "shell.execute_reply": "2026-02-21T02:10:21.560747Z"
    },
    "papermill": {
     "duration": 0.414358,
     "end_time": "2026-02-21T02:10:21.564382",
     "exception": false,
     "start_time": "2026-02-21T02:10:21.150024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backtest last-H (%): avg 5.7701 | total 6.4797 | freq 6.2969 | sev 4.5335\n",
      "        ym     yF     pF            yT            pT\n",
      "0  2025-03  230.0  241.0  1.367924e+10  1.282809e+10\n",
      "1  2025-04  208.0  236.0  1.116425e+10  1.263251e+10\n",
      "2  2025-05  239.0  236.0  1.222680e+10  1.272460e+10\n",
      "3  2025-06  234.0  239.0  1.337312e+10  1.315459e+10\n",
      "4  2025-07  264.0  238.0  1.369923e+10  1.269653e+10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "def mape_frac(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred) & (y_true != 0)\n",
    "    return float(np.mean(np.abs((y_true[mask]-y_pred[mask]) / y_true[mask])))\n",
    "\n",
    "# PASTE best params Stage4 v23 kamu\n",
    "BEST = {\n",
    "    'k_anchor': 4,\n",
    "    'anchor_rate': 'mean',\n",
    "    'anchor_sev': 'mean',\n",
    "    'wt_rate': 0.3332890181752667,\n",
    "    'wt_sev': 0.5501072463530577,\n",
    "    'beta': 0.8005225975816733,\n",
    "    'damped': True,\n",
    "    'init_method': 'estimated',\n",
    "    'capR_low': 0.7818972835090717,\n",
    "    'capR_high': 1.3058960367487729,\n",
    "    'capS_low': 0.6976922342421228,\n",
    "    'capS_high': 1.1536381686265984\n",
    "}\n",
    "\n",
    "# monthly harus yang sama dengan Stage4 v23 (19 bulan, ada exposure/claim_rate/severity)\n",
    "# Kalau kamu sudah punya `monthly` dari Stage4 v23, pakai itu. Jangan rebuild lagi.\n",
    "assert \"monthly\" in globals(), \"monthly belum ada (pakai monthly dari Stage4 v23)\"\n",
    "monthly_stage4 = monthly.copy().reset_index(drop=True)\n",
    "\n",
    "# horizon sama seperti Kaggle (5)\n",
    "H = 5\n",
    "N = len(monthly_stage4)\n",
    "train_end = N - H\n",
    "train = monthly_stage4.iloc[:train_end].copy().reset_index(drop=True)\n",
    "valid = monthly_stage4.iloc[train_end:train_end+H].copy().reset_index(drop=True)\n",
    "\n",
    "def ets_1step_log(x_log: pd.Series, trend, damped, init_method):\n",
    "    try:\n",
    "        m = ExponentialSmoothing(\n",
    "            x_log,\n",
    "            trend=trend,\n",
    "            damped_trend=(damped if trend is not None else False),\n",
    "            seasonal=None,\n",
    "            initialization_method=init_method\n",
    "        ).fit()\n",
    "        return float(m.forecast(1).iloc[0])\n",
    "    except:\n",
    "        return float(x_log.iloc[-1])\n",
    "\n",
    "def anchor_level(x_level: pd.Series, k: int, how: str):\n",
    "    tail = np.asarray(x_level.tail(k), dtype=float)\n",
    "    return float(np.median(tail)) if how == \"median\" else float(np.mean(tail))\n",
    "\n",
    "sim = train.copy()\n",
    "pred_F, pred_T, pred_S = [], [], []\n",
    "\n",
    "for _ in range(H):\n",
    "    k = int(BEST[\"k_anchor\"])\n",
    "    exp_next = float(sim[\"exposure\"].iloc[-1])\n",
    "\n",
    "    aR = anchor_level(sim[\"claim_rate\"], k, BEST[\"anchor_rate\"])\n",
    "    aS = anchor_level(sim[\"severity\"],   k, BEST[\"anchor_sev\"])\n",
    "\n",
    "    lr = np.log(sim[\"claim_rate\"].astype(float).clip(lower=1e-12))\n",
    "    ls = np.log(sim[\"severity\"].astype(float).clip(lower=1e-12))\n",
    "\n",
    "    lr_add  = ets_1step_log(lr, trend=\"add\",  damped=bool(BEST[\"damped\"]), init_method=BEST[\"init_method\"])\n",
    "    lr_none = ets_1step_log(lr, trend=None,   damped=False,                init_method=BEST[\"init_method\"])\n",
    "    r_fc = float(np.exp(float(BEST[\"beta\"])*lr_add + (1-float(BEST[\"beta\"]))*lr_none))\n",
    "\n",
    "    ls_add  = ets_1step_log(ls, trend=\"add\",  damped=bool(BEST[\"damped\"]), init_method=BEST[\"init_method\"])\n",
    "    ls_none = ets_1step_log(ls, trend=None,   damped=False,                init_method=BEST[\"init_method\"])\n",
    "    s_fc = float(np.exp(float(BEST[\"beta\"])*ls_add + (1-float(BEST[\"beta\"]))*ls_none))\n",
    "\n",
    "    r_pred = float(BEST[\"wt_rate\"])*r_fc + (1-float(BEST[\"wt_rate\"]))*aR\n",
    "    s_pred = float(BEST[\"wt_sev\"]) *s_fc + (1-float(BEST[\"wt_sev\"])) *aS\n",
    "\n",
    "    r_pred = float(np.clip(r_pred, aR*float(BEST[\"capR_low\"]), aR*float(BEST[\"capR_high\"])))\n",
    "    s_pred = float(np.clip(s_pred, aS*float(BEST[\"capS_low\"]), aS*float(BEST[\"capS_high\"])))\n",
    "\n",
    "    f_pred = float(max(1.0, np.round(r_pred * exp_next)))\n",
    "    t_pred = float(max(1.0, f_pred * s_pred))\n",
    "    s_pred = float(t_pred / f_pred)\n",
    "\n",
    "    pred_F.append(f_pred)\n",
    "    pred_T.append(t_pred)\n",
    "    pred_S.append(s_pred)\n",
    "\n",
    "    sim = pd.concat([sim, pd.DataFrame([{\n",
    "        \"year_month\": sim[\"year_month\"].iloc[-1] + 1,\n",
    "        \"exposure\": exp_next,\n",
    "        \"claim_rate\": float(max(1e-12, f_pred/exp_next)),\n",
    "        \"severity\": s_pred,\n",
    "        \"frequency\": f_pred,\n",
    "        \"total_claim\": t_pred\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "yF = valid[\"frequency\"].values\n",
    "yT = valid[\"total_claim\"].values\n",
    "yS = (valid[\"total_claim\"].values / np.clip(valid[\"frequency\"].values, 1.0, np.inf))\n",
    "\n",
    "mf = mape_frac(yF, pred_F)\n",
    "mt = mape_frac(yT, pred_T)\n",
    "ms = mape_frac(yS, pred_S)\n",
    "avg = float(np.nanmean([mf, mt, ms]))\n",
    "\n",
    "print(\"Backtest last-H (%):\",\n",
    "      \"avg\", round(avg*100,4),\n",
    "      \"| total\", round(mt*100,4),\n",
    "      \"| freq\", round(mf*100,4),\n",
    "      \"| sev\", round(ms*100,4))\n",
    "print(pd.DataFrame({\"ym\": valid[\"year_month\"], \"yF\": yF, \"pF\": pred_F, \"yT\": yT, \"pT\": pred_T}))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15694757,
     "datasetId": 9488145,
     "sourceId": 14836320,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 865.311474,
   "end_time": "2026-02-21T02:10:22.600217",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-21T01:55:57.288743",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "07d6e17a70f04468b325a8acaee1e5e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0f1703fc02964c0cb4f828450fc2bfc3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "30f558bb77b34fb7b54898e7fa2edb23": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0f1703fc02964c0cb4f828450fc2bfc3",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_3573d84e21764a549876dbea7e4faf23",
       "tabbable": null,
       "tooltip": null,
       "value": "Bestâ€‡trial:â€‡660.â€‡Bestâ€‡value:â€‡0.0701207:â€‡100%"
      }
     },
     "3573d84e21764a549876dbea7e4faf23": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "36d21141cf4e4e5a9b04c59b50c76f9a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "98663fc44afc46f6826f3030e65ac624": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b86d9b0873b74e16864164d1e20c0f85",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_36d21141cf4e4e5a9b04c59b50c76f9a",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡700/700â€‡[12:46&lt;00:00,â€‡â€‡1.17s/it]"
      }
     },
     "a69c7e7b1a6c4a589de2b57075a615a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d4377338b8df4ec5b0648042f4d101b0",
       "max": 700.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_07d6e17a70f04468b325a8acaee1e5e7",
       "tabbable": null,
       "tooltip": null,
       "value": 700.0
      }
     },
     "b86d9b0873b74e16864164d1e20c0f85": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c1f356977b03439fa8283216a3840e5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_30f558bb77b34fb7b54898e7fa2edb23",
        "IPY_MODEL_a69c7e7b1a6c4a589de2b57075a615a3",
        "IPY_MODEL_98663fc44afc46f6826f3030e65ac624"
       ],
       "layout": "IPY_MODEL_ccb5d127cd5d4736a04cbfc129e87207",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ccb5d127cd5d4736a04cbfc129e87207": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d4377338b8df4ec5b0648042f4d101b0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
