{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9831f396",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-21T03:33:56.052258Z",
     "iopub.status.busy": "2026-02-21T03:33:56.051819Z",
     "iopub.status.idle": "2026-02-21T03:33:57.188271Z",
     "shell.execute_reply": "2026-02-21T03:33:57.186919Z"
    },
    "papermill": {
     "duration": 1.146813,
     "end_time": "2026-02-21T03:33:57.190625",
     "exception": false,
     "start_time": "2026-02-21T03:33:56.043812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/Data_Klaim.csv\n",
      "/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/sample_submission.csv\n",
      "/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/Data_Polis.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4996270c",
   "metadata": {
    "papermill": {
     "duration": 0.005475,
     "end_time": "2026-02-21T03:33:57.203470",
     "exception": false,
     "start_time": "2026-02-21T03:33:57.197995",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DATA FOUNDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "975687b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T03:33:57.216284Z",
     "iopub.status.busy": "2026-02-21T03:33:57.215490Z",
     "iopub.status.idle": "2026-02-21T03:33:57.462461Z",
     "shell.execute_reply": "2026-02-21T03:33:57.461413Z"
    },
    "papermill": {
     "duration": 0.25586,
     "end_time": "2026-02-21T03:33:57.464523",
     "exception": false,
     "start_time": "2026-02-21T03:33:57.208663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVICE_COL: tanggal_pasien_masuk_rs\n",
      "EXPOSURE_MODE: inforce\n",
      "Policy start col: tanggal_efektif_polis\n",
      "Frequency source: claim_id\n",
      "Monthly shape: (16, 34)\n",
      "Unique months: 16\n",
      "Exposure min/max: 4096.0 4096.0\n",
      "Total_claim min/max: 9610379678.55 17480540371.87\n",
      "\n",
      "STAGE 1 v4 â€” READY\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 1 v4 â€” FOUNDATION (DATASET-AWARE + NO TARGET DISTORTION)\n",
    "# - Fix YYYYMMDD parsing\n",
    "# - Keep RAW nominal for target (total_claim)\n",
    "# - Put winsorization into separate column (optional features)\n",
    "# - Build monthly with complete month range (fill missing months)\n",
    "# - Exposure: claimant / inforce (optional)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\n",
    "klaim = pd.read_csv(BASE_PATH + \"Data_Klaim.csv\")\n",
    "polis = pd.read_csv(BASE_PATH + \"Data_Polis.csv\")\n",
    "\n",
    "# =============================\n",
    "# CLEAN COLUMN NAMES\n",
    "# =============================\n",
    "def clean_columns(df):\n",
    "    df = df.copy()\n",
    "    df.columns = (\n",
    "        df.columns.astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"_\", regex=False)\n",
    "        .str.replace(\"/\", \"_\", regex=False)\n",
    "        .str.replace(\"-\", \"_\", regex=False)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "klaim = clean_columns(klaim)\n",
    "polis = clean_columns(polis)\n",
    "\n",
    "# =============================\n",
    "# DATE PARSING (handle YYYYMMDD int + dd/mm/yyyy)\n",
    "# =============================\n",
    "def parse_mixed_date(s: pd.Series) -> pd.Series:\n",
    "    s = s.copy()\n",
    "    idx = s.index\n",
    "\n",
    "    # normalize to string for pattern checks\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        ss = s.astype(\"Int64\").astype(str)\n",
    "    else:\n",
    "        ss = s.astype(str).str.strip()\n",
    "\n",
    "    ss = ss.replace({\"<NA>\": np.nan, \"nan\": np.nan, \"None\": np.nan, \"NaT\": np.nan})\n",
    "\n",
    "    out = pd.Series(pd.NaT, index=idx, dtype=\"datetime64[ns]\")\n",
    "\n",
    "    # YYYYMMDD (8 digits)\n",
    "    m8 = ss.str.fullmatch(r\"\\d{8}\", na=False)\n",
    "    if m8.any():\n",
    "        out.loc[m8] = pd.to_datetime(ss.loc[m8], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "\n",
    "    # remaining\n",
    "    rest = ~m8 & ss.notna()\n",
    "    if rest.any():\n",
    "        has_slash = ss.loc[rest].str.contains(\"/\", na=False)\n",
    "        if has_slash.any():\n",
    "            out.loc[rest[rest].index.intersection(ss.loc[rest][has_slash].index)] = pd.to_datetime(\n",
    "                ss.loc[rest][has_slash], errors=\"coerce\", dayfirst=True\n",
    "            )\n",
    "        if (~has_slash).any():\n",
    "            out.loc[rest[rest].index.intersection(ss.loc[rest][~has_slash].index)] = pd.to_datetime(\n",
    "                ss.loc[rest][~has_slash], errors=\"coerce\"\n",
    "            )\n",
    "\n",
    "    return out\n",
    "\n",
    "for col in klaim.columns:\n",
    "    if \"tanggal\" in col:\n",
    "        klaim[col] = parse_mixed_date(klaim[col])\n",
    "\n",
    "for col in polis.columns:\n",
    "    if \"tanggal\" in col:\n",
    "        polis[col] = parse_mixed_date(polis[col])\n",
    "\n",
    "# =============================\n",
    "# SAFE DEDUP\n",
    "# =============================\n",
    "claim_id_col = None\n",
    "for c in [\"claim_id\", \"id_klaim\", \"klaim_id\"]:\n",
    "    if c in klaim.columns:\n",
    "        claim_id_col = c\n",
    "        break\n",
    "\n",
    "if claim_id_col is not None:\n",
    "    klaim = klaim.drop_duplicates(subset=[claim_id_col]).reset_index(drop=True)\n",
    "else:\n",
    "    klaim = klaim.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "polis = polis.drop_duplicates(subset=[\"nomor_polis\"]).reset_index(drop=True)\n",
    "\n",
    "# =============================\n",
    "# BASIC CLEANING\n",
    "# =============================\n",
    "# choose service date column\n",
    "service_col = \"tanggal_pasien_masuk_rs\" if \"tanggal_pasien_masuk_rs\" in klaim.columns else None\n",
    "if service_col is None:\n",
    "    # fallback: first tanggal* column\n",
    "    tcols = [c for c in klaim.columns if \"tanggal\" in c]\n",
    "    service_col = tcols[0] if len(tcols) else None\n",
    "\n",
    "if service_col is None:\n",
    "    raise ValueError(\"No tanggal column found in klaim for building year_month.\")\n",
    "\n",
    "klaim = klaim.dropna(subset=[\"nomor_polis\", service_col]).copy()\n",
    "\n",
    "# nominal column\n",
    "nom_col = \"nominal_klaim_yang_disetujui\"\n",
    "if nom_col not in klaim.columns:\n",
    "    # fallback: try find 'nominal' column\n",
    "    cand = [c for c in klaim.columns if \"nominal\" in c]\n",
    "    if len(cand) == 0:\n",
    "        raise ValueError(\"No nominal column found in klaim.\")\n",
    "    nom_col = cand[0]\n",
    "\n",
    "# IMPORTANT: keep RAW nominal for target\n",
    "raw_nom = pd.to_numeric(klaim[nom_col], errors=\"coerce\").fillna(0).clip(lower=0)\n",
    "klaim[nom_col] = raw_nom\n",
    "\n",
    "# OPTIONAL: winsorized copy for feature engineering (NOT for target)\n",
    "klaim[\"nominal_klaim_clip\"] = raw_nom.copy()\n",
    "pos = klaim[\"nominal_klaim_clip\"] > 0\n",
    "if pos.any():\n",
    "    low_q  = klaim.loc[pos, \"nominal_klaim_clip\"].quantile(0.005)\n",
    "    high_q = klaim.loc[pos, \"nominal_klaim_clip\"].quantile(0.995)\n",
    "    klaim.loc[pos, \"nominal_klaim_clip\"] = klaim.loc[pos, \"nominal_klaim_clip\"].clip(low_q, high_q)\n",
    "\n",
    "# =============================\n",
    "# MERGE\n",
    "# =============================\n",
    "df = klaim.merge(polis, on=\"nomor_polis\", how=\"left\")\n",
    "\n",
    "# =============================\n",
    "# SERVICE MONTH\n",
    "# =============================\n",
    "df[\"year_month\"] = df[service_col].dt.to_period(\"M\")\n",
    "\n",
    "min_m = df[\"year_month\"].min()\n",
    "max_m = df[\"year_month\"].max()\n",
    "all_months = pd.period_range(min_m, max_m, freq=\"M\")\n",
    "\n",
    "# ============================================================\n",
    "# EXPOSURE OPTIONS\n",
    "# ============================================================\n",
    "EXPOSURE_MODE = \"inforce\"  # \"claimant\" or \"inforce\"\n",
    "\n",
    "# claimant exposure: unique policies that claim in that month\n",
    "expo_claimant = (\n",
    "    df.groupby(\"year_month\")[\"nomor_polis\"].nunique()\n",
    "      .reindex(all_months, fill_value=0)\n",
    "      .rename(\"exposure_claimant\")\n",
    "      .rename_axis(\"year_month\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# inforce exposure: cumulative started policies (no end date available)\n",
    "start_col = None\n",
    "for c in [\"tanggal_efektif_polis\", \"tanggal_mulai_polis\", \"tanggal_mulai\"]:\n",
    "    if c in polis.columns:\n",
    "        start_col = c\n",
    "        break\n",
    "\n",
    "if start_col is not None:\n",
    "    p = polis[[\"nomor_polis\", start_col]].dropna(subset=[start_col]).copy()\n",
    "    p[\"start_m\"] = p[start_col].dt.to_period(\"M\")\n",
    "\n",
    "    base = p.loc[p[\"start_m\"] < min_m, \"nomor_polis\"].nunique()\n",
    "    inc = p.loc[p[\"start_m\"] >= min_m].groupby(\"start_m\")[\"nomor_polis\"].nunique()\n",
    "\n",
    "    expo_inforce = (\n",
    "        (base + inc.reindex(all_months, fill_value=0).cumsum())\n",
    "        .rename(\"exposure_inforce\")\n",
    "        .rename_axis(\"year_month\")\n",
    "        .reset_index()\n",
    "    )\n",
    "else:\n",
    "    expo_inforce = expo_claimant[[\"year_month\"]].copy()\n",
    "    expo_inforce[\"exposure_inforce\"] = 0\n",
    "\n",
    "expo = expo_claimant.merge(expo_inforce, on=\"year_month\", how=\"left\")\n",
    "\n",
    "# choose exposure with fallback safety\n",
    "expo[\"exposure\"] = np.where(EXPOSURE_MODE == \"inforce\", expo[\"exposure_inforce\"], expo[\"exposure_claimant\"])\n",
    "# if inforce is mostly 0 (bad parsing / missing), fallback to claimant\n",
    "if (EXPOSURE_MODE == \"inforce\") and (expo[\"exposure\"].sum() == 0):\n",
    "    expo[\"exposure\"] = expo[\"exposure_claimant\"]\n",
    "\n",
    "# merge exposure into df (keperluan stage lain)\n",
    "df = df.merge(expo[[\"year_month\", \"exposure\"]], on=\"year_month\", how=\"left\")\n",
    "df[\"active_policies\"] = df[\"exposure\"]\n",
    "\n",
    "# ============================================================\n",
    "# MONTHLY CORE TABLE (complete months)\n",
    "# target total_claim MUST be RAW nominal\n",
    "# ============================================================\n",
    "freq_col = claim_id_col if claim_id_col is not None else \"nomor_polis\"\n",
    "\n",
    "monthly_core = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(\n",
    "          frequency=(freq_col, \"count\"),\n",
    "          total_claim=(nom_col, \"sum\")\n",
    "      )\n",
    "      .reindex(all_months, fill_value=0)\n",
    "      .rename_axis(\"year_month\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "monthly = monthly_core.merge(expo[[\"year_month\", \"exposure\"]], on=\"year_month\", how=\"left\")\n",
    "\n",
    "monthly[\"severity\"] = monthly[\"total_claim\"] / monthly[\"frequency\"].replace(0, np.nan)\n",
    "monthly[\"claim_rate\"] = monthly[\"frequency\"] / monthly[\"exposure\"].replace(0, np.nan)\n",
    "\n",
    "# ============================================================\n",
    "# LOG FEATURES\n",
    "# ============================================================\n",
    "monthly[\"log_total\"] = np.log1p(monthly[\"total_claim\"])\n",
    "monthly[\"log_freq\"]  = np.log1p(monthly[\"frequency\"])\n",
    "monthly[\"log_sev\"]   = np.log1p(monthly[\"severity\"])\n",
    "monthly[\"log_rate\"]  = np.log1p(monthly[\"claim_rate\"])\n",
    "\n",
    "# ============================================================\n",
    "# VOLATILITY\n",
    "# ============================================================\n",
    "monthly[\"roll6\"] = monthly[\"total_claim\"].rolling(6, min_periods=3).mean()\n",
    "monthly[\"std6\"]  = monthly[\"total_claim\"].rolling(6, min_periods=3).std()\n",
    "monthly[\"vol_ratio\"] = monthly[\"std6\"] / monthly[\"roll6\"]\n",
    "monthly[\"high_vol_regime\"] = (monthly[\"vol_ratio\"] > monthly[\"vol_ratio\"].median()).astype(int)\n",
    "\n",
    "# ============================================================\n",
    "# TIME FEATURES\n",
    "# ============================================================\n",
    "monthly[\"month\"] = monthly[\"year_month\"].dt.month\n",
    "monthly[\"month_sin\"] = np.sin(2*np.pi*monthly[\"month\"]/12)\n",
    "monthly[\"month_cos\"] = np.cos(2*np.pi*monthly[\"month\"]/12)\n",
    "monthly[\"month_index\"] = np.arange(len(monthly))\n",
    "\n",
    "# ============================================================\n",
    "# SAFE LAGS\n",
    "# ============================================================\n",
    "for col in [\"log_total\", \"log_freq\", \"log_sev\", \"log_rate\"]:\n",
    "    monthly[f\"{col}_lag1\"] = monthly[col].shift(1)\n",
    "    monthly[f\"{col}_lag2\"] = monthly[col].shift(2)\n",
    "    monthly[f\"{col}_lag3\"] = monthly[col].shift(3)\n",
    "    monthly[f\"{col}_roll3\"] = monthly[col].shift(1).rolling(3).mean()\n",
    "\n",
    "monthly = monthly.dropna().reset_index(drop=True)\n",
    "\n",
    "# ============================================================\n",
    "# FINAL CHECK\n",
    "# ============================================================\n",
    "print(\"SERVICE_COL:\", service_col)\n",
    "print(\"EXPOSURE_MODE:\", EXPOSURE_MODE)\n",
    "print(\"Policy start col:\", start_col)\n",
    "print(\"Frequency source:\", freq_col)\n",
    "print(\"Monthly shape:\", monthly.shape)\n",
    "print(\"Unique months:\", monthly[\"year_month\"].nunique())\n",
    "print(\"Exposure min/max:\", float(monthly[\"exposure\"].min()), float(monthly[\"exposure\"].max()))\n",
    "print(\"Total_claim min/max:\", float(monthly[\"total_claim\"].min()), float(monthly[\"total_claim\"].max()))\n",
    "print(\"\\nSTAGE 1 v4 â€” READY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "413153cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T03:33:57.477619Z",
     "iopub.status.busy": "2026-02-21T03:33:57.477315Z",
     "iopub.status.idle": "2026-02-21T03:33:57.492733Z",
     "shell.execute_reply": "2026-02-21T03:33:57.491244Z"
    },
    "papermill": {
     "duration": 0.025029,
     "end_time": "2026-02-21T03:33:57.495062",
     "exception": false,
     "start_time": "2026-02-21T03:33:57.470033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year_month  frequency  exposure  freq_per_exposure\n",
      "6     2024-10        274      4096           0.066895\n",
      "7     2024-11        270      4096           0.065918\n",
      "8     2024-12        238      4096           0.058105\n",
      "9     2025-01        216      4096           0.052734\n",
      "10    2025-02        246      4096           0.060059\n",
      "11    2025-03        230      4096           0.056152\n",
      "12    2025-04        208      4096           0.050781\n",
      "13    2025-05        239      4096           0.058350\n",
      "14    2025-06        234      4096           0.057129\n",
      "15    2025-07        264      4096           0.064453\n",
      "freq_per_exposure min/max: 0.05078125 0.06689453125\n"
     ]
    }
   ],
   "source": [
    "tmp = monthly.copy()\n",
    "tmp[\"freq_per_exposure\"] = tmp[\"frequency\"] / tmp[\"exposure\"]\n",
    "print(tmp[[\"year_month\",\"frequency\",\"exposure\",\"freq_per_exposure\"]].tail(10))\n",
    "print(\"freq_per_exposure min/max:\",\n",
    "      tmp[\"freq_per_exposure\"].min(),\n",
    "      tmp[\"freq_per_exposure\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad4110a",
   "metadata": {
    "papermill": {
     "duration": 0.005483,
     "end_time": "2026-02-21T03:33:57.506161",
     "exception": false,
     "start_time": "2026-02-21T03:33:57.500678",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TIME-SERIES DATASET ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8457ad09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T03:33:57.519607Z",
     "iopub.status.busy": "2026-02-21T03:33:57.518868Z",
     "iopub.status.idle": "2026-02-21T03:33:57.634311Z",
     "shell.execute_reply": "2026-02-21T03:33:57.633243Z"
    },
    "papermill": {
     "duration": 0.124573,
     "end_time": "2026-02-21T03:33:57.636168",
     "exception": false,
     "start_time": "2026-02-21T03:33:57.511595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPACT PANEL SHAPE: (414, 29)\n",
      "Unique segments: 41\n",
      "Columns: 29\n",
      "\n",
      "STAGE 2 â€” ELITE SEGMENT PANEL READY\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 2 â€” ELITE SEGMENT PANEL (SAFE VERSION)\n",
    "# No KeyError â€¢ Auto-create missing columns â€¢ Short series safe\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ ENSURE REQUIRED SEGMENT COLUMNS EXIST\n",
    "# ============================================================\n",
    "\n",
    "# Care Type\n",
    "if \"care_type\" not in df.columns:\n",
    "    if \"inpatient_outpatient\" in df.columns:\n",
    "        df[\"care_type\"] = (\n",
    "            df[\"inpatient_outpatient\"]\n",
    "            .astype(str)\n",
    "            .str.upper()\n",
    "            .str.strip()\n",
    "        )\n",
    "    else:\n",
    "        df[\"care_type\"] = \"UNKNOWN\"\n",
    "\n",
    "df[\"care_type\"] = df[\"care_type\"].fillna(\"UNKNOWN\")\n",
    "\n",
    "\n",
    "# Cashless\n",
    "if \"is_cashless\" not in df.columns:\n",
    "    if \"reimburse_cashless\" in df.columns:\n",
    "        rc = df[\"reimburse_cashless\"].astype(str).str.upper().str.strip()\n",
    "        df[\"is_cashless\"] = rc.eq(\"C\").astype(int)\n",
    "    else:\n",
    "        df[\"is_cashless\"] = 0\n",
    "\n",
    "\n",
    "# RS Bucket\n",
    "if \"rs_bucket\" not in df.columns:\n",
    "    if \"lokasi_rs\" in df.columns:\n",
    "        loc = df[\"lokasi_rs\"].astype(str).str.upper().str.strip()\n",
    "        df[\"rs_bucket\"] = np.select(\n",
    "            [\n",
    "                loc.eq(\"INDONESIA\"),\n",
    "                loc.eq(\"SINGAPORE\"),\n",
    "                loc.eq(\"MALAYSIA\")\n",
    "            ],\n",
    "            [\"ID\",\"SG\",\"MY\"],\n",
    "            default=\"OTHER\"\n",
    "        )\n",
    "    else:\n",
    "        df[\"rs_bucket\"] = \"OTHER\"\n",
    "\n",
    "df[\"rs_bucket\"] = df[\"rs_bucket\"].fillna(\"OTHER\")\n",
    "\n",
    "\n",
    "# Plan Code\n",
    "if \"plan_code\" not in df.columns:\n",
    "    df[\"plan_code\"] = \"UNKNOWN\"\n",
    "\n",
    "df[\"plan_code\"] = df[\"plan_code\"].fillna(\"UNKNOWN\")\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ DEFINE SEGMENT COLUMNS\n",
    "# ============================================================\n",
    "\n",
    "seg_cols = [\"plan_code\",\"care_type\",\"is_cashless\",\"rs_bucket\"]\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ BUILD SEGMENT MONTHLY\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly = (\n",
    "    df.groupby([\"year_month\"] + seg_cols)\n",
    "      .agg(\n",
    "          frequency=(\"nomor_polis\",\"count\"),\n",
    "          total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\"),\n",
    "          exposure=(\"nomor_polis\",\"nunique\")\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(seg_cols + [\"year_month\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ TARGETS\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"severity\"] = (\n",
    "    seg_monthly[\"total_claim\"] /\n",
    "    seg_monthly[\"frequency\"].replace(0, np.nan)\n",
    ")\n",
    "\n",
    "seg_monthly[\"log_total\"] = np.log1p(seg_monthly[\"total_claim\"])\n",
    "seg_monthly[\"log_freq\"]  = np.log1p(seg_monthly[\"frequency\"])\n",
    "seg_monthly[\"log_sev\"]   = np.log1p(seg_monthly[\"severity\"])\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ CALENDAR\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"month\"] = seg_monthly[\"year_month\"].dt.month\n",
    "seg_monthly[\"month_sin\"] = np.sin(2*np.pi*seg_monthly[\"month\"]/12)\n",
    "seg_monthly[\"month_cos\"] = np.cos(2*np.pi*seg_monthly[\"month\"]/12)\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ LAGS (STRICT NO LEAKAGE)\n",
    "# ============================================================\n",
    "\n",
    "for col in [\"log_total\",\"log_freq\",\"log_sev\"]:\n",
    "    \n",
    "    seg_monthly[f\"{col}_lag1\"] = \\\n",
    "        seg_monthly.groupby(seg_cols)[col].shift(1)\n",
    "    \n",
    "    seg_monthly[f\"{col}_lag2\"] = \\\n",
    "        seg_monthly.groupby(seg_cols)[col].shift(2)\n",
    "    \n",
    "    seg_monthly[f\"{col}_lag3\"] = \\\n",
    "        seg_monthly.groupby(seg_cols)[col].shift(3)\n",
    "\n",
    "    seg_monthly[f\"{col}_roll3\"] = \\\n",
    "        seg_monthly.groupby(seg_cols)[col] \\\n",
    "        .transform(lambda x: x.shift(1).rolling(3).mean())\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ MOMENTUM\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"momentum_total\"] = (\n",
    "    seg_monthly[\"log_total_lag1\"] -\n",
    "    seg_monthly[\"log_total_lag2\"]\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ SEGMENT WEIGHT\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"seg_weight\"] = (\n",
    "    seg_monthly[\"frequency\"] /\n",
    "    seg_monthly.groupby(\"year_month\")[\"frequency\"].transform(\"sum\")\n",
    ").fillna(0)\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ SAFE TRAIN WINDOW\n",
    "# ============================================================\n",
    "\n",
    "seg_model = seg_monthly[\n",
    "    seg_monthly[\"log_total_lag3\"].notna()\n",
    "].reset_index(drop=True)\n",
    "\n",
    "seg_model = seg_model.fillna(0)\n",
    "\n",
    "# ============================================================\n",
    "# FINAL CHECK\n",
    "# ============================================================\n",
    "\n",
    "print(\"COMPACT PANEL SHAPE:\", seg_model.shape)\n",
    "print(\"Unique segments:\", seg_model[seg_cols].drop_duplicates().shape[0])\n",
    "print(\"Columns:\", len(seg_model.columns))\n",
    "print(\"\\nSTAGE 2 â€” ELITE SEGMENT PANEL READY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6e95e0",
   "metadata": {
    "papermill": {
     "duration": 0.005904,
     "end_time": "2026-02-21T03:33:57.647761",
     "exception": false,
     "start_time": "2026-02-21T03:33:57.641857",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MODEL DEVELOPMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0969c94f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T03:33:57.660800Z",
     "iopub.status.busy": "2026-02-21T03:33:57.660351Z",
     "iopub.status.idle": "2026-02-21T03:34:52.396546Z",
     "shell.execute_reply": "2026-02-21T03:34:52.395578Z"
    },
    "papermill": {
     "duration": 54.754381,
     "end_time": "2026-02-21T03:34:52.407645",
     "exception": false,
     "start_time": "2026-02-21T03:33:57.653264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Horizon months used : 5\n",
      "Best Config:\n",
      "  wt_total=0.85 (ETS weight), anchor_total=median\n",
      "  wt_freq =0.2 (ETS weight), anchor_freq =mean\n",
      "------------------------------\n",
      "STAGE 3 v17 MAPE Frequency : 5.1557\n",
      "STAGE 3 v17 MAPE Total     : 7.9753\n",
      "STAGE 3 v17 MAPE Severity  : 4.7684\n",
      "Estimated Score            : 5.9665\n",
      "==============================\n",
      "\n",
      "Preview last horizon months:\n",
      "   year_month  frequency   total_claim      severity  pred_frequency  \\\n",
      "14    2025-03        230  1.367924e+10  5.947496e+07      234.031716   \n",
      "15    2025-04        208  1.116425e+10  5.367427e+07      232.851773   \n",
      "16    2025-05        239  1.222680e+10  5.115814e+07      237.225688   \n",
      "17    2025-06        234  1.337312e+10  5.715008e+07      234.888808   \n",
      "18    2025-07        264  1.369923e+10  5.189101e+07      235.077202   \n",
      "\n",
      "      pred_total  pred_severity  \n",
      "14  1.224504e+10   5.232214e+07  \n",
      "15  1.224868e+10   5.260289e+07  \n",
      "16  1.222798e+10   5.154577e+07  \n",
      "17  1.221086e+10   5.198572e+07  \n",
      "18  1.219531e+10   5.187790e+07  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 3 v17 â€” KAGGLE-MATCH VALIDATION (AUTO-TUNED SHRINK)\n",
    "# - Horizon = unique months in sample_submission (usually 5)\n",
    "# - Predict TOTAL & FREQ directly (ETS on log1p), derive SEVERITY\n",
    "# - True recursive (refit each step on simulated history)\n",
    "# - Auto grid-search shrink weights + anchor type (mean/median)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\n",
    "sample_sub = pd.read_csv(BASE_PATH + \"sample_submission.csv\")\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "# ==============================\n",
    "# BUILD MONTHLY (consistent with Stage 1 v3)\n",
    "# ==============================\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(\n",
    "          frequency=(\"claim_id\",\"count\"),\n",
    "          total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\"),\n",
    "          exposure=(\"active_policies\",\"first\")\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(\"year_month\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "monthly[\"severity\"]   = monthly[\"total_claim\"] / monthly[\"frequency\"].replace(0, np.nan)\n",
    "monthly[\"claim_rate\"] = monthly[\"frequency\"] / monthly[\"exposure\"].replace(0, np.nan)\n",
    "\n",
    "# ==============================\n",
    "# HORIZON = months in sample_submission (Kaggle behavior)\n",
    "# ==============================\n",
    "sample_sub[\"year\"]  = sample_sub[\"id\"].str.split(\"_\").str[0]\n",
    "sample_sub[\"month\"] = sample_sub[\"id\"].str.split(\"_\").str[1]\n",
    "sample_sub[\"month_key\"] = sample_sub[\"year\"] + \"-\" + sample_sub[\"month\"]\n",
    "\n",
    "future_periods = (\n",
    "    pd.PeriodIndex(sample_sub[\"month_key\"], freq=\"M\")\n",
    "      .unique()\n",
    "      .sort_values()\n",
    ")\n",
    "\n",
    "H = int(len(future_periods))\n",
    "H = min(H, max(1, len(monthly) - 6))  # safety\n",
    "\n",
    "# ==============================\n",
    "# SIMULATOR (true recursive)\n",
    "# ==============================\n",
    "def simulate(train_df, H, wt_total, wt_freq, anchor_total=\"mean\", anchor_freq=\"mean\"):\n",
    "    sim_df = train_df.copy()\n",
    "\n",
    "    pred_total, pred_freq, pred_sev = [], [], []\n",
    "\n",
    "    for step in range(H):\n",
    "        hist = sim_df.copy()\n",
    "\n",
    "        # ---- TOTAL ETS on log1p ----\n",
    "        try:\n",
    "            mdl_t = ExponentialSmoothing(\n",
    "                np.log1p(hist[\"total_claim\"].astype(float)),\n",
    "                trend=\"add\",\n",
    "                damped_trend=True,\n",
    "                seasonal=None\n",
    "            ).fit()\n",
    "            total_fc = float(np.expm1(mdl_t.forecast(1).iloc[0]))\n",
    "        except:\n",
    "            total_fc = float(hist[\"total_claim\"].iloc[-1])\n",
    "\n",
    "        # anchor total\n",
    "        if anchor_total == \"median\":\n",
    "            total_anchor = float(hist[\"total_claim\"].tail(3).median())\n",
    "        else:\n",
    "            total_anchor = float(hist[\"total_claim\"].tail(3).mean())\n",
    "\n",
    "        total_pred = wt_total * total_fc + (1 - wt_total) * total_anchor\n",
    "        total_pred = max(float(total_pred), 1.0)\n",
    "\n",
    "        # ---- FREQ ETS on log1p ----\n",
    "        try:\n",
    "            mdl_f = ExponentialSmoothing(\n",
    "                np.log1p(hist[\"frequency\"].astype(float)),\n",
    "                trend=\"add\",\n",
    "                damped_trend=True,\n",
    "                seasonal=None\n",
    "            ).fit()\n",
    "            freq_fc = float(np.expm1(mdl_f.forecast(1).iloc[0]))\n",
    "        except:\n",
    "            freq_fc = float(hist[\"frequency\"].iloc[-1])\n",
    "\n",
    "        # anchor freq\n",
    "        if anchor_freq == \"median\":\n",
    "            freq_anchor = float(hist[\"frequency\"].tail(3).median())\n",
    "        else:\n",
    "            freq_anchor = float(hist[\"frequency\"].tail(3).mean())\n",
    "\n",
    "        freq_pred = wt_freq * freq_fc + (1 - wt_freq) * freq_anchor\n",
    "        freq_pred = max(float(freq_pred), 1.0)\n",
    "\n",
    "        sev_pred = total_pred / freq_pred\n",
    "\n",
    "        pred_total.append(total_pred)\n",
    "        pred_freq.append(freq_pred)\n",
    "        pred_sev.append(sev_pred)\n",
    "\n",
    "        # ---- append recursive row (keep year_month progressing) ----\n",
    "        last_period = hist[\"year_month\"].iloc[-1]\n",
    "        next_period = last_period + 1\n",
    "        exposure_next = float(hist[\"exposure\"].iloc[-1]) if \"exposure\" in hist.columns else np.nan\n",
    "\n",
    "        sim_df = pd.concat([sim_df, pd.DataFrame([{\n",
    "            \"year_month\": next_period,\n",
    "            \"frequency\": freq_pred,\n",
    "            \"total_claim\": total_pred,\n",
    "            \"exposure\": exposure_next,\n",
    "            \"severity\": sev_pred,\n",
    "            \"claim_rate\": (freq_pred / exposure_next) if (exposure_next and exposure_next > 0) else np.nan\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    return pred_total, pred_freq, pred_sev\n",
    "\n",
    "# ==============================\n",
    "# SPLIT (Kaggle-match horizon)\n",
    "# ==============================\n",
    "train = monthly.iloc[:-H].copy()\n",
    "valid = monthly.iloc[-H:].copy()\n",
    "\n",
    "# ==============================\n",
    "# AUTO SEARCH (small grid, fast)\n",
    "# ==============================\n",
    "wt_total_grid = [0.35, 0.45, 0.55, 0.60, 0.65, 0.75, 0.85]\n",
    "wt_freq_grid  = [0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80]\n",
    "\n",
    "best = {\n",
    "    \"score\": 1e18,\n",
    "    \"params\": None,\n",
    "    \"detail\": None\n",
    "}\n",
    "\n",
    "for wt_t in wt_total_grid:\n",
    "    for wt_f in wt_freq_grid:\n",
    "        for a_t in [\"mean\", \"median\"]:\n",
    "            for a_f in [\"mean\", \"median\"]:\n",
    "\n",
    "                pt, pf, ps = simulate(train, H, wt_t, wt_f, a_t, a_f)\n",
    "\n",
    "                mf = mape(valid[\"frequency\"], pf)\n",
    "                mt = mape(valid[\"total_claim\"], pt)\n",
    "                ms = mape(valid[\"severity\"], ps)\n",
    "                avg = float(np.nanmean([mf, mt, ms]))\n",
    "\n",
    "                if avg < best[\"score\"]:\n",
    "                    best[\"score\"] = avg\n",
    "                    best[\"params\"] = (wt_t, wt_f, a_t, a_f)\n",
    "                    best[\"detail\"] = (mf, mt, ms)\n",
    "\n",
    "# ==============================\n",
    "# RUN BEST + REPORT\n",
    "# ==============================\n",
    "wt_t, wt_f, a_t, a_f = best[\"params\"]\n",
    "pt, pf, ps = simulate(train, H, wt_t, wt_f, a_t, a_f)\n",
    "\n",
    "mf, mt, ms = best[\"detail\"]\n",
    "avg = best[\"score\"]\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(f\"Horizon months used : {H}\")\n",
    "print(\"Best Config:\")\n",
    "print(f\"  wt_total={wt_t} (ETS weight), anchor_total={a_t}\")\n",
    "print(f\"  wt_freq ={wt_f} (ETS weight), anchor_freq ={a_f}\")\n",
    "print(\"------------------------------\")\n",
    "print(\"STAGE 3 v17 MAPE Frequency :\", round(mf, 4))\n",
    "print(\"STAGE 3 v17 MAPE Total     :\", round(mt, 4))\n",
    "print(\"STAGE 3 v17 MAPE Severity  :\", round(ms, 4))\n",
    "print(\"Estimated Score            :\", round(avg, 4))\n",
    "print(\"==============================\")\n",
    "\n",
    "check = valid[[\"year_month\",\"frequency\",\"total_claim\",\"severity\"]].copy()\n",
    "check[\"pred_frequency\"] = pf\n",
    "check[\"pred_total\"] = pt\n",
    "check[\"pred_severity\"] = ps\n",
    "print(\"\\nPreview last horizon months:\")\n",
    "print(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c974075d",
   "metadata": {
    "papermill": {
     "duration": 0.008985,
     "end_time": "2026-02-21T03:34:52.425687",
     "exception": false,
     "start_time": "2026-02-21T03:34:52.416702",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TOTAL CLAIM OPTIMIZATION & VALIDATION, OPTUNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9113ec6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T03:34:52.447372Z",
     "iopub.status.busy": "2026-02-21T03:34:52.446966Z",
     "iopub.status.idle": "2026-02-21T03:42:02.548243Z",
     "shell.execute_reply": "2026-02-21T03:42:02.546275Z"
    },
    "papermill": {
     "duration": 430.115617,
     "end_time": "2026-02-21T03:42:02.550537",
     "exception": false,
     "start_time": "2026-02-21T03:34:52.434920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N months: 19 | Horizon H: 5 | Has exposure: True | Future MOY: [8, 9, 10, 11, 12]\n",
      "CV train_ends: [7, 8, 13, 14] | weights: [0.401, 0.343, 0.123, 0.133]\n",
      "  split te= 7 | valid months: ['2024-08', '2024-09', '2024-10', '2024-11', '2024-12']\n",
      "  split te= 8 | valid months: ['2024-09', '2024-10', '2024-11', '2024-12', '2025-01']\n",
      "  split te= 13 | valid months: ['2025-02', '2025-03', '2025-04', '2025-05', '2025-06']\n",
      "  split te= 14 | valid months: ['2025-03', '2025-04', '2025-05', '2025-06', '2025-07']\n",
      "Baseline CV %: 11.5766\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe18d02b52c4b4da114ef3722a98903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Horizon: 5 | Splits: [7, 8, 13, 14] | weights: [0.401, 0.343, 0.123, 0.133]\n",
      "Best Params: {'k_anchor': 6, 'anchor_rate': 'mean', 'anchor_sev': 'mean', 'wt_rate': 0.5886366695116942, 'wt_sev': 0.3579173577972926, 'beta': 0.9218093484579944, 'damped': True, 'init_method': 'heuristic', 'capR_low': 0.8718563627174186, 'capR_high': 1.2295914190903159, 'capS_low': 0.8363199994736286, 'capS_high': 1.9567160683078664, 'season_w': 7.233033409505144e-05, 'season_how': 'mean', 'season_cap_low': 0.8414284311003727, 'season_cap_high': 1.224730419283397, 'round_freq': False}\n",
      "CV Best %  : 9.5159\n",
      "==============================\n",
      "\n",
      "Per-split metrics (%):\n",
      "               avg  mape_total  mape_freq  mape_sev  stab_pen\n",
      "train_end                                                    \n",
      "7           8.2643      6.4960    10.4590    7.8379    1.7876\n",
      "8          11.3648     11.3639     9.8619   12.8686    2.1538\n",
      "13         11.6766     14.6912     4.6199   15.7187    2.3723\n",
      "14          6.3561      7.4995     6.2268    5.3420    1.7607\n",
      "\n",
      "STAGE 4 v24 â€” READY (season-aware)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 4 v24 â€” SEASON-AWARE RATE+SEV (EXPOSURE) + ETS ENSEMBLE + OPTUNA\n",
    "# - CV splits otomatis: include window yang month-of-year mirip future (Augâ€“Dec)\n",
    "# - Clamp range dilonggarkan (anti \"too flat\")\n",
    "# - Optional month-of-year seasonal adjustment (train-only, no leakage)\n",
    "# ============================================================\n",
    "\n",
    "!pip install -q optuna statsmodels\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\n",
    "sample_sub = pd.read_csv(BASE_PATH + \"sample_submission.csv\")\n",
    "\n",
    "assert \"df\" in globals(), \"Variabel df belum ada. Jalankan Stage 1 dulu.\"\n",
    "assert \"year_month\" in df.columns, \"df['year_month'] belum ada. Buat dulu: df['year_month']=tanggal.dt.to_period('M')\"\n",
    "\n",
    "# ------------------------------\n",
    "# MAPE (fraction)\n",
    "# ------------------------------\n",
    "def mape_frac(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred) & (y_true != 0)\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return float(np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])))\n",
    "\n",
    "# ------------------------------\n",
    "# Horizon from sample_submission (Kaggle)\n",
    "# ------------------------------\n",
    "sample_sub[\"year\"]  = sample_sub[\"id\"].str.split(\"_\").str[0]\n",
    "sample_sub[\"month\"] = sample_sub[\"id\"].str.split(\"_\").str[1]\n",
    "sample_sub[\"month_key\"] = sample_sub[\"year\"] + \"-\" + sample_sub[\"month\"]\n",
    "\n",
    "future_periods = (\n",
    "    pd.PeriodIndex(sample_sub[\"month_key\"], freq=\"M\")\n",
    "      .unique()\n",
    "      .sort_values()\n",
    ")\n",
    "H = int(len(future_periods))\n",
    "future_moy = set([p.month for p in future_periods])\n",
    "\n",
    "# ------------------------------\n",
    "# BUILD MONTHLY (reindex full months, robust)\n",
    "# ------------------------------\n",
    "has_exposure = \"active_policies\" in df.columns\n",
    "\n",
    "agg_dict = {\n",
    "    \"frequency\": (\"claim_id\",\"count\"),\n",
    "    \"total_claim\": (\"nominal_klaim_yang_disetujui\",\"sum\"),\n",
    "}\n",
    "if has_exposure:\n",
    "    agg_dict[\"exposure\"] = (\"active_policies\",\"first\")\n",
    "\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(**agg_dict)\n",
    "      .reset_index()\n",
    "      .sort_values(\"year_month\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ensure Period[M]\n",
    "if len(monthly) == 0:\n",
    "    raise ValueError(\"monthly kosong. Cek df/year_month.\")\n",
    "if not isinstance(monthly.loc[0, \"year_month\"], pd.Period):\n",
    "    monthly[\"year_month\"] = pd.PeriodIndex(monthly[\"year_month\"], freq=\"M\")\n",
    "\n",
    "# reindex complete month range\n",
    "min_m = monthly[\"year_month\"].min()\n",
    "max_m = monthly[\"year_month\"].max()\n",
    "all_months = pd.period_range(min_m, max_m, freq=\"M\")\n",
    "\n",
    "monthly = (\n",
    "    monthly.set_index(\"year_month\")\n",
    "           .reindex(all_months)\n",
    "           .rename_axis(\"year_month\")\n",
    "           .reset_index()\n",
    ")\n",
    "\n",
    "# fill missing frequency/total with 0; exposure forward/back fill\n",
    "monthly[\"frequency\"]   = pd.to_numeric(monthly[\"frequency\"], errors=\"coerce\").fillna(0.0)\n",
    "monthly[\"total_claim\"] = pd.to_numeric(monthly[\"total_claim\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "if not has_exposure:\n",
    "    monthly[\"exposure\"] = float(np.nanmean(monthly[\"frequency\"])) * 10.0\n",
    "else:\n",
    "    monthly[\"exposure\"] = pd.to_numeric(monthly[\"exposure\"], errors=\"coerce\")\n",
    "\n",
    "monthly[\"exposure\"] = monthly[\"exposure\"].ffill().bfill()\n",
    "monthly[\"exposure\"] = monthly[\"exposure\"].fillna(float(np.nanmean(monthly[\"frequency\"])) * 10.0)\n",
    "\n",
    "# safety clip (level series must be >0 for log work)\n",
    "monthly[\"frequency\"]   = monthly[\"frequency\"].astype(float).clip(lower=1.0)\n",
    "monthly[\"total_claim\"] = monthly[\"total_claim\"].astype(float).clip(lower=1.0)\n",
    "monthly[\"exposure\"]    = monthly[\"exposure\"].astype(float).clip(lower=1.0)\n",
    "\n",
    "# components\n",
    "monthly[\"severity\"]   = (monthly[\"total_claim\"] / monthly[\"frequency\"]).astype(float).clip(lower=1e-9)\n",
    "monthly[\"claim_rate\"] = (monthly[\"frequency\"]   / monthly[\"exposure\"]).astype(float).clip(lower=1e-12)\n",
    "monthly[\"month\"]      = monthly[\"year_month\"].dt.month\n",
    "\n",
    "N = len(monthly)\n",
    "H = min(H, max(1, N - 10))   # sedikit lebih longgar supaya split season bisa masuk\n",
    "print(\"N months:\", N, \"| Horizon H:\", H, \"| Has exposure:\", has_exposure, \"| Future MOY:\", sorted(list(future_moy)))\n",
    "\n",
    "# ------------------------------\n",
    "# Choose CV splits (season-aware)\n",
    "# - kandidat: semua train_end yang memungkinkan valid H bulan\n",
    "# - scoring: overlap bulan valid dengan future months (Aug-Dec) + recency\n",
    "# ------------------------------\n",
    "min_train = 7  # supaya split Aug-Dec 2024 bisa masuk (train Jan-Jul 2024)\n",
    "cands = []\n",
    "for te in range(min_train, N - H + 1):\n",
    "    valid = monthly.iloc[te:te+H]\n",
    "    if len(valid) < H:\n",
    "        continue\n",
    "    overlap = sum([1 for m in valid[\"month\"].tolist() if m in future_moy]) / float(H)\n",
    "    recency = te / float(N)\n",
    "    score = 0.65*overlap + 0.35*recency\n",
    "    cands.append((score, overlap, recency, te))\n",
    "\n",
    "if len(cands) == 0:\n",
    "    raise ValueError(\"Tidak ada split CV yang valid. Cek N/H.\")\n",
    "\n",
    "# ambil top splits: 1-2 yang paling season-match + 1-2 yang paling recent\n",
    "cands_sorted = sorted(cands, reverse=True)\n",
    "\n",
    "top_season = sorted(cands, key=lambda x: (x[1], x[0]), reverse=True)[:2]\n",
    "top_recent = sorted(cands, key=lambda x: x[2], reverse=True)[:2]\n",
    "picked = {x[3] for x in (top_season + top_recent)}\n",
    "\n",
    "train_ends = sorted(list(picked))\n",
    "# weights: proporsional score\n",
    "w_raw = np.array([dict((x[3], x[0]) for x in cands_sorted).get(te, 0.1) for te in train_ends], dtype=float)\n",
    "w_raw = np.maximum(w_raw, 1e-6)\n",
    "split_w = w_raw / w_raw.sum()\n",
    "\n",
    "print(\"CV train_ends:\", train_ends, \"| weights:\", split_w.round(3).tolist())\n",
    "for te in train_ends:\n",
    "    v = monthly.iloc[te:te+H][[\"year_month\",\"month\"]]\n",
    "    print(\"  split te=\", te, \"| valid months:\", v[\"year_month\"].astype(str).tolist())\n",
    "\n",
    "# ------------------------------\n",
    "# Helpers\n",
    "# ------------------------------\n",
    "def ets_1step_log(x_log: pd.Series, trend, damped, init_method):\n",
    "    # extra safety for short series\n",
    "    if len(x_log) < 4:\n",
    "        return float(x_log.iloc[-1])\n",
    "    # for short series, trend add sering unstable\n",
    "    if trend is not None and len(x_log) < 10:\n",
    "        trend = None\n",
    "        damped = False\n",
    "    try:\n",
    "        m = ExponentialSmoothing(\n",
    "            x_log,\n",
    "            trend=trend,\n",
    "            damped_trend=(damped if trend is not None else False),\n",
    "            seasonal=None,\n",
    "            initialization_method=init_method\n",
    "        ).fit()\n",
    "        return float(m.forecast(1).iloc[0])\n",
    "    except:\n",
    "        return float(x_log.iloc[-1])\n",
    "\n",
    "def anchor_level(x_level: pd.Series, k: int, how: str):\n",
    "    tail = np.asarray(x_level.tail(k), dtype=float)\n",
    "    if len(tail) == 0:\n",
    "        return float(np.nan)\n",
    "    return float(np.median(tail)) if how == \"median\" else float(np.mean(tail))\n",
    "\n",
    "def seasonal_factor(hist: pd.DataFrame, col: str, m_next: int, how: str, cap_low: float, cap_high: float):\n",
    "    x = hist[[col, \"month\"]].copy()\n",
    "    x = x[np.isfinite(x[col].values)]\n",
    "    if len(x) < 6:\n",
    "        return 1.0\n",
    "    if how == \"median\":\n",
    "        overall = float(np.median(x[col].values))\n",
    "        mvals = x.loc[x[\"month\"] == m_next, col].values\n",
    "        mm = float(np.median(mvals)) if len(mvals) else np.nan\n",
    "    else:\n",
    "        overall = float(np.mean(x[col].values))\n",
    "        mvals = x.loc[x[\"month\"] == m_next, col].values\n",
    "        mm = float(np.mean(mvals)) if len(mvals) else np.nan\n",
    "\n",
    "    if (not np.isfinite(overall)) or overall <= 0 or (not np.isfinite(mm)) or mm <= 0:\n",
    "        return 1.0\n",
    "\n",
    "    fac = mm / overall\n",
    "    fac = float(np.clip(fac, cap_low, cap_high))\n",
    "    return fac\n",
    "\n",
    "# ------------------------------\n",
    "# One split TRUE RECURSIVE\n",
    "# ------------------------------\n",
    "def run_split(monthly_all: pd.DataFrame, train_end: int, H: int, P: dict):\n",
    "    train = monthly_all.iloc[:train_end].copy().reset_index(drop=True)\n",
    "    valid = monthly_all.iloc[train_end:train_end+H].copy().reset_index(drop=True)\n",
    "    if len(valid) < H or len(train) < 4:\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    sim = train.copy()\n",
    "\n",
    "    pred_F, pred_T, pred_S = [], [], []\n",
    "    pen = []\n",
    "\n",
    "    for step in range(H):\n",
    "        k = int(P[\"k_anchor\"])\n",
    "\n",
    "        exp_next = float(sim[\"exposure\"].iloc[-1])\n",
    "        exp_next = max(1.0, exp_next)\n",
    "\n",
    "        # anchors (LEVEL)\n",
    "        aR = anchor_level(sim[\"claim_rate\"], k, P[\"anchor_rate\"])\n",
    "        aS = anchor_level(sim[\"severity\"],   k, P[\"anchor_sev\"])\n",
    "\n",
    "        # build log series for ETS\n",
    "        lr = np.log(sim[\"claim_rate\"].astype(float).clip(lower=1e-12))\n",
    "        ls = np.log(sim[\"severity\"].astype(float).clip(lower=1e-12))\n",
    "\n",
    "        # ETS ensemble for rate\n",
    "        lr_add  = ets_1step_log(lr, trend=\"add\",  damped=bool(P[\"damped\"]), init_method=P[\"init_method\"])\n",
    "        lr_none = ets_1step_log(lr, trend=None,   damped=False,             init_method=P[\"init_method\"])\n",
    "        lr_hat  = float(P[\"beta\"])*lr_add + (1-float(P[\"beta\"]))*lr_none\n",
    "        r_fc    = float(np.exp(lr_hat))\n",
    "\n",
    "        # ETS ensemble for severity\n",
    "        ls_add  = ets_1step_log(ls, trend=\"add\",  damped=bool(P[\"damped\"]), init_method=P[\"init_method\"])\n",
    "        ls_none = ets_1step_log(ls, trend=None,   damped=False,             init_method=P[\"init_method\"])\n",
    "        ls_hat  = float(P[\"beta\"])*ls_add + (1-float(P[\"beta\"]))*ls_none\n",
    "        s_fc    = float(np.exp(ls_hat))\n",
    "\n",
    "        # shrink to anchor\n",
    "        r_pred = float(P[\"wt_rate\"])*r_fc + (1-float(P[\"wt_rate\"]))*aR\n",
    "        s_pred = float(P[\"wt_sev\"]) *s_fc + (1-float(P[\"wt_sev\"])) *aS\n",
    "\n",
    "        # seasonal adjustment (train-only)\n",
    "        next_period = sim[\"year_month\"].iloc[-1] + 1\n",
    "        m_next = int(next_period.month)\n",
    "        if float(P[\"season_w\"]) > 0:\n",
    "            fac_r = seasonal_factor(sim, \"claim_rate\", m_next, P[\"season_how\"], P[\"season_cap_low\"], P[\"season_cap_high\"])\n",
    "            fac_s = seasonal_factor(sim, \"severity\",   m_next, P[\"season_how\"], P[\"season_cap_low\"], P[\"season_cap_high\"])\n",
    "            # apply softly (power)\n",
    "            r_pred *= float(fac_r) ** float(P[\"season_w\"])\n",
    "            s_pred *= float(fac_s) ** float(P[\"season_w\"])\n",
    "\n",
    "        # clamp ratio vs anchor (dilonggarkan)\n",
    "        r_pred = float(np.clip(r_pred, aR*float(P[\"capR_low\"]), aR*float(P[\"capR_high\"])))\n",
    "        s_pred = float(np.clip(s_pred, aS*float(P[\"capS_low\"]), aS*float(P[\"capS_high\"])))\n",
    "\n",
    "        # reconstruct\n",
    "        f_pred = float(max(1.0, r_pred * exp_next))\n",
    "        if bool(P[\"round_freq\"]):\n",
    "            f_pred = float(max(1.0, np.round(f_pred)))\n",
    "\n",
    "        t_pred = float(max(1.0, f_pred * s_pred))\n",
    "        s_pred = float(max(1e-9, t_pred / max(1.0, f_pred)))\n",
    "\n",
    "        pred_F.append(f_pred)\n",
    "        pred_T.append(t_pred)\n",
    "        pred_S.append(s_pred)\n",
    "\n",
    "        # penalty drift vs anchor implied\n",
    "        aF = float(max(1.0, (aR * exp_next)))\n",
    "        if bool(P[\"round_freq\"]):\n",
    "            aF = float(max(1.0, np.round(aF)))\n",
    "        aT = float(max(1.0, aF * aS))\n",
    "        pen_f = abs(f_pred - aF) / (abs(aF) + 1e-9)\n",
    "        pen_t = abs(t_pred - aT) / (abs(aT) + 1e-9)\n",
    "        pen.append(0.5*(pen_f + pen_t))\n",
    "\n",
    "        # append recursive row\n",
    "        sim = pd.concat([sim, pd.DataFrame([{\n",
    "            \"year_month\": next_period,\n",
    "            \"month\": m_next,\n",
    "            \"exposure\": exp_next,\n",
    "            \"claim_rate\": float(max(1e-12, f_pred/exp_next)),\n",
    "            \"severity\": s_pred,\n",
    "            \"frequency\": f_pred,\n",
    "            \"total_claim\": t_pred\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    # targets\n",
    "    yF = valid[\"frequency\"].astype(float).values\n",
    "    yT = valid[\"total_claim\"].astype(float).values\n",
    "    yS = (valid[\"total_claim\"].astype(float).values /\n",
    "          np.clip(valid[\"frequency\"].astype(float).values, 1.0, np.inf))\n",
    "\n",
    "    mf = mape_frac(yF, pred_F)\n",
    "    mt = mape_frac(yT, pred_T)\n",
    "    ms = mape_frac(yS, pred_S)\n",
    "    avg = float(np.nanmean([mf, mt, ms]))\n",
    "    stab = float(np.mean(pen)) if len(pen) else np.nan\n",
    "    return avg, mt, mf, ms, stab\n",
    "\n",
    "# ------------------------------\n",
    "# Baseline params (lebih fleksibel)\n",
    "# ------------------------------\n",
    "P0 = dict(\n",
    "    k_anchor=4,\n",
    "    anchor_rate=\"mean\",\n",
    "    anchor_sev=\"mean\",\n",
    "    wt_rate=0.30,\n",
    "    wt_sev=0.75,\n",
    "    beta=0.80,\n",
    "    damped=True,\n",
    "    init_method=\"estimated\",\n",
    "\n",
    "    # clamp (lebih longgar dari v23)\n",
    "    capR_low=0.70, capR_high=1.40,\n",
    "    capS_low=0.60, capS_high=1.60,\n",
    "\n",
    "    # seasonal\n",
    "    season_w=0.35,\n",
    "    season_how=\"median\",\n",
    "    season_cap_low=0.85,\n",
    "    season_cap_high=1.15,\n",
    "\n",
    "    round_freq=False,\n",
    ")\n",
    "\n",
    "def cv_score(P, pen_w=0.01):\n",
    "    cv = 0.0\n",
    "    pen = 0.0\n",
    "    ok = 0\n",
    "    for te, w in zip(train_ends, split_w):\n",
    "        avg, _, _, _, stab = run_split(monthly, te, H, P)\n",
    "        if np.isfinite(avg):\n",
    "            cv += w*avg\n",
    "            ok += 1\n",
    "        if np.isfinite(stab):\n",
    "            pen += w*stab\n",
    "    if ok == 0:\n",
    "        return 1e9\n",
    "    return float(cv + pen_w*pen)\n",
    "\n",
    "print(\"Baseline CV %:\", round(cv_score(P0)*100, 4))\n",
    "\n",
    "# ------------------------------\n",
    "# OPTUNA objective\n",
    "# ------------------------------\n",
    "PEN_W = 0.01\n",
    "\n",
    "def objective(trial):\n",
    "    P = dict(\n",
    "        k_anchor=trial.suggest_int(\"k_anchor\", 2, 6),\n",
    "        anchor_rate=trial.suggest_categorical(\"anchor_rate\", [\"mean\",\"median\"]),\n",
    "        anchor_sev=trial.suggest_categorical(\"anchor_sev\", [\"mean\",\"median\"]),\n",
    "\n",
    "        wt_rate=trial.suggest_float(\"wt_rate\", 0.05, 0.65),\n",
    "        wt_sev=trial.suggest_float(\"wt_sev\",  0.35, 0.98),\n",
    "\n",
    "        beta=trial.suggest_float(\"beta\", 0.25, 0.95),\n",
    "        damped=trial.suggest_categorical(\"damped\", [False, True]),\n",
    "        init_method=trial.suggest_categorical(\"init_method\", [\"estimated\",\"heuristic\"]),\n",
    "\n",
    "        # clamp (lebih lebar, terutama severity)\n",
    "        capR_low=trial.suggest_float(\"capR_low\", 0.55, 0.90),\n",
    "        capR_high=trial.suggest_float(\"capR_high\", 1.15, 1.70),\n",
    "        capS_low=trial.suggest_float(\"capS_low\", 0.45, 0.85),\n",
    "        capS_high=trial.suggest_float(\"capS_high\", 1.15, 2.00),\n",
    "\n",
    "        # seasonal (tuning ringan)\n",
    "        season_w=trial.suggest_float(\"season_w\", 0.0, 0.75),\n",
    "        season_how=trial.suggest_categorical(\"season_how\", [\"mean\",\"median\"]),\n",
    "        season_cap_low=trial.suggest_float(\"season_cap_low\", 0.80, 0.95),\n",
    "        season_cap_high=trial.suggest_float(\"season_cap_high\", 1.05, 1.25),\n",
    "\n",
    "        round_freq=trial.suggest_categorical(\"round_freq\", [False, True]),\n",
    "    )\n",
    "\n",
    "    cv = 0.0\n",
    "    pen = 0.0\n",
    "    ok = 0\n",
    "    for te, w in zip(train_ends, split_w):\n",
    "        avg, _, _, _, stab = run_split(monthly, te, H, P)\n",
    "        if np.isfinite(avg):\n",
    "            cv += w*avg\n",
    "            ok += 1\n",
    "        if np.isfinite(stab):\n",
    "            pen += w*stab\n",
    "\n",
    "    if ok == 0:\n",
    "        return 1e9\n",
    "    return float(cv + PEN_W*pen)\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "\n",
    "study.enqueue_trial(P0)\n",
    "study.optimize(objective, n_trials=500, show_progress_bar=True)\n",
    "\n",
    "bestP = study.best_params\n",
    "print(\"\\n==============================\")\n",
    "print(\"Horizon:\", H, \"| Splits:\", train_ends, \"| weights:\", split_w.round(3).tolist())\n",
    "print(\"Best Params:\", bestP)\n",
    "print(\"CV Best %  :\", round(study.best_value*100, 4))\n",
    "print(\"==============================\")\n",
    "\n",
    "# per-split report\n",
    "rows = []\n",
    "for te in train_ends:\n",
    "    avg, mt, mf, ms, stab = run_split(monthly, te, H, bestP)\n",
    "    rows.append([te, avg, mt, mf, ms, stab])\n",
    "\n",
    "spl = pd.DataFrame(rows, columns=[\"train_end\",\"avg\",\"mape_total\",\"mape_freq\",\"mape_sev\",\"stab_pen\"])\n",
    "print(\"\\nPer-split metrics (%):\")\n",
    "print((spl.set_index(\"train_end\")[[\"avg\",\"mape_total\",\"mape_freq\",\"mape_sev\",\"stab_pen\"]]*100).round(4))\n",
    "\n",
    "print(\"\\nSTAGE 4 v24 â€” READY (season-aware)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1168aaa",
   "metadata": {
    "papermill": {
     "duration": 0.009541,
     "end_time": "2026-02-21T03:42:02.570427",
     "exception": false,
     "start_time": "2026-02-21T03:42:02.560886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TEST PREDICTION & KAGGLE SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e067a54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T03:42:02.594412Z",
     "iopub.status.busy": "2026-02-21T03:42:02.592701Z",
     "iopub.status.idle": "2026-02-21T03:42:49.390503Z",
     "shell.execute_reply": "2026-02-21T03:42:49.388408Z"
    },
    "papermill": {
     "duration": 46.813047,
     "end_time": "2026-02-21T03:42:49.393571",
     "exception": false,
     "start_time": "2026-02-21T03:42:02.580524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N months: 19 | H: 5 | Has exposure: True | Future MOY: [8, 9, 10, 11, 12]\n",
      "CV train_ends: [7, 8, 13, 14] | weights: [0.401, 0.343, 0.123, 0.133]\n",
      "  split te= 7 | valid months: ['2024-08', '2024-09', '2024-10', '2024-11', '2024-12']\n",
      "  split te= 8 | valid months: ['2024-09', '2024-10', '2024-11', '2024-12', '2025-01']\n",
      "  split te= 13 | valid months: ['2025-02', '2025-03', '2025-04', '2025-05', '2025-06']\n",
      "  split te= 14 | valid months: ['2025-03', '2025-04', '2025-05', '2025-06', '2025-07']\n",
      "\n",
      "Top candidates by mean_avg:\n",
      "   w_stage4  mean_avg  median_avg  worst_avg   split_0   split_1   split_2  \\\n",
      "0      0.00  0.090516    0.096506   0.109386  0.086339  0.109386  0.106673   \n",
      "1      0.05  0.090949    0.096635   0.110987  0.086303  0.110987  0.106967   \n",
      "2      0.10  0.091390    0.096764   0.112587  0.086267  0.112587  0.107261   \n",
      "3      0.15  0.091831    0.096892   0.114187  0.086230  0.114187  0.107553   \n",
      "4      0.20  0.092285    0.097045   0.115785  0.086193  0.115785  0.107897   \n",
      "5      0.25  0.092770    0.097261   0.117383  0.086156  0.117383  0.108367   \n",
      "6      0.30  0.093254    0.097477   0.118979  0.086118  0.118979  0.108836   \n",
      "7      0.35  0.093738    0.097692   0.120575  0.086079  0.120575  0.109304   \n",
      "\n",
      "    split_3  \n",
      "0  0.059665  \n",
      "1  0.059539  \n",
      "2  0.059447  \n",
      "3  0.059355  \n",
      "4  0.059264  \n",
      "5  0.059173  \n",
      "6  0.059083  \n",
      "7  0.058994  \n",
      "\n",
      "Top candidates by median_avg:\n",
      "   w_stage4  mean_avg  median_avg  worst_avg   split_0   split_1   split_2  \\\n",
      "0      0.00  0.090516    0.096506   0.109386  0.086339  0.109386  0.106673   \n",
      "1      0.05  0.090949    0.096635   0.110987  0.086303  0.110987  0.106967   \n",
      "2      0.10  0.091390    0.096764   0.112587  0.086267  0.112587  0.107261   \n",
      "3      0.15  0.091831    0.096892   0.114187  0.086230  0.114187  0.107553   \n",
      "4      0.20  0.092285    0.097045   0.115785  0.086193  0.115785  0.107897   \n",
      "5      0.25  0.092770    0.097261   0.117383  0.086156  0.117383  0.108367   \n",
      "6      0.30  0.093254    0.097477   0.118979  0.086118  0.118979  0.108836   \n",
      "7      0.35  0.093738    0.097692   0.120575  0.086079  0.120575  0.109304   \n",
      "\n",
      "    split_3  \n",
      "0  0.059665  \n",
      "1  0.059539  \n",
      "2  0.059447  \n",
      "3  0.059355  \n",
      "4  0.059264  \n",
      "5  0.059173  \n",
      "6  0.059083  \n",
      "7  0.058994  \n",
      "\n",
      "Chosen w_stage4: 0.0 (w=1 pure Stage4, w=0 pure Stage3)\n",
      "NaN in submission: 0\n",
      "\n",
      "Preview future predictions:\n",
      "    period   pred_freq    pred_total      pred_sev\n",
      "0  2025-08  243.565185  1.257064e+10  5.161099e+07\n",
      "1  2025-09  244.925261  1.257311e+10  5.133448e+07\n",
      "2  2025-10  247.950462  1.245642e+10  5.023752e+07\n",
      "3  2025-11  243.827599  1.245245e+10  5.107072e+07\n",
      "4  2025-12  243.980696  1.243264e+10  5.095746e+07\n",
      "\n",
      "Saved: submission.csv\n",
      "                         id         value\n",
      "0   2025_08_Claim_Frequency  2.435652e+02\n",
      "1    2025_08_Claim_Severity  5.161099e+07\n",
      "2       2025_08_Total_Claim  1.257064e+10\n",
      "3   2025_09_Claim_Frequency  2.449253e+02\n",
      "4    2025_09_Claim_Severity  5.133448e+07\n",
      "5       2025_09_Total_Claim  1.257311e+10\n",
      "6   2025_10_Claim_Frequency  2.479505e+02\n",
      "7    2025_10_Claim_Severity  5.023752e+07\n",
      "8       2025_10_Total_Claim  1.245642e+10\n",
      "9   2025_11_Claim_Frequency  2.438276e+02\n",
      "10   2025_11_Claim_Severity  5.107072e+07\n",
      "11      2025_11_Total_Claim  1.245245e+10\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 5 v26 â€” ROBUST ENSEMBLE (Stage3 + Stage4) + SEASON-AWARE CV + MEAN BLEND\n",
    "# Changes vs v25:\n",
    "# - CV splits season-aware (overlap months with Augâ€“Dec) + recent\n",
    "# - Choose blend weight by MEAN / MEDIAN (not worst-case)\n",
    "# - Avoid extreme w=1 unless clearly better\n",
    "# ============================================================\n",
    "\n",
    "!pip install -q statsmodels\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\n",
    "sample_sub = pd.read_csv(BASE_PATH + \"sample_submission.csv\")\n",
    "\n",
    "assert \"df\" in globals(), \"df belum ada. Jalankan Stage 1 dulu.\"\n",
    "assert \"year_month\" in df.columns, \"df['year_month'] belum ada.\"\n",
    "\n",
    "# ------------------------------\n",
    "# Kaggle horizon from sample_sub\n",
    "# ------------------------------\n",
    "sample_sub[\"year\"]  = sample_sub[\"id\"].str.split(\"_\").str[0]\n",
    "sample_sub[\"month\"] = sample_sub[\"id\"].str.split(\"_\").str[1]\n",
    "sample_sub[\"month_key\"] = sample_sub[\"year\"] + \"-\" + sample_sub[\"month\"]\n",
    "\n",
    "future_periods = (\n",
    "    pd.PeriodIndex(sample_sub[\"month_key\"], freq=\"M\")\n",
    "      .unique()\n",
    "      .sort_values()\n",
    ")\n",
    "H = int(len(future_periods))\n",
    "future_moy = set([p.month for p in future_periods])\n",
    "\n",
    "# ------------------------------\n",
    "# BUILD monthly (robust + complete month range)\n",
    "# ------------------------------\n",
    "has_exposure = \"active_policies\" in df.columns\n",
    "agg_dict = {\n",
    "    \"frequency\": (\"claim_id\",\"count\"),\n",
    "    \"total_claim\": (\"nominal_klaim_yang_disetujui\",\"sum\"),\n",
    "}\n",
    "if has_exposure:\n",
    "    agg_dict[\"exposure\"] = (\"active_policies\",\"first\")\n",
    "\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(**agg_dict)\n",
    "      .reset_index()\n",
    "      .sort_values(\"year_month\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "if len(monthly) == 0:\n",
    "    raise ValueError(\"monthly kosong. Cek df/year_month.\")\n",
    "\n",
    "if not isinstance(monthly.loc[0, \"year_month\"], pd.Period):\n",
    "    monthly[\"year_month\"] = pd.PeriodIndex(monthly[\"year_month\"], freq=\"M\")\n",
    "\n",
    "# reindex full range\n",
    "min_m = monthly[\"year_month\"].min()\n",
    "max_m = monthly[\"year_month\"].max()\n",
    "all_months = pd.period_range(min_m, max_m, freq=\"M\")\n",
    "\n",
    "monthly = (\n",
    "    monthly.set_index(\"year_month\")\n",
    "           .reindex(all_months)\n",
    "           .rename_axis(\"year_month\")\n",
    "           .reset_index()\n",
    ")\n",
    "\n",
    "monthly[\"frequency\"]   = pd.to_numeric(monthly[\"frequency\"], errors=\"coerce\").fillna(0.0)\n",
    "monthly[\"total_claim\"] = pd.to_numeric(monthly[\"total_claim\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "if not has_exposure:\n",
    "    monthly[\"exposure\"] = float(np.nanmean(monthly[\"frequency\"])) * 10.0\n",
    "else:\n",
    "    monthly[\"exposure\"] = pd.to_numeric(monthly[\"exposure\"], errors=\"coerce\")\n",
    "\n",
    "monthly[\"exposure\"] = monthly[\"exposure\"].ffill().bfill()\n",
    "monthly[\"exposure\"] = monthly[\"exposure\"].fillna(float(np.nanmean(monthly[\"frequency\"])) * 10.0)\n",
    "\n",
    "# safety clip\n",
    "monthly[\"frequency\"]   = monthly[\"frequency\"].astype(float).clip(lower=1.0)\n",
    "monthly[\"total_claim\"] = monthly[\"total_claim\"].astype(float).clip(lower=1.0)\n",
    "monthly[\"exposure\"]    = monthly[\"exposure\"].astype(float).clip(lower=1.0)\n",
    "\n",
    "monthly[\"severity\"]   = (monthly[\"total_claim\"] / monthly[\"frequency\"]).astype(float).clip(lower=1e-9)\n",
    "monthly[\"claim_rate\"] = (monthly[\"frequency\"]   / monthly[\"exposure\"]).astype(float).clip(lower=1e-12)\n",
    "monthly[\"month\"]      = monthly[\"year_month\"].dt.month\n",
    "\n",
    "N = len(monthly)\n",
    "print(\"N months:\", N, \"| H:\", H, \"| Has exposure:\", has_exposure, \"| Future MOY:\", sorted(list(future_moy)))\n",
    "\n",
    "# ------------------------------\n",
    "# Metrics\n",
    "# ------------------------------\n",
    "def mape_frac(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred) & (y_true != 0)\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return float(np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])))\n",
    "\n",
    "def avg_mape(yF, pF, yT, pT):\n",
    "    yF = np.asarray(yF, float)\n",
    "    yT = np.asarray(yT, float)\n",
    "    pF = np.asarray(pF, float)\n",
    "    pT = np.asarray(pT, float)\n",
    "    yS = yT / np.clip(yF, 1.0, np.inf)\n",
    "    pS = pT / np.clip(pF, 1.0, np.inf)\n",
    "    mf = mape_frac(yF, pF)\n",
    "    mt = mape_frac(yT, pT)\n",
    "    ms = mape_frac(yS, pS)\n",
    "    return float(np.nanmean([mf, mt, ms])), mt, mf, ms\n",
    "\n",
    "# ============================================================\n",
    "# Stage 3 v17 (fixed)\n",
    "# ============================================================\n",
    "S3 = dict(\n",
    "    wt_total=0.85, anchor_total=\"median\",\n",
    "    wt_freq=0.20,  anchor_freq=\"mean\",\n",
    "    k_anchor=3\n",
    ")\n",
    "\n",
    "def anchor_level(x: pd.Series, k: int, how: str):\n",
    "    tail = np.asarray(x.tail(k), dtype=float)\n",
    "    return float(np.median(tail)) if how == \"median\" else float(np.mean(tail))\n",
    "\n",
    "def ets_1step_log1p(level_series: pd.Series, trend=\"add\", damped=True):\n",
    "    y = np.log1p(level_series.astype(float).clip(lower=1e-12))\n",
    "    # guard short series\n",
    "    if len(y) < 4:\n",
    "        return float(np.expm1(y.iloc[-1]))\n",
    "    if trend is not None and len(y) < 10:\n",
    "        trend = None\n",
    "        damped = False\n",
    "    try:\n",
    "        m = ExponentialSmoothing(\n",
    "            y, trend=trend,\n",
    "            damped_trend=(damped if trend is not None else False),\n",
    "            seasonal=None\n",
    "        ).fit()\n",
    "        return float(np.expm1(m.forecast(1).iloc[0]))\n",
    "    except:\n",
    "        return float(level_series.iloc[-1])\n",
    "\n",
    "def simulate_stage3(train_df: pd.DataFrame, H: int):\n",
    "    sim = train_df.copy().reset_index(drop=True)\n",
    "    pF, pT = [], []\n",
    "    for _ in range(H):\n",
    "        k = int(S3[\"k_anchor\"])\n",
    "\n",
    "        tot_fc = ets_1step_log1p(sim[\"total_claim\"], trend=\"add\", damped=True)\n",
    "        tot_anchor = anchor_level(sim[\"total_claim\"], k, S3[\"anchor_total\"])\n",
    "        tot_pred = float(S3[\"wt_total\"])*tot_fc + (1-float(S3[\"wt_total\"]))*tot_anchor\n",
    "        tot_pred = max(1.0, tot_pred)\n",
    "\n",
    "        fre_fc = ets_1step_log1p(sim[\"frequency\"], trend=\"add\", damped=True)\n",
    "        fre_anchor = anchor_level(sim[\"frequency\"], k, S3[\"anchor_freq\"])\n",
    "        fre_pred = float(S3[\"wt_freq\"])*fre_fc + (1-float(S3[\"wt_freq\"]))*fre_anchor\n",
    "        fre_pred = max(1.0, fre_pred)\n",
    "\n",
    "        pF.append(fre_pred)\n",
    "        pT.append(tot_pred)\n",
    "\n",
    "        exp_last = float(sim[\"exposure\"].iloc[-1]) if \"exposure\" in sim.columns else 1.0\n",
    "        exp_last = max(1.0, exp_last)\n",
    "\n",
    "        sim = pd.concat([sim, pd.DataFrame([{\n",
    "            \"year_month\": sim[\"year_month\"].iloc[-1] + 1,\n",
    "            \"frequency\": fre_pred,\n",
    "            \"total_claim\": tot_pred,\n",
    "            \"exposure\": exp_last,\n",
    "            \"severity\": float(tot_pred / fre_pred),\n",
    "            \"claim_rate\": float(fre_pred / exp_last)\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    return np.array(pF, float), np.array(pT, float)\n",
    "\n",
    "# ============================================================\n",
    "# Stage 4 (pakai params kamu v23; ganti jika pakai v24 best)\n",
    "# ============================================================\n",
    "S4 = {\n",
    "    'k_anchor': 4,\n",
    "    'anchor_rate': 'mean',\n",
    "    'anchor_sev': 'mean',\n",
    "    'wt_rate': 0.3332890181752667,\n",
    "    'wt_sev': 0.5501072463530577,\n",
    "    'beta': 0.8005225975816733,\n",
    "    'damped': True,\n",
    "    'init_method': 'estimated',\n",
    "    'capR_low': 0.7818972835090717,\n",
    "    'capR_high': 1.3058960367487729,\n",
    "    'capS_low': 0.6976922342421228,\n",
    "    'capS_high': 1.1536381686265984\n",
    "}\n",
    "\n",
    "ROUND_FREQ = False  # LB biasanya lebih baik False\n",
    "\n",
    "def ets_1step_log(x_log: pd.Series, trend, damped, init_method):\n",
    "    if len(x_log) < 4:\n",
    "        return float(x_log.iloc[-1])\n",
    "    if trend is not None and len(x_log) < 10:\n",
    "        trend = None\n",
    "        damped = False\n",
    "    try:\n",
    "        m = ExponentialSmoothing(\n",
    "            x_log, trend=trend,\n",
    "            damped_trend=(damped if trend is not None else False),\n",
    "            seasonal=None, initialization_method=init_method\n",
    "        ).fit()\n",
    "        return float(m.forecast(1).iloc[0])\n",
    "    except:\n",
    "        return float(x_log.iloc[-1])\n",
    "\n",
    "def simulate_stage4(train_df: pd.DataFrame, H: int):\n",
    "    sim = train_df.copy().reset_index(drop=True)\n",
    "    pF, pT = [], []\n",
    "    for _ in range(H):\n",
    "        k = int(S4[\"k_anchor\"])\n",
    "        exp_next = float(sim[\"exposure\"].iloc[-1])\n",
    "        exp_next = max(1.0, exp_next)\n",
    "\n",
    "        aR = anchor_level(sim[\"claim_rate\"], k, S4[\"anchor_rate\"])\n",
    "        aS = anchor_level(sim[\"severity\"],   k, S4[\"anchor_sev\"])\n",
    "\n",
    "        lr = np.log(sim[\"claim_rate\"].astype(float).clip(lower=1e-12))\n",
    "        ls = np.log(sim[\"severity\"].astype(float).clip(lower=1e-12))\n",
    "\n",
    "        lr_add  = ets_1step_log(lr, trend=\"add\",  damped=bool(S4[\"damped\"]), init_method=S4[\"init_method\"])\n",
    "        lr_none = ets_1step_log(lr, trend=None,   damped=False,              init_method=S4[\"init_method\"])\n",
    "        r_fc = float(np.exp(float(S4[\"beta\"])*lr_add + (1-float(S4[\"beta\"]))*lr_none))\n",
    "\n",
    "        ls_add  = ets_1step_log(ls, trend=\"add\",  damped=bool(S4[\"damped\"]), init_method=S4[\"init_method\"])\n",
    "        ls_none = ets_1step_log(ls, trend=None,   damped=False,              init_method=S4[\"init_method\"])\n",
    "        s_fc = float(np.exp(float(S4[\"beta\"])*ls_add + (1-float(S4[\"beta\"]))*ls_none))\n",
    "\n",
    "        r_pred = float(S4[\"wt_rate\"])*r_fc + (1-float(S4[\"wt_rate\"]))*aR\n",
    "        s_pred = float(S4[\"wt_sev\"]) *s_fc + (1-float(S4[\"wt_sev\"])) *aS\n",
    "\n",
    "        r_pred = float(np.clip(r_pred, aR*float(S4[\"capR_low\"]), aR*float(S4[\"capR_high\"])))\n",
    "        s_pred = float(np.clip(s_pred, aS*float(S4[\"capS_low\"]), aS*float(S4[\"capS_high\"])))\n",
    "\n",
    "        f_pred = float(max(1.0, r_pred * exp_next))\n",
    "        if ROUND_FREQ:\n",
    "            f_pred = float(max(1.0, np.round(f_pred)))\n",
    "\n",
    "        t_pred = float(max(1.0, f_pred * s_pred))\n",
    "\n",
    "        pF.append(f_pred)\n",
    "        pT.append(t_pred)\n",
    "\n",
    "        sim = pd.concat([sim, pd.DataFrame([{\n",
    "            \"year_month\": sim[\"year_month\"].iloc[-1] + 1,\n",
    "            \"exposure\": exp_next,\n",
    "            \"claim_rate\": float(max(1e-12, f_pred/exp_next)),\n",
    "            \"severity\": float(max(1e-9, t_pred/np.clip(f_pred, 1.0, np.inf))),\n",
    "            \"frequency\": f_pred,\n",
    "            \"total_claim\": t_pred\n",
    "        }])], ignore_index=True)\n",
    "    return np.array(pF, float), np.array(pT, float)\n",
    "\n",
    "# ============================================================\n",
    "# Season-aware CV splits for blend selection\n",
    "# ============================================================\n",
    "# candidates: te such that valid length H\n",
    "min_train = 7\n",
    "cands = []\n",
    "for te in range(min_train, N - H + 1):\n",
    "    valid = monthly.iloc[te:te+H]\n",
    "    if len(valid) < H:\n",
    "        continue\n",
    "    overlap = sum([1 for m in valid[\"month\"].tolist() if m in future_moy]) / float(H)\n",
    "    recency = te / float(N)\n",
    "    score = 0.65*overlap + 0.35*recency\n",
    "    cands.append((score, overlap, recency, te))\n",
    "\n",
    "cands_sorted = sorted(cands, reverse=True)\n",
    "top_season = sorted(cands, key=lambda x: (x[1], x[0]), reverse=True)[:2]\n",
    "top_recent = sorted(cands, key=lambda x: x[2], reverse=True)[:2]\n",
    "picked = {x[3] for x in (top_season + top_recent)}\n",
    "train_ends = sorted(list(picked))\n",
    "\n",
    "w_raw = np.array([dict((x[3], x[0]) for x in cands_sorted).get(te, 0.1) for te in train_ends], dtype=float)\n",
    "w_raw = np.maximum(w_raw, 1e-6)\n",
    "split_w = w_raw / w_raw.sum()\n",
    "\n",
    "print(\"CV train_ends:\", train_ends, \"| weights:\", split_w.round(3).tolist())\n",
    "for te in train_ends:\n",
    "    v = monthly.iloc[te:te+H][[\"year_month\",\"month\"]]\n",
    "    print(\"  split te=\", te, \"| valid months:\", v[\"year_month\"].astype(str).tolist())\n",
    "\n",
    "# ============================================================\n",
    "# Blend search (choose by MEAN/MEDIAN, not worst-case)\n",
    "# ============================================================\n",
    "w_grid = np.linspace(0.0, 1.0, 21)  # lebih rapat: step 0.05\n",
    "\n",
    "rows = []\n",
    "best_mean = None\n",
    "best_median = None\n",
    "\n",
    "for w in w_grid:\n",
    "    split_scores = []\n",
    "    for te in train_ends:\n",
    "        tr = monthly.iloc[:te].copy().reset_index(drop=True)\n",
    "        va = monthly.iloc[te:te+H].copy().reset_index(drop=True)\n",
    "\n",
    "        pF3, pT3 = simulate_stage3(tr, H)\n",
    "        pF4, pT4 = simulate_stage4(tr, H)\n",
    "\n",
    "        pF = w*pF4 + (1-w)*pF3\n",
    "        pT = w*pT4 + (1-w)*pT3\n",
    "\n",
    "        sc, mt, mf, ms = avg_mape(va[\"frequency\"].values, pF, va[\"total_claim\"].values, pT)\n",
    "        split_scores.append(sc)\n",
    "\n",
    "    meanv = float(np.mean(split_scores))\n",
    "    medv  = float(np.median(split_scores))\n",
    "    worst = float(np.max(split_scores))\n",
    "    rows.append([w, meanv, medv, worst] + split_scores)\n",
    "\n",
    "    cand_mean = (meanv, medv, worst, w)\n",
    "    cand_med  = (medv, meanv, worst, w)\n",
    "\n",
    "    if (best_mean is None) or (cand_mean < best_mean):\n",
    "        best_mean = cand_mean\n",
    "    if (best_median is None) or (cand_med < best_median):\n",
    "        best_median = cand_med\n",
    "\n",
    "df_rows = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\"w_stage4\",\"mean_avg\",\"median_avg\",\"worst_avg\"] + [f\"split_{i}\" for i in range(len(train_ends))]\n",
    ")\n",
    "\n",
    "print(\"\\nTop candidates by mean_avg:\")\n",
    "print(df_rows.sort_values([\"mean_avg\",\"median_avg\",\"worst_avg\"]).head(8))\n",
    "\n",
    "print(\"\\nTop candidates by median_avg:\")\n",
    "print(df_rows.sort_values([\"median_avg\",\"mean_avg\",\"worst_avg\"]).head(8))\n",
    "\n",
    "w_mean = float(best_mean[-1])\n",
    "w_med  = float(best_median[-1])\n",
    "\n",
    "# Stabilize: kalau mean dan median beda kecil, ambil tengah supaya tidak ekstrem\n",
    "if abs(w_mean - w_med) <= 0.10:\n",
    "    w_best = float(0.5*(w_mean + w_med))\n",
    "else:\n",
    "    # kalau beda jauh, pilih mean-based (lebih smooth)\n",
    "    w_best = w_mean\n",
    "\n",
    "# Anti-extreme: jika w_best sangat dekat 1.0, pastikan memang jauh lebih baik dari 0.8\n",
    "if w_best >= 0.95:\n",
    "    s1 = df_rows.loc[df_rows[\"w_stage4\"].sub(1.0).abs().idxmin(), \"mean_avg\"]\n",
    "    s8 = df_rows.loc[df_rows[\"w_stage4\"].sub(0.8).abs().idxmin(), \"mean_avg\"]\n",
    "    # kalau bedanya kecil, turunkan ke 0.8 supaya lebih robust\n",
    "    if (s1 - s8) <= 0.002:\n",
    "        w_best = 0.80\n",
    "\n",
    "print(\"\\nChosen w_stage4:\", w_best, \"(w=1 pure Stage4, w=0 pure Stage3)\")\n",
    "\n",
    "# ============================================================\n",
    "# FINAL FORECAST on FULL data\n",
    "# ============================================================\n",
    "pF3, pT3 = simulate_stage3(monthly, H)\n",
    "pF4, pT4 = simulate_stage4(monthly, H)\n",
    "\n",
    "pF = w_best*pF4 + (1-w_best)*pF3\n",
    "pT = w_best*pT4 + (1-w_best)*pT3\n",
    "pS = pT / np.clip(pF, 1.0, np.inf)\n",
    "\n",
    "pred_map = {}\n",
    "preview = []\n",
    "\n",
    "for i, period in enumerate(future_periods):\n",
    "    key = f\"{period.year}_{str(period.month).zfill(2)}\"\n",
    "    pred_map[f\"{key}_Claim_Frequency\"] = float(pF[i])\n",
    "    pred_map[f\"{key}_Total_Claim\"]     = float(pT[i])\n",
    "    pred_map[f\"{key}_Claim_Severity\"]  = float(pS[i])\n",
    "    preview.append([str(period), float(pF[i]), float(pT[i]), float(pS[i])])\n",
    "\n",
    "sub = sample_sub.copy()\n",
    "sub[\"value\"] = sub[\"id\"].map(pred_map)\n",
    "\n",
    "missing = int(sub[\"value\"].isna().sum())\n",
    "print(\"NaN in submission:\", missing)\n",
    "assert missing == 0, \"Ada id belum terisi. Cek key format.\"\n",
    "\n",
    "sub = sub[[\"id\",\"value\"]]\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"\\nPreview future predictions:\")\n",
    "print(pd.DataFrame(preview, columns=[\"period\",\"pred_freq\",\"pred_total\",\"pred_sev\"]))\n",
    "print(\"\\nSaved: submission.csv\")\n",
    "print(sub.head(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1820cc28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T03:42:49.417501Z",
     "iopub.status.busy": "2026-02-21T03:42:49.415775Z",
     "iopub.status.idle": "2026-02-21T03:42:49.427523Z",
     "shell.execute_reply": "2026-02-21T03:42:49.426705Z"
    },
    "papermill": {
     "duration": 0.026683,
     "end_time": "2026-02-21T03:42:49.430148",
     "exception": false,
     "start_time": "2026-02-21T03:42:49.403465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         id         value\n",
      "0   2025_08_Claim_Frequency  2.435652e+02\n",
      "1    2025_08_Claim_Severity  5.161099e+07\n",
      "2       2025_08_Total_Claim  1.257064e+10\n",
      "3   2025_09_Claim_Frequency  2.449253e+02\n",
      "4    2025_09_Claim_Severity  5.133448e+07\n",
      "5       2025_09_Total_Claim  1.257311e+10\n",
      "6   2025_10_Claim_Frequency  2.479505e+02\n",
      "7    2025_10_Claim_Severity  5.023752e+07\n",
      "8       2025_10_Total_Claim  1.245642e+10\n",
      "9   2025_11_Claim_Frequency  2.438276e+02\n",
      "10   2025_11_Claim_Severity  5.107072e+07\n",
      "11      2025_11_Total_Claim  1.245245e+10\n"
     ]
    }
   ],
   "source": [
    "print(sub.head(12)) ##  12 persen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "309e12c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T03:42:49.452487Z",
     "iopub.status.busy": "2026-02-21T03:42:49.452123Z",
     "iopub.status.idle": "2026-02-21T03:42:49.840375Z",
     "shell.execute_reply": "2026-02-21T03:42:49.839583Z"
    },
    "papermill": {
     "duration": 0.402674,
     "end_time": "2026-02-21T03:42:49.842752",
     "exception": false,
     "start_time": "2026-02-21T03:42:49.440078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backtest last-H (%): avg 5.7701 | total 6.4797 | freq 6.2969 | sev 4.5335\n",
      "        ym     yF     pF            yT            pT\n",
      "0  2025-03  230.0  241.0  1.367924e+10  1.282809e+10\n",
      "1  2025-04  208.0  236.0  1.116425e+10  1.263251e+10\n",
      "2  2025-05  239.0  236.0  1.222680e+10  1.272460e+10\n",
      "3  2025-06  234.0  239.0  1.337312e+10  1.315459e+10\n",
      "4  2025-07  264.0  238.0  1.369923e+10  1.269653e+10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "def mape_frac(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred) & (y_true != 0)\n",
    "    return float(np.mean(np.abs((y_true[mask]-y_pred[mask]) / y_true[mask])))\n",
    "\n",
    "# PASTE best params Stage4 v23 kamu\n",
    "BEST = {\n",
    "    'k_anchor': 4,\n",
    "    'anchor_rate': 'mean',\n",
    "    'anchor_sev': 'mean',\n",
    "    'wt_rate': 0.3332890181752667,\n",
    "    'wt_sev': 0.5501072463530577,\n",
    "    'beta': 0.8005225975816733,\n",
    "    'damped': True,\n",
    "    'init_method': 'estimated',\n",
    "    'capR_low': 0.7818972835090717,\n",
    "    'capR_high': 1.3058960367487729,\n",
    "    'capS_low': 0.6976922342421228,\n",
    "    'capS_high': 1.1536381686265984\n",
    "}\n",
    "\n",
    "# monthly harus yang sama dengan Stage4 v23 (19 bulan, ada exposure/claim_rate/severity)\n",
    "# Kalau kamu sudah punya `monthly` dari Stage4 v23, pakai itu. Jangan rebuild lagi.\n",
    "assert \"monthly\" in globals(), \"monthly belum ada (pakai monthly dari Stage4 v23)\"\n",
    "monthly_stage4 = monthly.copy().reset_index(drop=True)\n",
    "\n",
    "# horizon sama seperti Kaggle (5)\n",
    "H = 5\n",
    "N = len(monthly_stage4)\n",
    "train_end = N - H\n",
    "train = monthly_stage4.iloc[:train_end].copy().reset_index(drop=True)\n",
    "valid = monthly_stage4.iloc[train_end:train_end+H].copy().reset_index(drop=True)\n",
    "\n",
    "def ets_1step_log(x_log: pd.Series, trend, damped, init_method):\n",
    "    try:\n",
    "        m = ExponentialSmoothing(\n",
    "            x_log,\n",
    "            trend=trend,\n",
    "            damped_trend=(damped if trend is not None else False),\n",
    "            seasonal=None,\n",
    "            initialization_method=init_method\n",
    "        ).fit()\n",
    "        return float(m.forecast(1).iloc[0])\n",
    "    except:\n",
    "        return float(x_log.iloc[-1])\n",
    "\n",
    "def anchor_level(x_level: pd.Series, k: int, how: str):\n",
    "    tail = np.asarray(x_level.tail(k), dtype=float)\n",
    "    return float(np.median(tail)) if how == \"median\" else float(np.mean(tail))\n",
    "\n",
    "sim = train.copy()\n",
    "pred_F, pred_T, pred_S = [], [], []\n",
    "\n",
    "for _ in range(H):\n",
    "    k = int(BEST[\"k_anchor\"])\n",
    "    exp_next = float(sim[\"exposure\"].iloc[-1])\n",
    "\n",
    "    aR = anchor_level(sim[\"claim_rate\"], k, BEST[\"anchor_rate\"])\n",
    "    aS = anchor_level(sim[\"severity\"],   k, BEST[\"anchor_sev\"])\n",
    "\n",
    "    lr = np.log(sim[\"claim_rate\"].astype(float).clip(lower=1e-12))\n",
    "    ls = np.log(sim[\"severity\"].astype(float).clip(lower=1e-12))\n",
    "\n",
    "    lr_add  = ets_1step_log(lr, trend=\"add\",  damped=bool(BEST[\"damped\"]), init_method=BEST[\"init_method\"])\n",
    "    lr_none = ets_1step_log(lr, trend=None,   damped=False,                init_method=BEST[\"init_method\"])\n",
    "    r_fc = float(np.exp(float(BEST[\"beta\"])*lr_add + (1-float(BEST[\"beta\"]))*lr_none))\n",
    "\n",
    "    ls_add  = ets_1step_log(ls, trend=\"add\",  damped=bool(BEST[\"damped\"]), init_method=BEST[\"init_method\"])\n",
    "    ls_none = ets_1step_log(ls, trend=None,   damped=False,                init_method=BEST[\"init_method\"])\n",
    "    s_fc = float(np.exp(float(BEST[\"beta\"])*ls_add + (1-float(BEST[\"beta\"]))*ls_none))\n",
    "\n",
    "    r_pred = float(BEST[\"wt_rate\"])*r_fc + (1-float(BEST[\"wt_rate\"]))*aR\n",
    "    s_pred = float(BEST[\"wt_sev\"]) *s_fc + (1-float(BEST[\"wt_sev\"])) *aS\n",
    "\n",
    "    r_pred = float(np.clip(r_pred, aR*float(BEST[\"capR_low\"]), aR*float(BEST[\"capR_high\"])))\n",
    "    s_pred = float(np.clip(s_pred, aS*float(BEST[\"capS_low\"]), aS*float(BEST[\"capS_high\"])))\n",
    "\n",
    "    f_pred = float(max(1.0, np.round(r_pred * exp_next)))\n",
    "    t_pred = float(max(1.0, f_pred * s_pred))\n",
    "    s_pred = float(t_pred / f_pred)\n",
    "\n",
    "    pred_F.append(f_pred)\n",
    "    pred_T.append(t_pred)\n",
    "    pred_S.append(s_pred)\n",
    "\n",
    "    sim = pd.concat([sim, pd.DataFrame([{\n",
    "        \"year_month\": sim[\"year_month\"].iloc[-1] + 1,\n",
    "        \"exposure\": exp_next,\n",
    "        \"claim_rate\": float(max(1e-12, f_pred/exp_next)),\n",
    "        \"severity\": s_pred,\n",
    "        \"frequency\": f_pred,\n",
    "        \"total_claim\": t_pred\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "yF = valid[\"frequency\"].values\n",
    "yT = valid[\"total_claim\"].values\n",
    "yS = (valid[\"total_claim\"].values / np.clip(valid[\"frequency\"].values, 1.0, np.inf))\n",
    "\n",
    "mf = mape_frac(yF, pred_F)\n",
    "mt = mape_frac(yT, pred_T)\n",
    "ms = mape_frac(yS, pred_S)\n",
    "avg = float(np.nanmean([mf, mt, ms]))\n",
    "\n",
    "print(\"Backtest last-H (%):\",\n",
    "      \"avg\", round(avg*100,4),\n",
    "      \"| total\", round(mt*100,4),\n",
    "      \"| freq\", round(mf*100,4),\n",
    "      \"| sev\", round(ms*100,4))\n",
    "print(pd.DataFrame({\"ym\": valid[\"year_month\"], \"yF\": yF, \"pF\": pred_F, \"yT\": yT, \"pT\": pred_T}))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15694757,
     "datasetId": 9488145,
     "sourceId": 14836320,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 537.986175,
   "end_time": "2026-02-21T03:42:50.574712",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-21T03:33:52.588537",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0c0eade17e294a029cc3223171f44a0c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4fe18d02b52c4b4da114ef3722a98903": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_75e313e1e56548a0896663989320e881",
        "IPY_MODEL_7dbb472e134e411b857aafb209bafd76",
        "IPY_MODEL_b43d64437fa44f6da99342a09199e107"
       ],
       "layout": "IPY_MODEL_92c267d01b8c4a3b9cb89cc2b9fa86ee",
       "tabbable": null,
       "tooltip": null
      }
     },
     "75e313e1e56548a0896663989320e881": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0c0eade17e294a029cc3223171f44a0c",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_f43a770e657b4eb299900d945eec07d9",
       "tabbable": null,
       "tooltip": null,
       "value": "Bestâ€‡trial:â€‡360.â€‡Bestâ€‡value:â€‡0.0951591:â€‡100%"
      }
     },
     "7dbb472e134e411b857aafb209bafd76": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_da26d4b93aa140459a8328be64485027",
       "max": 500.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8ec7abcf59d24288ac44ee0a6b3081a0",
       "tabbable": null,
       "tooltip": null,
       "value": 500.0
      }
     },
     "8ec7abcf59d24288ac44ee0a6b3081a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "92c267d01b8c4a3b9cb89cc2b9fa86ee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "adf1a09e207b497b9c041fc70fd69ac2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b43d64437fa44f6da99342a09199e107": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_adf1a09e207b497b9c041fc70fd69ac2",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_b7d820b8b91948e082e977bc54ab0f5c",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡500/500â€‡[07:01&lt;00:00,â€‡â€‡1.15it/s]"
      }
     },
     "b7d820b8b91948e082e977bc54ab0f5c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "da26d4b93aa140459a8328be64485027": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f43a770e657b4eb299900d945eec07d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
