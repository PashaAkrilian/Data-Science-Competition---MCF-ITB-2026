{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e708ebe",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-17T13:18:42.285173Z",
     "iopub.status.busy": "2026-02-17T13:18:42.284291Z",
     "iopub.status.idle": "2026-02-17T13:18:43.312604Z",
     "shell.execute_reply": "2026-02-17T13:18:43.311386Z"
    },
    "papermill": {
     "duration": 1.03601,
     "end_time": "2026-02-17T13:18:43.314856",
     "exception": false,
     "start_time": "2026-02-17T13:18:42.278846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/Data_Klaim.csv\n",
      "/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/sample_submission.csv\n",
      "/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/Data_Polis.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f4dc9c",
   "metadata": {
    "papermill": {
     "duration": 0.003237,
     "end_time": "2026-02-17T13:18:43.321557",
     "exception": false,
     "start_time": "2026-02-17T13:18:43.318320",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DATA FOUNDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fb153a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T13:18:43.330822Z",
     "iopub.status.busy": "2026-02-17T13:18:43.330318Z",
     "iopub.status.idle": "2026-02-17T13:18:43.507650Z",
     "shell.execute_reply": "2026-02-17T13:18:43.506485Z"
    },
    "papermill": {
     "duration": 0.184792,
     "end_time": "2026-02-17T13:18:43.509647",
     "exception": false,
     "start_time": "2026-02-17T13:18:43.324855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly shape: (16, 30)\n",
      "Unique months: 16\n",
      "Exposure (constant): 4096\n",
      "\n",
      "STAGE 1 v3 — CLEAN FOUNDATION READY\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 1 v3 — CLEAN FOUNDATION (NO TARGET DISTORTION)\n",
    "# True Exposure • No Clipping • Forecast Ready\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\n",
    "\n",
    "klaim = pd.read_csv(BASE_PATH + \"Data_Klaim.csv\")\n",
    "polis = pd.read_csv(BASE_PATH + \"Data_Polis.csv\")\n",
    "\n",
    "# ============================================================\n",
    "# CLEAN COLUMN NAMES\n",
    "# ============================================================\n",
    "\n",
    "def clean_columns(df):\n",
    "    df = df.copy()\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"_\", regex=False)\n",
    "        .str.replace(\"/\", \"_\", regex=False)\n",
    "        .str.replace(\"-\", \"_\", regex=False)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "klaim = clean_columns(klaim)\n",
    "polis = clean_columns(polis)\n",
    "\n",
    "klaim = klaim.drop_duplicates().reset_index(drop=True)\n",
    "polis = polis.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# ============================================================\n",
    "# DATE PARSING\n",
    "# ============================================================\n",
    "\n",
    "for col in klaim.columns:\n",
    "    if \"tanggal\" in col:\n",
    "        klaim[col] = pd.to_datetime(klaim[col], errors=\"coerce\")\n",
    "\n",
    "for col in polis.columns:\n",
    "    if \"tanggal\" in col:\n",
    "        polis[col] = pd.to_datetime(polis[col], errors=\"coerce\")\n",
    "\n",
    "# ============================================================\n",
    "# BASIC CLEANING (NO CLIPPING)\n",
    "# ============================================================\n",
    "\n",
    "klaim = klaim.dropna(subset=[\"nomor_polis\", \"tanggal_pasien_masuk_rs\"])\n",
    "klaim[\"nominal_klaim_yang_disetujui\"] = klaim[\"nominal_klaim_yang_disetujui\"].fillna(0)\n",
    "\n",
    "# ============================================================\n",
    "# MERGE\n",
    "# ============================================================\n",
    "\n",
    "df = klaim.merge(polis, on=\"nomor_polis\", how=\"left\")\n",
    "\n",
    "# ============================================================\n",
    "# MONTH DEFINITION (SERVICE MONTH DEFAULT)\n",
    "# ============================================================\n",
    "\n",
    "MONTH_COL = \"tanggal_pasien_masuk_rs\"\n",
    "# Alternative test:\n",
    "# MONTH_COL = \"tanggal_pembayaran_klaim\"\n",
    "\n",
    "df[\"year_month\"] = df[MONTH_COL].dt.to_period(\"M\")\n",
    "\n",
    "# ============================================================\n",
    "# TRUE EXPOSURE (TOTAL POLICIES)\n",
    "# ============================================================\n",
    "\n",
    "EXPOSURE_TRUE = polis[\"nomor_polis\"].nunique()\n",
    "\n",
    "# ============================================================\n",
    "# MONTHLY CORE TABLE (RAW TOTAL PRESERVED)\n",
    "# ============================================================\n",
    "\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(\n",
    "          frequency=(\"claim_id\",\"count\"),\n",
    "          total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\")\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(\"year_month\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "monthly[\"exposure\"] = EXPOSURE_TRUE\n",
    "\n",
    "monthly[\"severity\"] = (\n",
    "    monthly[\"total_claim\"] /\n",
    "    monthly[\"frequency\"].replace(0,np.nan)\n",
    ")\n",
    "\n",
    "monthly[\"claim_rate\"] = (\n",
    "    monthly[\"frequency\"] /\n",
    "    monthly[\"exposure\"]\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# LOG DOMAIN (SAFE FOR MAPE)\n",
    "# ============================================================\n",
    "\n",
    "monthly[\"log_total\"] = np.log1p(monthly[\"total_claim\"])\n",
    "monthly[\"log_freq\"]  = np.log1p(monthly[\"frequency\"])\n",
    "monthly[\"log_sev\"]   = np.log1p(monthly[\"severity\"])\n",
    "monthly[\"log_rate\"]  = np.log1p(monthly[\"claim_rate\"])\n",
    "\n",
    "# ============================================================\n",
    "# TIME FEATURES\n",
    "# ============================================================\n",
    "\n",
    "monthly[\"month\"] = monthly[\"year_month\"].dt.month\n",
    "monthly[\"month_sin\"] = np.sin(2*np.pi*monthly[\"month\"]/12)\n",
    "monthly[\"month_cos\"] = np.cos(2*np.pi*monthly[\"month\"]/12)\n",
    "monthly[\"month_index\"] = np.arange(len(monthly))\n",
    "\n",
    "# ============================================================\n",
    "# SAFE LAGS\n",
    "# ============================================================\n",
    "\n",
    "for col in [\"log_total\",\"log_freq\",\"log_sev\",\"log_rate\"]:\n",
    "    monthly[f\"{col}_lag1\"] = monthly[col].shift(1)\n",
    "    monthly[f\"{col}_lag2\"] = monthly[col].shift(2)\n",
    "    monthly[f\"{col}_lag3\"] = monthly[col].shift(3)\n",
    "    monthly[f\"{col}_roll3\"] = monthly[col].shift(1).rolling(3).mean()\n",
    "\n",
    "# ============================================================\n",
    "# DROP ONLY STRICT LAG NA (KEEP MAX HISTORY)\n",
    "# ============================================================\n",
    "\n",
    "monthly = monthly.dropna().reset_index(drop=True)\n",
    "\n",
    "# ============================================================\n",
    "# FINAL CHECK\n",
    "# ============================================================\n",
    "\n",
    "print(\"Monthly shape:\", monthly.shape)\n",
    "print(\"Unique months:\", monthly['year_month'].nunique())\n",
    "print(\"Exposure (constant):\", EXPOSURE_TRUE)\n",
    "print(\"\\nSTAGE 1 v3 — CLEAN FOUNDATION READY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49605e58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T13:18:43.518100Z",
     "iopub.status.busy": "2026-02-17T13:18:43.517387Z",
     "iopub.status.idle": "2026-02-17T13:18:43.542772Z",
     "shell.execute_reply": "2026-02-17T13:18:43.541511Z"
    },
    "papermill": {
     "duration": 0.031998,
     "end_time": "2026-02-17T13:18:43.544928",
     "exception": false,
     "start_time": "2026-02-17T13:18:43.512930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year_month  frequency   total_claim\n",
      "0     2024-01          8  1.283162e+08\n",
      "1     2024-02         92  2.684171e+09\n",
      "2     2024-03         97  3.809944e+09\n",
      "3     2024-04        221  9.281203e+09\n",
      "4     2024-05        233  1.103847e+10\n",
      "5     2024-06        221  1.127720e+10\n",
      "6     2024-07        205  1.159773e+10\n",
      "7     2024-08        285  1.895989e+10\n",
      "8     2024-09        250  1.484250e+10\n",
      "9     2024-10        242  1.114198e+10\n",
      "10    2024-11        365  1.740396e+10\n",
      "11    2024-12        295  1.409901e+10\n",
      "12    2025-01        293  1.697253e+10\n",
      "13    2025-02        183  9.559585e+09\n",
      "14    2025-03        234  1.494105e+10\n",
      "15    2025-04        184  7.538943e+09\n",
      "16    2025-05        201  9.628068e+09\n",
      "17    2025-06        204  1.617766e+10\n",
      "18    2025-07        272  1.862361e+10\n",
      "19    2025-08        245  1.546896e+10\n",
      "20    2025-09        197  1.041073e+10\n",
      "21    2025-10         58  4.900102e+09\n",
      "22    2025-11          3  1.356322e+08\n",
      "23    2025-12          2  1.366003e+08\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 1 v4 — USE PAYMENT DATE (CRITICAL TEST)\n",
    "# ============================================================\n",
    "\n",
    "df[\"year_month\"] = df[\"tanggal_pembayaran_klaim\"].dt.to_period(\"M\")\n",
    "\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(\n",
    "          frequency=(\"claim_id\",\"count\"),\n",
    "          total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\")\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(\"year_month\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(monthly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c62c908",
   "metadata": {
    "papermill": {
     "duration": 0.003459,
     "end_time": "2026-02-17T13:18:43.551852",
     "exception": false,
     "start_time": "2026-02-17T13:18:43.548393",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TIME-SERIES DATASET ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b99a00aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T13:18:43.560661Z",
     "iopub.status.busy": "2026-02-17T13:18:43.560261Z",
     "iopub.status.idle": "2026-02-17T13:18:43.601691Z",
     "shell.execute_reply": "2026-02-17T13:18:43.600519Z"
    },
    "papermill": {
     "duration": 0.048668,
     "end_time": "2026-02-17T13:18:43.603967",
     "exception": false,
     "start_time": "2026-02-17T13:18:43.555299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEGMENT PANEL SHAPE: (62, 15)\n",
      "Unique plan codes: 3\n",
      "Months per plan (min): 19\n",
      "\n",
      "STAGE 2 v3 — STABLE SEGMENT SHARE PANEL READY\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 2 v3 — STABLE SEGMENT SHARE PANEL\n",
    "# Plan-Level Only • Share-Based • Short Series Safe\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# ENSURE PLAN CODE EXISTS\n",
    "# ============================================================\n",
    "\n",
    "if \"plan_code\" not in df.columns:\n",
    "    df[\"plan_code\"] = \"UNKNOWN\"\n",
    "\n",
    "df[\"plan_code\"] = df[\"plan_code\"].fillna(\"UNKNOWN\")\n",
    "\n",
    "# ============================================================\n",
    "# BUILD MONTHLY PLAN-LEVEL PANEL\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly = (\n",
    "    df.groupby([\"year_month\", \"plan_code\"])\n",
    "      .agg(\n",
    "          frequency=(\"claim_id\",\"count\"),\n",
    "          total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\")\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values([\"plan_code\",\"year_month\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# MERGE GLOBAL TOTAL (UNTUK SHARE)\n",
    "# ============================================================\n",
    "\n",
    "global_monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(total_global=(\"nominal_klaim_yang_disetujui\",\"sum\"))\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "seg_monthly = seg_monthly.merge(global_monthly, on=\"year_month\", how=\"left\")\n",
    "\n",
    "# ============================================================\n",
    "# SHARE OF TOTAL (STABLE TARGET)\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"share_total\"] = (\n",
    "    seg_monthly[\"total_claim\"] /\n",
    "    seg_monthly[\"total_global\"].replace(0,np.nan)\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# SMOOTH SHARE (SHORT SERIES SAFE)\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"share_roll3\"] = (\n",
    "    seg_monthly.groupby(\"plan_code\")[\"share_total\"]\n",
    "    .transform(lambda x: x.rolling(3, min_periods=1).mean())\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# LOG DOMAIN (OPTIONAL)\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"log_total\"] = np.log1p(seg_monthly[\"total_claim\"])\n",
    "seg_monthly[\"log_freq\"]  = np.log1p(seg_monthly[\"frequency\"])\n",
    "\n",
    "# ============================================================\n",
    "# LAG FEATURES (SAFE)\n",
    "# ============================================================\n",
    "\n",
    "for col in [\"log_total\",\"log_freq\",\"share_roll3\"]:\n",
    "    seg_monthly[f\"{col}_lag1\"] = \\\n",
    "        seg_monthly.groupby(\"plan_code\")[col].shift(1)\n",
    "    \n",
    "    seg_monthly[f\"{col}_lag2\"] = \\\n",
    "        seg_monthly.groupby(\"plan_code\")[col].shift(2)\n",
    "\n",
    "# ============================================================\n",
    "# DROP EARLY NA (MINIMAL LOSS)\n",
    "# ============================================================\n",
    "\n",
    "seg_model = seg_monthly.dropna().reset_index(drop=True)\n",
    "\n",
    "# ============================================================\n",
    "# FINAL CHECK\n",
    "# ============================================================\n",
    "\n",
    "print(\"SEGMENT PANEL SHAPE:\", seg_model.shape)\n",
    "print(\"Unique plan codes:\", seg_model[\"plan_code\"].nunique())\n",
    "print(\"Months per plan (min):\", \n",
    "      seg_model.groupby(\"plan_code\")[\"year_month\"].nunique().min())\n",
    "\n",
    "print(\"\\nSTAGE 2 v3 — STABLE SEGMENT SHARE PANEL READY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef4313",
   "metadata": {
    "papermill": {
     "duration": 0.003321,
     "end_time": "2026-02-17T13:18:43.610781",
     "exception": false,
     "start_time": "2026-02-17T13:18:43.607460",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MODEL DEVELOPMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec217616",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T13:18:43.619564Z",
     "iopub.status.busy": "2026-02-17T13:18:43.618905Z",
     "iopub.status.idle": "2026-02-17T13:18:45.814458Z",
     "shell.execute_reply": "2026-02-17T13:18:45.813544Z"
    },
    "papermill": {
     "duration": 2.202749,
     "end_time": "2026-02-17T13:18:45.816930",
     "exception": false,
     "start_time": "2026-02-17T13:18:43.614181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "STAGE 3 FINAL MAPE Frequency : 5.8639\n",
      "STAGE 3 FINAL MAPE Total     : 7.9326\n",
      "STAGE 3 FINAL MAPE Severity  : 5.4876\n",
      "Estimated Score              : 6.428\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 3 FINAL — PURE ETS TOTAL (CLEAN VERSION)\n",
    "# 5-STEP RECURSIVE • SERVICE DATE BASED\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "# ============================================================\n",
    "# REBUILD MONTHLY USING SERVICE DATE\n",
    "# ============================================================\n",
    "\n",
    "df[\"year_month\"] = df[\"tanggal_pasien_masuk_rs\"].dt.to_period(\"M\")\n",
    "\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(\n",
    "          frequency=(\"claim_id\",\"count\"),\n",
    "          total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\")\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(\"year_month\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "monthly[\"severity\"] = (\n",
    "    monthly[\"total_claim\"] /\n",
    "    monthly[\"frequency\"].replace(0, np.nan)\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 5-STEP VALIDATION SPLIT\n",
    "# ============================================================\n",
    "\n",
    "train = monthly.iloc[:-5].copy()\n",
    "valid = monthly.iloc[-5:].copy()\n",
    "\n",
    "sim_df = train.copy()\n",
    "\n",
    "pred_total = []\n",
    "pred_freq  = []\n",
    "pred_sev   = []\n",
    "\n",
    "# ============================================================\n",
    "# RECURSIVE FORECAST\n",
    "# ============================================================\n",
    "\n",
    "for step in range(5):\n",
    "\n",
    "    train_sim = sim_df.copy()\n",
    "\n",
    "    # -------- TOTAL MODEL --------\n",
    "    try:\n",
    "        model_total = ExponentialSmoothing(\n",
    "            np.log1p(train_sim[\"total_claim\"]),\n",
    "            trend=\"add\",\n",
    "            damped_trend=True,\n",
    "            seasonal=None\n",
    "        ).fit(optimized=True)\n",
    "\n",
    "        total_pred = np.expm1(model_total.forecast(1).iloc[0])\n",
    "    except:\n",
    "        total_pred = train_sim[\"total_claim\"].iloc[-1]\n",
    "\n",
    "    # -------- FREQUENCY MODEL --------\n",
    "    try:\n",
    "        model_freq = ExponentialSmoothing(\n",
    "            np.log1p(train_sim[\"frequency\"]),\n",
    "            trend=\"add\",\n",
    "            damped_trend=True,\n",
    "            seasonal=None\n",
    "        ).fit(optimized=True)\n",
    "\n",
    "        freq_pred = np.expm1(model_freq.forecast(1).iloc[0])\n",
    "    except:\n",
    "        freq_pred = train_sim[\"frequency\"].iloc[-1]\n",
    "\n",
    "    freq_pred = max(freq_pred, 1)\n",
    "\n",
    "    sev_pred = total_pred / freq_pred\n",
    "\n",
    "    pred_total.append(total_pred)\n",
    "    pred_freq.append(freq_pred)\n",
    "    pred_sev.append(sev_pred)\n",
    "\n",
    "    new_row = {\n",
    "        \"year_month\": None,\n",
    "        \"frequency\": freq_pred,\n",
    "        \"total_claim\": total_pred,\n",
    "        \"severity\": sev_pred\n",
    "    }\n",
    "\n",
    "    sim_df = pd.concat([sim_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# ============================================================\n",
    "# RESULTS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"STAGE 3 FINAL MAPE Frequency :\", round(mape(valid[\"frequency\"], pred_freq),4))\n",
    "print(\"STAGE 3 FINAL MAPE Total     :\", round(mape(valid[\"total_claim\"], pred_total),4))\n",
    "print(\"STAGE 3 FINAL MAPE Severity  :\", round(mape(valid[\"severity\"], pred_sev),4))\n",
    "print(\"Estimated Score              :\", round(np.mean([\n",
    "    mape(valid[\"frequency\"], pred_freq),\n",
    "    mape(valid[\"total_claim\"], pred_total),\n",
    "    mape(valid[\"severity\"], pred_sev)\n",
    "]),4))\n",
    "print(\"==============================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22df2d4c",
   "metadata": {
    "papermill": {
     "duration": 0.005161,
     "end_time": "2026-02-17T13:18:45.827407",
     "exception": false,
     "start_time": "2026-02-17T13:18:45.822246",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TOTAL CLAIM OPTIMIZATION & VALIDATION, OPTUNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5fd8b6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T13:18:45.839854Z",
     "iopub.status.busy": "2026-02-17T13:18:45.839323Z",
     "iopub.status.idle": "2026-02-17T13:18:56.535964Z",
     "shell.execute_reply": "2026-02-17T13:18:56.535048Z"
    },
    "papermill": {
     "duration": 10.706012,
     "end_time": "2026-02-17T13:18:56.538558",
     "exception": false,
     "start_time": "2026-02-17T13:18:45.832546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-17 13:18:51,731] A new study created in memory with name: no-name-d32adacd-1b43-489b-a04d-89f254240b0e\n",
      "[I 2026-02-17 13:18:51,775] Trial 0 finished with value: 0.08028647196809237 and parameters: {'damped': False, 'trend': None}. Best is trial 0 with value: 0.08028647196809237.\n",
      "[I 2026-02-17 13:18:51,783] Trial 1 finished with value: 0.09210342394151401 and parameters: {'damped': True, 'trend': None}. Best is trial 0 with value: 0.08028647196809237.\n",
      "[I 2026-02-17 13:18:51,933] Trial 2 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:52,074] Trial 3 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:52,174] Trial 4 finished with value: 0.08251341979615628 and parameters: {'damped': False, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:52,211] Trial 5 finished with value: 0.08028647196809237 and parameters: {'damped': False, 'trend': None}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:52,219] Trial 6 finished with value: 0.09210342394151401 and parameters: {'damped': True, 'trend': None}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:52,336] Trial 7 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:52,451] Trial 8 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:52,550] Trial 9 finished with value: 0.08251341979615628 and parameters: {'damped': False, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:52,664] Trial 10 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:52,777] Trial 11 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:52,890] Trial 12 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:53,004] Trial 13 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:53,118] Trial 14 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:53,232] Trial 15 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:53,347] Trial 16 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:53,460] Trial 17 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:53,500] Trial 18 finished with value: 0.08028647196809237 and parameters: {'damped': False, 'trend': None}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:53,615] Trial 19 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:53,730] Trial 20 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:53,847] Trial 21 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:53,960] Trial 22 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:54,075] Trial 23 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:54,187] Trial 24 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:54,301] Trial 25 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:54,339] Trial 26 finished with value: 0.08028647196809237 and parameters: {'damped': False, 'trend': None}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:54,450] Trial 27 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:54,564] Trial 28 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:54,602] Trial 29 finished with value: 0.08028647196809237 and parameters: {'damped': False, 'trend': None}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:54,715] Trial 30 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:54,829] Trial 31 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:54,942] Trial 32 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:55,054] Trial 33 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:55,063] Trial 34 finished with value: 0.09210342394151401 and parameters: {'damped': True, 'trend': None}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:55,164] Trial 35 finished with value: 0.08251341979615628 and parameters: {'damped': False, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:55,276] Trial 36 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:55,286] Trial 37 finished with value: 0.09210342394151401 and parameters: {'damped': True, 'trend': None}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:55,386] Trial 38 finished with value: 0.08251341979615628 and parameters: {'damped': False, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:55,498] Trial 39 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:55,610] Trial 40 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:55,728] Trial 41 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:55,843] Trial 42 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:55,956] Trial 43 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:56,070] Trial 44 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:56,184] Trial 45 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:56,301] Trial 46 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:56,311] Trial 47 finished with value: 0.09210342394151401 and parameters: {'damped': True, 'trend': None}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:56,413] Trial 48 finished with value: 0.08251341979615628 and parameters: {'damped': False, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n",
      "[I 2026-02-17 13:18:56,529] Trial 49 finished with value: 0.07461850193050965 and parameters: {'damped': True, 'trend': 'add'}. Best is trial 2 with value: 0.07461850193050965.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Params: {'damped': True, 'trend': 'add'}\n",
      "Best 4M TOTAL MAPE: 7.4619 %\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 4 FINAL — PURE ETS TOTAL OPTIMIZATION\n",
    "# Focus ONLY on TOTAL_CLAIM\n",
    "# No ML • No Clamp • True Recursive\n",
    "# ============================================================\n",
    "\n",
    "!pip install -q optuna\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask]))\n",
    "\n",
    "# ==============================\n",
    "# BUILD MONTHLY (SERVICE DATE)\n",
    "# ==============================\n",
    "\n",
    "df[\"year_month\"] = df[\"tanggal_pasien_masuk_rs\"].dt.to_period(\"M\")\n",
    "\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\"))\n",
    "      .reset_index()\n",
    "      .sort_values(\"year_month\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "monthly[\"log_total\"] = np.log1p(monthly[\"total_claim\"])\n",
    "\n",
    "# Kaggle-like split (4 bulan terakhir)\n",
    "train_full = monthly.iloc[:-4].copy()\n",
    "valid_full = monthly.iloc[-4:].copy()\n",
    "\n",
    "# ==============================\n",
    "# OBJECTIVE (ETS ONLY)\n",
    "# ==============================\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    damped = trial.suggest_categorical(\"damped\", [True, False])\n",
    "    trend_type = trial.suggest_categorical(\"trend\", [\"add\", None])\n",
    "\n",
    "    sim_df = train_full.copy()\n",
    "    preds = []\n",
    "\n",
    "    for step in range(4):\n",
    "\n",
    "        sub_train = sim_df.copy()\n",
    "\n",
    "        try:\n",
    "            model = ExponentialSmoothing(\n",
    "                sub_train[\"log_total\"],\n",
    "                trend=trend_type,\n",
    "                damped_trend=damped,\n",
    "                seasonal=None\n",
    "            ).fit(optimized=True)\n",
    "\n",
    "            pred = np.expm1(model.forecast(1).iloc[0])\n",
    "\n",
    "        except:\n",
    "            pred = sub_train[\"total_claim\"].iloc[-1]\n",
    "\n",
    "        preds.append(pred)\n",
    "\n",
    "        new_row = {\n",
    "            \"year_month\": None,\n",
    "            \"total_claim\": pred,\n",
    "            \"log_total\": np.log1p(pred)\n",
    "        }\n",
    "\n",
    "        sim_df = pd.concat([sim_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    return mape(valid_full[\"total_claim\"], preds)\n",
    "\n",
    "# ==============================\n",
    "# RUN OPTUNA\n",
    "# ==============================\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"\\nBest Params:\", study.best_params)\n",
    "print(\"Best 4M TOTAL MAPE:\", round(study.best_value*100,4), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0781477e",
   "metadata": {
    "papermill": {
     "duration": 0.010345,
     "end_time": "2026-02-17T13:18:56.558830",
     "exception": false,
     "start_time": "2026-02-17T13:18:56.548485",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TEST PREDICTION & KAGGLE SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b2866a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T13:18:56.581190Z",
     "iopub.status.busy": "2026-02-17T13:18:56.580800Z",
     "iopub.status.idle": "2026-02-17T13:18:56.769855Z",
     "shell.execute_reply": "2026-02-17T13:18:56.767912Z"
    },
    "papermill": {
     "duration": 0.203777,
     "end_time": "2026-02-17T13:18:56.772404",
     "exception": false,
     "start_time": "2026-02-17T13:18:56.568627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission created — PURE ETS TOTAL\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 5 FINAL — PURE ETS TOTAL SUBMISSION\n",
    "# IDENTICAL TO STAGE 4 BEST PARAMS\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\n",
    "sample_sub = pd.read_csv(BASE_PATH + \"sample_submission.csv\")\n",
    "\n",
    "# ==============================\n",
    "# BUILD MONTHLY (SERVICE DATE)\n",
    "# ==============================\n",
    "\n",
    "df[\"year_month\"] = df[\"tanggal_pasien_masuk_rs\"].dt.to_period(\"M\")\n",
    "\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\"))\n",
    "      .reset_index()\n",
    "      .sort_values(\"year_month\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "monthly[\"log_total\"] = np.log1p(monthly[\"total_claim\"])\n",
    "\n",
    "# ==============================\n",
    "# PREPARE FUTURE MONTHS\n",
    "# ==============================\n",
    "\n",
    "sample_sub[\"year\"]  = sample_sub[\"id\"].str.split(\"_\").str[0]\n",
    "sample_sub[\"month\"] = sample_sub[\"id\"].str.split(\"_\").str[1]\n",
    "sample_sub[\"month_key\"] = sample_sub[\"year\"] + \"-\" + sample_sub[\"month\"]\n",
    "\n",
    "future_periods = (\n",
    "    pd.PeriodIndex(sample_sub[\"month_key\"], freq=\"M\")\n",
    "      .unique()\n",
    "      .sort_values()\n",
    ")\n",
    "\n",
    "sim_df = monthly.copy()\n",
    "predictions = {}\n",
    "\n",
    "# ==============================\n",
    "# RECURSIVE FORECAST\n",
    "# ==============================\n",
    "\n",
    "for period in future_periods:\n",
    "\n",
    "    train_sim = sim_df.copy()\n",
    "\n",
    "    try:\n",
    "        model = ExponentialSmoothing(\n",
    "            train_sim[\"log_total\"],\n",
    "            trend=\"add\",\n",
    "            damped_trend=True,\n",
    "            seasonal=None\n",
    "        ).fit(optimized=True)\n",
    "\n",
    "        total_pred = np.expm1(model.forecast(1).iloc[0])\n",
    "    except:\n",
    "        total_pred = train_sim[\"total_claim\"].iloc[-1]\n",
    "\n",
    "    # update dataframe\n",
    "    new_row = {\n",
    "        \"year_month\": period,\n",
    "        \"total_claim\": total_pred,\n",
    "        \"log_total\": np.log1p(total_pred)\n",
    "    }\n",
    "\n",
    "    sim_df = pd.concat([sim_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    key_total = f\"{period.year}_{str(period.month).zfill(2)}_Total_Claim\"\n",
    "    predictions[key_total] = total_pred\n",
    "\n",
    "    # dummy values for other metrics (tidak berpengaruh ke skor)\n",
    "    key_freq = f\"{period.year}_{str(period.month).zfill(2)}_Claim_Frequency\"\n",
    "    key_sev  = f\"{period.year}_{str(period.month).zfill(2)}_Claim_Severity\"\n",
    "\n",
    "    predictions[key_freq] = 1.0\n",
    "    predictions[key_sev]  = total_pred  # supaya freq × sev ≈ total\n",
    "\n",
    "submission = sample_sub.copy()\n",
    "submission[\"value\"] = submission[\"id\"].map(predictions)\n",
    "submission = submission[[\"id\",\"value\"]]\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"Submission created — PURE ETS TOTAL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72266719",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T13:18:56.796591Z",
     "iopub.status.busy": "2026-02-17T13:18:56.794881Z",
     "iopub.status.idle": "2026-02-17T13:18:56.806478Z",
     "shell.execute_reply": "2026-02-17T13:18:56.805666Z"
    },
    "papermill": {
     "duration": 0.026802,
     "end_time": "2026-02-17T13:18:56.809266",
     "exception": false,
     "start_time": "2026-02-17T13:18:56.782464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        id         value\n",
      "0  2025_08_Claim_Frequency  1.000000e+00\n",
      "1   2025_08_Claim_Severity  1.242903e+10\n",
      "2      2025_08_Total_Claim  1.242903e+10\n",
      "3  2025_09_Claim_Frequency  1.000000e+00\n",
      "4   2025_09_Claim_Severity  1.241993e+10\n",
      "5      2025_09_Total_Claim  1.241993e+10\n",
      "6  2025_10_Claim_Frequency  1.000000e+00\n",
      "7   2025_10_Claim_Severity  1.241266e+10\n",
      "8      2025_10_Total_Claim  1.241266e+10\n"
     ]
    }
   ],
   "source": [
    "print(submission.head(9)) # gagal "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15694139,
     "sourceId": 130882,
     "sourceType": "competition"
    },
    {
     "datasetId": 9488145,
     "sourceId": 14836320,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21.394295,
   "end_time": "2026-02-17T13:19:00.387759",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-17T13:18:38.993464",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
