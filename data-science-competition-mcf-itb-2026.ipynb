{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "897da3a5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-16T06:06:25.660701Z",
     "iopub.status.busy": "2026-02-16T06:06:25.659612Z",
     "iopub.status.idle": "2026-02-16T06:06:26.759236Z",
     "shell.execute_reply": "2026-02-16T06:06:26.758006Z"
    },
    "papermill": {
     "duration": 1.106956,
     "end_time": "2026-02-16T06:06:26.761193",
     "exception": false,
     "start_time": "2026-02-16T06:06:25.654237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/Data_Klaim.csv\n",
      "/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/sample_submission.csv\n",
      "/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/Data_Polis.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f67e4",
   "metadata": {
    "papermill": {
     "duration": 0.002674,
     "end_time": "2026-02-16T06:06:26.766930",
     "exception": false,
     "start_time": "2026-02-16T06:06:26.764256",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DATA FOUNDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca8e36c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T06:06:26.775443Z",
     "iopub.status.busy": "2026-02-16T06:06:26.774791Z",
     "iopub.status.idle": "2026-02-16T06:06:26.999648Z",
     "shell.execute_reply": "2026-02-16T06:06:26.998480Z"
    },
    "papermill": {
     "duration": 0.231613,
     "end_time": "2026-02-16T06:06:27.001650",
     "exception": false,
     "start_time": "2026-02-16T06:06:26.770037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly shape: (19, 32)\n",
      "Unique months: 19\n",
      "Missing rate: 0.0\n",
      "\n",
      "STAGE 1 ‚Äî OPTIMIZED FORECAST READY\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 1 ‚Äî OPTIMIZED FORECAST VERSION\n",
    "# Clean ‚Ä¢ No Leakage ‚Ä¢ Exposure Aware ‚Ä¢ Regime Features\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\n",
    "\n",
    "klaim = pd.read_csv(BASE_PATH + \"Data_Klaim.csv\")\n",
    "polis = pd.read_csv(BASE_PATH + \"Data_Polis.csv\")\n",
    "\n",
    "# ============================================================\n",
    "# CLEAN COLUMN NAMES\n",
    "# ============================================================\n",
    "\n",
    "def clean_columns(df):\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"_\", regex=False)\n",
    "        .str.replace(\"/\", \"_\", regex=False)\n",
    "        .str.replace(\"-\", \"_\", regex=False)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "klaim = clean_columns(klaim)\n",
    "polis = clean_columns(polis)\n",
    "\n",
    "klaim = klaim.drop_duplicates().reset_index(drop=True)\n",
    "polis = polis.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# ============================================================\n",
    "# DATE PARSING\n",
    "# ============================================================\n",
    "\n",
    "for col in klaim.columns:\n",
    "    if \"tanggal\" in col:\n",
    "        klaim[col] = pd.to_datetime(klaim[col], errors=\"coerce\")\n",
    "\n",
    "for col in polis.columns:\n",
    "    if \"tanggal\" in col:\n",
    "        polis[col] = pd.to_datetime(polis[col], errors=\"coerce\")\n",
    "\n",
    "# ============================================================\n",
    "# BASIC CLEANING\n",
    "# ============================================================\n",
    "\n",
    "klaim = klaim.dropna(subset=[\"nomor_polis\", \"tanggal_pasien_masuk_rs\"])\n",
    "klaim[\"nominal_klaim_yang_disetujui\"] = klaim[\"nominal_klaim_yang_disetujui\"].fillna(0)\n",
    "\n",
    "# Winsorize (robust)\n",
    "low_q = klaim[\"nominal_klaim_yang_disetujui\"].quantile(0.005)\n",
    "high_q = klaim[\"nominal_klaim_yang_disetujui\"].quantile(0.995)\n",
    "klaim[\"nominal_klaim_yang_disetujui\"] = \\\n",
    "    klaim[\"nominal_klaim_yang_disetujui\"].clip(low_q, high_q)\n",
    "\n",
    "# ============================================================\n",
    "# MERGE\n",
    "# ============================================================\n",
    "\n",
    "df = klaim.merge(polis, on=\"nomor_polis\", how=\"left\")\n",
    "\n",
    "for col in [\"plan_code\", \"gender\", \"domisili\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(\"UNKNOWN\")\n",
    "\n",
    "# ============================================================\n",
    "# SERVICE MONTH (NO LEAKAGE)\n",
    "# ============================================================\n",
    "\n",
    "df[\"year_month\"] = df[\"tanggal_pasien_masuk_rs\"].dt.to_period(\"M\")\n",
    "\n",
    "# ============================================================\n",
    "# EXPOSURE FEATURE (üî• IMPORTANT)\n",
    "# ============================================================\n",
    "\n",
    "exposure_monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(active_policies=(\"nomor_polis\",\"nunique\"))\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "df = df.merge(exposure_monthly, on=\"year_month\", how=\"left\")\n",
    "\n",
    "# ============================================================\n",
    "# DEMOGRAPHIC FEATURES\n",
    "# ============================================================\n",
    "\n",
    "if \"tanggal_lahir\" in df.columns:\n",
    "    df[\"age\"] = (\n",
    "        (df[\"tanggal_pasien_masuk_rs\"] - df[\"tanggal_lahir\"]).dt.days / 365\n",
    "    ).clip(0, 100)\n",
    "    df[\"age\"] = df[\"age\"].fillna(df[\"age\"].median())\n",
    "\n",
    "if \"tanggal_efektif_polis\" in df.columns:\n",
    "    df[\"tenure_days\"] = (\n",
    "        df[\"tanggal_pasien_masuk_rs\"] -\n",
    "        df[\"tanggal_efektif_polis\"]\n",
    "    ).dt.days.clip(lower=0)\n",
    "    df[\"tenure_days\"] = df[\"tenure_days\"].fillna(0)\n",
    "\n",
    "if \"tanggal_pasien_keluar_rs\" in df.columns:\n",
    "    df[\"los\"] = (\n",
    "        df[\"tanggal_pasien_keluar_rs\"] -\n",
    "        df[\"tanggal_pasien_masuk_rs\"]\n",
    "    ).dt.days.clip(lower=0)\n",
    "    df[\"los\"] = df[\"los\"].fillna(0)\n",
    "\n",
    "# ============================================================\n",
    "# SEGMENT FEATURES\n",
    "# ============================================================\n",
    "\n",
    "df[\"care_type\"] = df[\"inpatient_outpatient\"].astype(str).str.upper().str.strip()\n",
    "df[\"care_type\"] = df[\"care_type\"].replace([\"NAN\",\"NONE\"],\"UNKNOWN\")\n",
    "df[\"is_inpatient\"] = df[\"care_type\"].eq(\"IP\").astype(int)\n",
    "\n",
    "rc = df[\"reimburse_cashless\"].astype(str).str.upper().str.strip()\n",
    "df[\"is_cashless\"] = rc.eq(\"C\").astype(int)\n",
    "\n",
    "loc = df[\"lokasi_rs\"].astype(str).str.upper().str.strip()\n",
    "\n",
    "df[\"rs_bucket\"] = np.select(\n",
    "    [\n",
    "        loc.eq(\"INDONESIA\"),\n",
    "        loc.eq(\"SINGAPORE\"),\n",
    "        loc.eq(\"MALAYSIA\")\n",
    "    ],\n",
    "    [\"ID\",\"SG\",\"MY\"],\n",
    "    default=\"OTHER\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# ICD REDUCTION\n",
    "# ============================================================\n",
    "\n",
    "df[\"icd_group_raw\"] = (\n",
    "    df[\"icd_diagnosis\"]\n",
    "    .astype(str)\n",
    "    .str.split(\".\").str[0]\n",
    "    .str[:3]\n",
    ")\n",
    "\n",
    "top_icd = df[\"icd_group_raw\"].value_counts().head(40).index\n",
    "\n",
    "df[\"icd_group\"] = np.where(\n",
    "    df[\"icd_group_raw\"].isin(top_icd),\n",
    "    df[\"icd_group_raw\"],\n",
    "    \"OTHER\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# MONTHLY AGGREGATION (CORE FORECAST TABLE)\n",
    "# ============================================================\n",
    "\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(\n",
    "          frequency=(\"nomor_polis\",\"count\"),\n",
    "          total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\"),\n",
    "          exposure=(\"active_policies\",\"first\")\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(\"year_month\")\n",
    ")\n",
    "\n",
    "monthly[\"severity\"] = (\n",
    "    monthly[\"total_claim\"] /\n",
    "    monthly[\"frequency\"].replace(0,np.nan)\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# SAFE TIME-SERIES FEATURES (NO LEAKAGE)\n",
    "# ============================================================\n",
    "\n",
    "for col in [\"frequency\",\"total_claim\",\"severity\"]:\n",
    "    monthly[f\"{col}_lag1\"] = monthly[col].shift(1)\n",
    "    monthly[f\"{col}_lag2\"] = monthly[col].shift(2)\n",
    "    monthly[f\"{col}_lag3\"] = monthly[col].shift(3)\n",
    "\n",
    "    monthly[f\"{col}_roll3\"] = monthly[col].shift(1).rolling(3).mean()\n",
    "    monthly[f\"{col}_roll6\"] = monthly[col].shift(1).rolling(6).mean()\n",
    "\n",
    "    monthly[f\"{col}_ewm3\"] = monthly[col].shift(1).ewm(span=3).mean()\n",
    "    monthly[f\"{col}_ewm6\"] = monthly[col].shift(1).ewm(span=6).mean()\n",
    "\n",
    "    monthly[f\"{col}_momentum\"] = (\n",
    "        monthly[f\"{col}_lag1\"] - monthly[f\"{col}_lag2\"]\n",
    "    )\n",
    "\n",
    "# Volatility regime\n",
    "monthly[\"total_vol6\"] = (\n",
    "    monthly[\"total_claim\"].shift(1).rolling(6).std()\n",
    ")\n",
    "\n",
    "vol_threshold = monthly[\"total_vol6\"].median()\n",
    "monthly[\"high_vol_regime\"] = (\n",
    "    monthly[\"total_vol6\"] > vol_threshold\n",
    ").astype(int)\n",
    "\n",
    "# Time index\n",
    "monthly[\"month_index\"] = np.arange(len(monthly))\n",
    "\n",
    "# ============================================================\n",
    "# CLEAN NA (NO BFILL LEAKAGE)\n",
    "# ============================================================\n",
    "\n",
    "monthly = monthly.fillna(0)\n",
    "\n",
    "# ============================================================\n",
    "# FINAL CHECK\n",
    "# ============================================================\n",
    "\n",
    "print(\"Monthly shape:\", monthly.shape)\n",
    "print(\"Unique months:\", monthly[\"year_month\"].nunique())\n",
    "print(\"Missing rate:\", monthly.isna().mean().mean())\n",
    "print(\"\\nSTAGE 1 ‚Äî OPTIMIZED FORECAST READY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ffa4d4",
   "metadata": {
    "papermill": {
     "duration": 0.002912,
     "end_time": "2026-02-16T06:06:27.007527",
     "exception": false,
     "start_time": "2026-02-16T06:06:27.004615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TIME-SERIES DATASET ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ba113e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T06:06:27.014856Z",
     "iopub.status.busy": "2026-02-16T06:06:27.014564Z",
     "iopub.status.idle": "2026-02-16T06:06:27.127981Z",
     "shell.execute_reply": "2026-02-16T06:06:27.127134Z"
    },
    "papermill": {
     "duration": 0.119545,
     "end_time": "2026-02-16T06:06:27.129961",
     "exception": false,
     "start_time": "2026-02-16T06:06:27.010416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPACT PANEL SHAPE: (414, 33)\n",
      "Unique segments: 41\n",
      "Columns: 33\n",
      "\n",
      "STAGE 2 ‚Äî ELITE SEGMENT PANEL READY\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 2 ‚Äî ELITE SEGMENT PANEL (FORECAST ALIGNED)\n",
    "# Compact ‚Ä¢ Exposure-aware ‚Ä¢ Regime-aware ‚Ä¢ No Leakage\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "seg_cols = [\n",
    "    \"plan_code\",\n",
    "    \"care_type\",\n",
    "    \"is_cashless\",\n",
    "    \"rs_bucket\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# 1. BUILD SEGMENT MONTHLY\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly = (\n",
    "    df.groupby([\"year_month\"] + seg_cols)\n",
    "      .agg(\n",
    "          frequency=(\"nomor_polis\",\"count\"),\n",
    "          total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\"),\n",
    "          exposure=(\"nomor_polis\",\"nunique\")\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(seg_cols + [\"year_month\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Severity per segmen\n",
    "seg_monthly[\"severity\"] = (\n",
    "    seg_monthly[\"total_claim\"] /\n",
    "    seg_monthly[\"frequency\"].replace(0, np.nan)\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 2. TARGET TRANSFORM\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"log_total\"] = np.log1p(seg_monthly[\"total_claim\"])\n",
    "seg_monthly[\"log_freq\"]  = np.log1p(seg_monthly[\"frequency\"])\n",
    "seg_monthly[\"log_sev\"]   = np.log1p(seg_monthly[\"severity\"])\n",
    "\n",
    "# ============================================================\n",
    "# 3. CALENDAR\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"month\"] = seg_monthly[\"year_month\"].dt.month\n",
    "seg_monthly[\"month_sin\"] = np.sin(2*np.pi*seg_monthly[\"month\"]/12)\n",
    "seg_monthly[\"month_cos\"] = np.cos(2*np.pi*seg_monthly[\"month\"]/12)\n",
    "\n",
    "# ============================================================\n",
    "# 4. CORE LAGS (STRICT NO LEAKAGE)\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly = seg_monthly.sort_values(seg_cols + [\"year_month\"])\n",
    "\n",
    "for col in [\"log_total\",\"log_freq\",\"log_sev\"]:\n",
    "\n",
    "    seg_monthly[f\"{col}_lag1\"] = \\\n",
    "        seg_monthly.groupby(seg_cols)[col].shift(1)\n",
    "\n",
    "    seg_monthly[f\"{col}_lag2\"] = \\\n",
    "        seg_monthly.groupby(seg_cols)[col].shift(2)\n",
    "\n",
    "    seg_monthly[f\"{col}_lag3\"] = \\\n",
    "        seg_monthly.groupby(seg_cols)[col].shift(3)\n",
    "\n",
    "    seg_monthly[f\"{col}_roll3\"] = \\\n",
    "        seg_monthly.groupby(seg_cols)[col] \\\n",
    "        .transform(lambda x: x.shift(1).rolling(3).mean())\n",
    "\n",
    "# ============================================================\n",
    "# 5. MOMENTUM + GROWTH (VERY STRONG SIGNAL)\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"momentum_total\"] = (\n",
    "    seg_monthly[\"log_total_lag1\"] -\n",
    "    seg_monthly[\"log_total_lag2\"]\n",
    ")\n",
    "\n",
    "seg_monthly[\"momentum_freq\"] = (\n",
    "    seg_monthly[\"log_freq_lag1\"] -\n",
    "    seg_monthly[\"log_freq_lag2\"]\n",
    ")\n",
    "\n",
    "seg_monthly[\"growth_total\"] = (\n",
    "    seg_monthly[\"log_total_lag1\"] -\n",
    "    seg_monthly[\"log_total_lag3\"]\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 6. VOLATILITY REGIME (PER SEGMENT)\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"seg_vol3\"] = (\n",
    "    seg_monthly.groupby(seg_cols)[\"log_total\"]\n",
    "    .transform(lambda x: x.shift(1).rolling(3).std())\n",
    ")\n",
    "\n",
    "vol_threshold = seg_monthly[\"seg_vol3\"].median()\n",
    "\n",
    "seg_monthly[\"high_vol_regime\"] = (\n",
    "    seg_monthly[\"seg_vol3\"] > vol_threshold\n",
    ").astype(int)\n",
    "\n",
    "# ============================================================\n",
    "# 7. SEGMENT WEIGHT (STABLE AGGREGATION)\n",
    "# ============================================================\n",
    "\n",
    "seg_monthly[\"seg_weight\"] = (\n",
    "    seg_monthly[\"frequency\"] /\n",
    "    seg_monthly.groupby(\"year_month\")[\"frequency\"].transform(\"sum\")\n",
    ").fillna(0)\n",
    "\n",
    "# ============================================================\n",
    "# 8. DROP EARLY MONTHS (SAFE TRAINING WINDOW)\n",
    "# ============================================================\n",
    "\n",
    "seg_model = seg_monthly[\n",
    "    seg_monthly[\"log_total_lag3\"].notna()\n",
    "].reset_index(drop=True)\n",
    "\n",
    "seg_model = seg_model.fillna(0)\n",
    "\n",
    "# ============================================================\n",
    "# FINAL CHECK\n",
    "# ============================================================\n",
    "\n",
    "print(\"COMPACT PANEL SHAPE:\", seg_model.shape)\n",
    "print(\"Unique segments:\", seg_model[seg_cols].drop_duplicates().shape[0])\n",
    "print(\"Columns:\", len(seg_model.columns))\n",
    "print(\"\\nSTAGE 2 ‚Äî ELITE SEGMENT PANEL READY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea4511",
   "metadata": {
    "papermill": {
     "duration": 0.003125,
     "end_time": "2026-02-16T06:06:27.136821",
     "exception": false,
     "start_time": "2026-02-16T06:06:27.133696",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MODEL DEVELOPMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b480cd73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T06:06:27.144710Z",
     "iopub.status.busy": "2026-02-16T06:06:27.144212Z",
     "iopub.status.idle": "2026-02-16T06:06:29.553027Z",
     "shell.execute_reply": "2026-02-16T06:06:29.552219Z"
    },
    "papermill": {
     "duration": 2.41655,
     "end_time": "2026-02-16T06:06:29.556357",
     "exception": false,
     "start_time": "2026-02-16T06:06:27.139807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "STRUCT v6 WMAPE Frequency : 3.97\n",
      "STRUCT v6 WMAPE Total     : 9.95\n",
      "STRUCT v6 WMAPE Severity  : 7.25\n",
      "Estimated Score           : 7.06\n",
      "STRUCTURAL v6 COMPLETE\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STRUCTURAL v6 (ADAPTIVE SHRINKAGE)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def wmape(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    weights = y_true[mask] / y_true[mask].sum()\n",
    "    return np.sum(weights * np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(\n",
    "          frequency=(\"claim_id\",\"count\"),\n",
    "          total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\"),\n",
    "          exposure=(\"active_policies\",\"first\")\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(\"year_month\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "monthly[\"severity\"] = monthly[\"total_claim\"] / monthly[\"frequency\"].replace(0,np.nan)\n",
    "monthly[\"claim_rate\"] = monthly[\"frequency\"] / monthly[\"exposure\"].replace(0,np.nan)\n",
    "\n",
    "long_run_sev = monthly[\"severity\"].mean()\n",
    "long_run_std = monthly[\"severity\"].std()\n",
    "\n",
    "freq_scores = []\n",
    "total_scores = []\n",
    "sev_scores = []\n",
    "\n",
    "for i in range(8, len(monthly)-1):\n",
    "\n",
    "    train = monthly.iloc[:i]\n",
    "    valid = monthly.iloc[i]\n",
    "\n",
    "    # -------- CLAIM RATE --------\n",
    "    model_rate = ExponentialSmoothing(\n",
    "        np.log1p(train[\"claim_rate\"]),\n",
    "        trend=\"add\",\n",
    "        damped_trend=True,\n",
    "        seasonal=None\n",
    "    ).fit()\n",
    "\n",
    "    pred_rate = np.expm1(model_rate.forecast(1).iloc[0])\n",
    "    pred_freq = pred_rate * valid[\"exposure\"]\n",
    "\n",
    "    # -------- SEVERITY --------\n",
    "    model_sev = ExponentialSmoothing(\n",
    "        np.log1p(train[\"severity\"]),\n",
    "        trend=\"add\",\n",
    "        damped_trend=True,\n",
    "        seasonal=None\n",
    "    ).fit()\n",
    "\n",
    "    pred_sev_raw = np.expm1(model_sev.forecast(1).iloc[0])\n",
    "\n",
    "    # -------- ADAPTIVE SHRINKAGE --------\n",
    "    recent_std = train[\"severity\"].tail(6).std()\n",
    "    volatility_ratio = recent_std / long_run_std if long_run_std != 0 else 1\n",
    "\n",
    "    w = 1 / (1 + volatility_ratio)\n",
    "    w = np.clip(w, 0.6, 0.9)\n",
    "\n",
    "    pred_sev = w * pred_sev_raw + (1 - w) * long_run_sev\n",
    "\n",
    "    pred_total = pred_freq * pred_sev\n",
    "\n",
    "    total_scores.append(wmape([valid[\"total_claim\"]],[pred_total]))\n",
    "    freq_scores.append(wmape([valid[\"frequency\"]],[pred_freq]))\n",
    "    sev_scores.append(wmape([valid[\"severity\"]],[pred_sev]))\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"STRUCT v6 WMAPE Frequency :\", round(np.mean(freq_scores),2))\n",
    "print(\"STRUCT v6 WMAPE Total     :\", round(np.mean(total_scores),2))\n",
    "print(\"STRUCT v6 WMAPE Severity  :\", round(np.mean(sev_scores),2))\n",
    "print(\"Estimated Score           :\", round(np.mean([\n",
    "    np.mean(freq_scores),\n",
    "    np.mean(total_scores),\n",
    "    np.mean(sev_scores)\n",
    "]),2))\n",
    "print(\"STRUCTURAL v6 COMPLETE\")\n",
    "print(\"==============================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a366a",
   "metadata": {
    "papermill": {
     "duration": 0.005387,
     "end_time": "2026-02-16T06:06:29.566726",
     "exception": false,
     "start_time": "2026-02-16T06:06:29.561339",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TOTAL CLAIM OPTIMIZATION & VALIDATION, OPTUNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a5e8b85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T06:06:29.579362Z",
     "iopub.status.busy": "2026-02-16T06:06:29.578809Z",
     "iopub.status.idle": "2026-02-16T06:06:31.278602Z",
     "shell.execute_reply": "2026-02-16T06:06:31.277463Z"
    },
    "papermill": {
     "duration": 1.708635,
     "end_time": "2026-02-16T06:06:31.280485",
     "exception": false,
     "start_time": "2026-02-16T06:06:29.571850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "FREQ WMAPE : 0.0414\n",
      "SEV  WMAPE : 0.0731\n",
      "TOTAL WMAPE: 0.0974\n",
      "EST SCORE  : 0.0706\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 4 v17 ‚Äî DUAL TOTAL ANCHOR MODEL\n",
    "# AGGRESSIVE 5% ATTEMPT\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def wmape(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    weights = y_true[mask] / y_true[mask].sum()\n",
    "    return np.sum(weights * np.abs((y_true[mask] - y_pred[mask]) / y_true[mask]))\n",
    "\n",
    "# ============================================================\n",
    "# BUILD MONTHLY\n",
    "# ============================================================\n",
    "\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(\n",
    "          frequency=(\"claim_id\",\"count\"),\n",
    "          total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\"),\n",
    "          exposure=(\"active_policies\",\"first\")\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(\"year_month\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "monthly[\"severity\"] = monthly[\"total_claim\"] / monthly[\"frequency\"].replace(0,np.nan)\n",
    "monthly[\"claim_rate\"] = monthly[\"frequency\"] / monthly[\"exposure\"].replace(0,np.nan)\n",
    "\n",
    "monthly[\"log_rate\"] = np.log1p(monthly[\"claim_rate\"])\n",
    "monthly[\"log_sev\"]  = np.log1p(monthly[\"severity\"])\n",
    "monthly[\"log_total\"] = np.log1p(monthly[\"total_claim\"])\n",
    "\n",
    "monthly[\"t\"] = np.arange(len(monthly))\n",
    "\n",
    "long_run_rate = monthly[\"claim_rate\"].mean()\n",
    "long_run_sev  = monthly[\"severity\"].mean()\n",
    "\n",
    "freq_err = []\n",
    "sev_err = []\n",
    "total_err = []\n",
    "\n",
    "for i in range(8, len(monthly)-1):\n",
    "\n",
    "    train = monthly.iloc[:i]\n",
    "    valid = monthly.iloc[i]\n",
    "\n",
    "    # =========================\n",
    "    # CLAIM RATE MODEL\n",
    "    # =========================\n",
    "    X = pd.DataFrame({\"const\":1, \"t\":train[\"t\"]})\n",
    "    model_rate = sm.OLS(train[\"log_rate\"], X).fit()\n",
    "\n",
    "    future_X = pd.DataFrame({\"const\":[1], \"t\":[train[\"t\"].iloc[-1]+1]})\n",
    "    trend_rate = model_rate.predict(future_X)[0]\n",
    "\n",
    "    residual_rate = train[\"log_rate\"] - model_rate.predict(X)\n",
    "    resid_rate = residual_rate.tail(3).mean()\n",
    "\n",
    "    rate_pred = np.expm1(trend_rate + 0.5*resid_rate)\n",
    "    rate_pred = 0.9*rate_pred + 0.1*long_run_rate\n",
    "\n",
    "    freq_pred = rate_pred * valid[\"exposure\"]\n",
    "\n",
    "    # =========================\n",
    "    # SEVERITY MODEL\n",
    "    # =========================\n",
    "    model_sev = sm.OLS(train[\"log_sev\"], X).fit()\n",
    "    trend_sev = model_sev.predict(future_X)[0]\n",
    "\n",
    "    residual_sev = train[\"log_sev\"] - model_sev.predict(X)\n",
    "    resid_sev = residual_sev.tail(3).mean()\n",
    "\n",
    "    sev_pred = np.expm1(trend_sev + 0.6*resid_sev)\n",
    "\n",
    "    recent_std = train[\"severity\"].tail(6).std()\n",
    "    global_std = monthly[\"severity\"].std()\n",
    "\n",
    "    vol_ratio = recent_std/global_std if global_std!=0 else 1\n",
    "    w = np.clip(1/(1+vol_ratio), 0.6, 0.9)\n",
    "\n",
    "    sev_pred = w*sev_pred + (1-w)*long_run_sev\n",
    "\n",
    "    # =========================\n",
    "    # TOTAL ANCHOR MODEL\n",
    "    # =========================\n",
    "    model_total = sm.OLS(train[\"log_total\"], X).fit()\n",
    "    trend_total = model_total.predict(future_X)[0]\n",
    "\n",
    "    residual_total = train[\"log_total\"] - model_total.predict(X)\n",
    "    resid_total = residual_total.tail(3).mean()\n",
    "\n",
    "    total_anchor = np.expm1(trend_total + 0.6*resid_total)\n",
    "\n",
    "    # =========================\n",
    "    # STRUCTURAL TOTAL\n",
    "    # =========================\n",
    "    total_struct = freq_pred * sev_pred\n",
    "\n",
    "    # =========================\n",
    "    # FINAL BLEND\n",
    "    # =========================\n",
    "    total_pred = 0.6*total_anchor + 0.4*total_struct\n",
    "\n",
    "    # anti-drift clamp\n",
    "    lower = train[\"total_claim\"].tail(6).min()*0.85\n",
    "    upper = train[\"total_claim\"].tail(6).max()*1.15\n",
    "    total_pred = np.clip(total_pred, lower, upper)\n",
    "\n",
    "    # =========================\n",
    "    # METRICS\n",
    "    # =========================\n",
    "    freq_err.append(wmape([valid[\"frequency\"]],[freq_pred]))\n",
    "    sev_err.append(wmape([valid[\"severity\"]],[sev_pred]))\n",
    "    total_err.append(wmape([valid[\"total_claim\"]],[total_pred]))\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"FREQ WMAPE :\", round(np.mean(freq_err),4))\n",
    "print(\"SEV  WMAPE :\", round(np.mean(sev_err),4))\n",
    "print(\"TOTAL WMAPE:\", round(np.mean(total_err),4))\n",
    "print(\"EST SCORE  :\", round(np.mean([\n",
    "    np.mean(freq_err),\n",
    "    np.mean(sev_err),\n",
    "    np.mean(total_err)\n",
    "]),4))\n",
    "print(\"==============================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85ca629",
   "metadata": {
    "papermill": {
     "duration": 0.003085,
     "end_time": "2026-02-16T06:06:31.286825",
     "exception": false,
     "start_time": "2026-02-16T06:06:31.283740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TEST PREDICTION & KAGGLE SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13a73671",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T06:06:31.295962Z",
     "iopub.status.busy": "2026-02-16T06:06:31.294914Z",
     "iopub.status.idle": "2026-02-16T06:06:31.373999Z",
     "shell.execute_reply": "2026-02-16T06:06:31.372896Z"
    },
    "papermill": {
     "duration": 0.085548,
     "end_time": "2026-02-16T06:06:31.375877",
     "exception": false,
     "start_time": "2026-02-16T06:06:31.290329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission created ‚Äî MATCHED TO STAGE 4 v17\n",
      "                        id         value\n",
      "0  2025_08_Claim_Frequency  2.623287e+02\n",
      "1   2025_08_Claim_Severity  5.122518e+07\n",
      "2      2025_08_Total_Claim  1.254777e+10\n",
      "3  2025_09_Claim_Frequency  2.649729e+02\n",
      "4   2025_09_Claim_Severity  5.098988e+07\n",
      "5      2025_09_Total_Claim  1.264436e+10\n",
      "6  2025_10_Claim_Frequency  2.636708e+02\n",
      "7   2025_10_Claim_Severity  5.048497e+07\n",
      "8      2025_10_Total_Claim  1.263944e+10\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 5 ‚Äî FINAL SUBMISSION (CONSISTENT WITH STAGE 4 v17)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\n",
    "sample_sub = pd.read_csv(BASE_PATH + \"sample_submission.csv\")\n",
    "\n",
    "# ============================================================\n",
    "# BUILD MONTHLY (IDENTICAL TO STAGE 4 v17)\n",
    "# ============================================================\n",
    "\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(\n",
    "          frequency=(\"claim_id\",\"count\"),\n",
    "          total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\"),\n",
    "          exposure=(\"active_policies\",\"first\")\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(\"year_month\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "monthly[\"severity\"] = monthly[\"total_claim\"] / monthly[\"frequency\"].replace(0,np.nan)\n",
    "monthly[\"claim_rate\"] = monthly[\"frequency\"] / monthly[\"exposure\"].replace(0,np.nan)\n",
    "\n",
    "monthly[\"log_rate\"] = np.log1p(monthly[\"claim_rate\"])\n",
    "monthly[\"log_sev\"]  = np.log1p(monthly[\"severity\"])\n",
    "monthly[\"log_total\"] = np.log1p(monthly[\"total_claim\"])\n",
    "\n",
    "monthly[\"t\"] = np.arange(len(monthly))\n",
    "\n",
    "long_run_rate = monthly[\"claim_rate\"].mean()\n",
    "long_run_sev  = monthly[\"severity\"].mean()\n",
    "\n",
    "# ============================================================\n",
    "# PREPARE FUTURE PERIODS\n",
    "# ============================================================\n",
    "\n",
    "sample_sub[\"year\"]  = sample_sub[\"id\"].str.split(\"_\").str[0]\n",
    "sample_sub[\"month\"] = sample_sub[\"id\"].str.split(\"_\").str[1]\n",
    "sample_sub[\"month_key\"] = sample_sub[\"year\"] + \"-\" + sample_sub[\"month\"]\n",
    "\n",
    "future_periods = (\n",
    "    pd.PeriodIndex(sample_sub[\"month_key\"], freq=\"M\")\n",
    "      .unique()\n",
    "      .sort_values()\n",
    ")\n",
    "\n",
    "sim_df = monthly.copy()\n",
    "predictions = {}\n",
    "\n",
    "# ============================================================\n",
    "# TRUE RECURSIVE FORECAST (IDENTICAL STRUCTURE)\n",
    "# ============================================================\n",
    "\n",
    "for period in future_periods:\n",
    "\n",
    "    train = sim_df.copy()\n",
    "\n",
    "    X = pd.DataFrame({\"const\":1, \"t\":train[\"t\"]})\n",
    "    future_t = train[\"t\"].iloc[-1] + 1\n",
    "    future_X = pd.DataFrame({\"const\":[1], \"t\":[future_t]})\n",
    "\n",
    "    # =========================\n",
    "    # 1Ô∏è‚É£ CLAIM RATE\n",
    "    # =========================\n",
    "    model_rate = sm.OLS(train[\"log_rate\"], X).fit()\n",
    "    trend_rate = model_rate.predict(future_X)[0]\n",
    "\n",
    "    residual_rate = train[\"log_rate\"] - model_rate.predict(X)\n",
    "    resid_rate = residual_rate.tail(3).mean()\n",
    "\n",
    "    rate_pred = np.expm1(trend_rate + 0.5*resid_rate)\n",
    "    rate_pred = 0.9*rate_pred + 0.1*long_run_rate\n",
    "\n",
    "    exposure_next = train[\"exposure\"].iloc[-1]\n",
    "    freq_pred = rate_pred * exposure_next\n",
    "    freq_pred = max(freq_pred, 1)\n",
    "\n",
    "    # =========================\n",
    "    # 2Ô∏è‚É£ SEVERITY\n",
    "    # =========================\n",
    "    model_sev = sm.OLS(train[\"log_sev\"], X).fit()\n",
    "    trend_sev = model_sev.predict(future_X)[0]\n",
    "\n",
    "    residual_sev = train[\"log_sev\"] - model_sev.predict(X)\n",
    "    resid_sev = residual_sev.tail(3).mean()\n",
    "\n",
    "    sev_pred = np.expm1(trend_sev + 0.6*resid_sev)\n",
    "\n",
    "    recent_std = train[\"severity\"].tail(6).std()\n",
    "    global_std = sim_df[\"severity\"].std()\n",
    "\n",
    "    vol_ratio = recent_std/global_std if global_std!=0 else 1\n",
    "    w = np.clip(1/(1+vol_ratio), 0.6, 0.9)\n",
    "\n",
    "    sev_pred = w*sev_pred + (1-w)*long_run_sev\n",
    "\n",
    "    # =========================\n",
    "    # 3Ô∏è‚É£ TOTAL ANCHOR\n",
    "    # =========================\n",
    "    model_total = sm.OLS(train[\"log_total\"], X).fit()\n",
    "    trend_total = model_total.predict(future_X)[0]\n",
    "\n",
    "    residual_total = train[\"log_total\"] - model_total.predict(X)\n",
    "    resid_total = residual_total.tail(3).mean()\n",
    "\n",
    "    total_anchor = np.expm1(trend_total + 0.6*resid_total)\n",
    "\n",
    "    # =========================\n",
    "    # 4Ô∏è‚É£ STRUCTURAL TOTAL\n",
    "    # =========================\n",
    "    total_struct = freq_pred * sev_pred\n",
    "\n",
    "    # =========================\n",
    "    # 5Ô∏è‚É£ FINAL BLEND (IDENTICAL TO STAGE 4)\n",
    "    # =========================\n",
    "    total_pred = 0.6*total_anchor + 0.4*total_struct\n",
    "\n",
    "    lower = train[\"total_claim\"].tail(6).min()*0.85\n",
    "    upper = train[\"total_claim\"].tail(6).max()*1.15\n",
    "    total_pred = np.clip(total_pred, lower, upper)\n",
    "\n",
    "    # =========================\n",
    "    # UPDATE SIMULATION\n",
    "    # =========================\n",
    "    new_row = {\n",
    "        \"year_month\": period,\n",
    "        \"frequency\": freq_pred,\n",
    "        \"total_claim\": total_pred,\n",
    "        \"exposure\": exposure_next,\n",
    "        \"severity\": sev_pred,\n",
    "        \"claim_rate\": rate_pred,\n",
    "        \"log_rate\": np.log1p(rate_pred),\n",
    "        \"log_sev\": np.log1p(sev_pred),\n",
    "        \"log_total\": np.log1p(total_pred),\n",
    "        \"t\": future_t\n",
    "    }\n",
    "\n",
    "    sim_df = pd.concat([sim_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    key = f\"{period.year}_{str(period.month).zfill(2)}\"\n",
    "    predictions[f\"{key}_Total_Claim\"] = total_pred\n",
    "    predictions[f\"{key}_Claim_Frequency\"] = freq_pred\n",
    "    predictions[f\"{key}_Claim_Severity\"] = sev_pred\n",
    "\n",
    "# ============================================================\n",
    "# BUILD SUBMISSION\n",
    "# ============================================================\n",
    "\n",
    "submission = sample_sub.copy()\n",
    "submission[\"value\"] = submission[\"id\"].map(predictions)\n",
    "submission = submission[[\"id\",\"value\"]]\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"Submission created ‚Äî MATCHED TO STAGE 4 v17\")\n",
    "print(submission.head(9))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15694139,
     "sourceId": 130882,
     "sourceType": "competition"
    },
    {
     "datasetId": 9488145,
     "sourceId": 14836320,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.634418,
   "end_time": "2026-02-16T06:06:31.999560",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-16T06:06:22.365142",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
