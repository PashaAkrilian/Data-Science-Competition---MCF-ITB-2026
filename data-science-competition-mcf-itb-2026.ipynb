{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":14836320,"datasetId":9488145,"databundleVersionId":15694757}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":865.311474,"end_time":"2026-02-21T02:10:22.600217","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-02-21T01:55:57.288743","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"07d6e17a70f04468b325a8acaee1e5e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0f1703fc02964c0cb4f828450fc2bfc3":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30f558bb77b34fb7b54898e7fa2edb23":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_0f1703fc02964c0cb4f828450fc2bfc3","placeholder":"â€‹","style":"IPY_MODEL_3573d84e21764a549876dbea7e4faf23","tabbable":null,"tooltip":null,"value":"Bestâ€‡trial:â€‡660.â€‡Bestâ€‡value:â€‡0.0701207:â€‡100%"}},"3573d84e21764a549876dbea7e4faf23":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"36d21141cf4e4e5a9b04c59b50c76f9a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"98663fc44afc46f6826f3030e65ac624":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_b86d9b0873b74e16864164d1e20c0f85","placeholder":"â€‹","style":"IPY_MODEL_36d21141cf4e4e5a9b04c59b50c76f9a","tabbable":null,"tooltip":null,"value":"â€‡700/700â€‡[12:46&lt;00:00,â€‡â€‡1.17s/it]"}},"a69c7e7b1a6c4a589de2b57075a615a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_d4377338b8df4ec5b0648042f4d101b0","max":700,"min":0,"orientation":"horizontal","style":"IPY_MODEL_07d6e17a70f04468b325a8acaee1e5e7","tabbable":null,"tooltip":null,"value":700}},"b86d9b0873b74e16864164d1e20c0f85":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1f356977b03439fa8283216a3840e5b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_30f558bb77b34fb7b54898e7fa2edb23","IPY_MODEL_a69c7e7b1a6c4a589de2b57075a615a3","IPY_MODEL_98663fc44afc46f6826f3030e65ac624"],"layout":"IPY_MODEL_ccb5d127cd5d4736a04cbfc129e87207","tabbable":null,"tooltip":null}},"ccb5d127cd5d4736a04cbfc129e87207":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4377338b8df4ec5b0648042f4d101b0":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"b03572ea","cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2026-02-22T04:36:53.526641Z","iopub.execute_input":"2026-02-22T04:36:53.526977Z","iopub.status.idle":"2026-02-22T04:36:54.965566Z","shell.execute_reply.started":"2026-02-22T04:36:53.526950Z","shell.execute_reply":"2026-02-22T04:36:54.964554Z"},"papermill":{"duration":1.26471,"end_time":"2026-02-21T01:56:02.348114","exception":false,"start_time":"2026-02-21T01:56:01.083404","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/Data_Klaim.csv\n/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/sample_submission.csv\n/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/Data_Polis.csv\n","output_type":"stream"}],"execution_count":1},{"id":"cdc8b6d1","cell_type":"markdown","source":"# DATA FOUNDATION","metadata":{"papermill":{"duration":0.004108,"end_time":"2026-02-21T01:56:02.356826","exception":false,"start_time":"2026-02-21T01:56:02.352718","status":"completed"},"tags":[]}},{"id":"afdd05a7","cell_type":"code","source":"# ============================================================\n# STAGE 1 v5 â€” FOUNDATION (LB-GRADE AUDITABLE)\n# Key upgrades:\n# - Multi date-mode: build year_month from service/payment/other tanggal columns\n# - No month gets dropped because severity/rate NaN (safe fill + eps logs)\n# - Exposure: claimant + inforce (audit + fallback) without hiding issues\n# ============================================================\n\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nBASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\nklaim = pd.read_csv(BASE_PATH + \"Data_Klaim.csv\")\npolis = pd.read_csv(BASE_PATH + \"Data_Polis.csv\")\n\n# =============================\n# CLEAN COLUMN NAMES\n# =============================\ndef clean_columns(df):\n    df = df.copy()\n    df.columns = (\n        df.columns.astype(str)\n        .str.strip()\n        .str.lower()\n        .str.replace(\" \", \"_\", regex=False)\n        .str.replace(\"/\", \"_\", regex=False)\n        .str.replace(\"-\", \"_\", regex=False)\n    )\n    return df\n\nklaim = clean_columns(klaim)\npolis = clean_columns(polis)\n\n# =============================\n# DATE PARSING (YYYYMMDD int + dd/mm/yyyy + general)\n# =============================\ndef parse_mixed_date(s: pd.Series) -> pd.Series:\n    s = s.copy()\n    idx = s.index\n\n    if pd.api.types.is_numeric_dtype(s):\n        ss = s.astype(\"Int64\").astype(str)\n    else:\n        ss = s.astype(str).str.strip()\n\n    ss = ss.replace({\"<NA>\": np.nan, \"nan\": np.nan, \"None\": np.nan, \"NaT\": np.nan})\n\n    out = pd.Series(pd.NaT, index=idx, dtype=\"datetime64[ns]\")\n\n    m8 = ss.str.fullmatch(r\"\\d{8}\", na=False)\n    if m8.any():\n        out.loc[m8] = pd.to_datetime(ss.loc[m8], format=\"%Y%m%d\", errors=\"coerce\")\n\n    rest = ~m8 & ss.notna()\n    if rest.any():\n        has_slash = ss.loc[rest].str.contains(\"/\", na=False)\n        if has_slash.any():\n            out.loc[ss.loc[rest][has_slash].index] = pd.to_datetime(\n                ss.loc[rest][has_slash], errors=\"coerce\", dayfirst=True\n            )\n        if (~has_slash).any():\n            out.loc[ss.loc[rest][~has_slash].index] = pd.to_datetime(\n                ss.loc[rest][~has_slash], errors=\"coerce\"\n            )\n    return out\n\nfor col in klaim.columns:\n    if \"tanggal\" in col:\n        klaim[col] = parse_mixed_date(klaim[col])\n\nfor col in polis.columns:\n    if \"tanggal\" in col:\n        polis[col] = parse_mixed_date(polis[col])\n\n# =============================\n# SAFE DEDUP\n# =============================\nclaim_id_col = None\nfor c in [\"claim_id\", \"id_klaim\", \"klaim_id\"]:\n    if c in klaim.columns:\n        claim_id_col = c\n        break\n\nif claim_id_col is not None:\n    klaim = klaim.drop_duplicates(subset=[claim_id_col]).reset_index(drop=True)\nelse:\n    klaim = klaim.drop_duplicates().reset_index(drop=True)\n\nif \"nomor_polis\" in polis.columns:\n    polis = polis.drop_duplicates(subset=[\"nomor_polis\"]).reset_index(drop=True)\nelse:\n    polis = polis.drop_duplicates().reset_index(drop=True)\n\n# =============================\n# NOMINAL COLUMN (RAW TARGET)\n# =============================\nnom_col = \"nominal_klaim_yang_disetujui\"\nif nom_col not in klaim.columns:\n    cand = [c for c in klaim.columns if \"nominal\" in c]\n    if len(cand) == 0:\n        raise ValueError(\"No nominal column found in klaim.\")\n    nom_col = cand[0]\n\nraw_nom = pd.to_numeric(klaim[nom_col], errors=\"coerce\").fillna(0.0)\nraw_nom = raw_nom.clip(lower=0.0)\nklaim[nom_col] = raw_nom\n\n# Optional winsor copy (features only)\nklaim[\"nominal_klaim_clip\"] = raw_nom.copy()\npos = klaim[\"nominal_klaim_clip\"] > 0\nif pos.any():\n    low_q  = klaim.loc[pos, \"nominal_klaim_clip\"].quantile(0.005)\n    high_q = klaim.loc[pos, \"nominal_klaim_clip\"].quantile(0.995)\n    klaim.loc[pos, \"nominal_klaim_clip\"] = klaim.loc[pos, \"nominal_klaim_clip\"].clip(low_q, high_q)\n\n# =============================\n# MERGE\n# =============================\nif \"nomor_polis\" not in klaim.columns:\n    raise ValueError(\"klaim tidak punya kolom nomor_polis.\")\ndf0 = klaim.merge(polis, on=\"nomor_polis\", how=\"left\")\n\n# =============================\n# DATE MODE: build multiple year_month candidates\n# =============================\n# Kamu boleh ganti prioritas kandidat ini sesuai kolom real di datamu.\ndate_candidates = []\n# Prefer \"service\" style\nfor c in [\"tanggal_pasien_masuk_rs\", \"tanggal_masuk_rs\", \"tanggal_perawatan\"]:\n    if c in df0.columns:\n        date_candidates.append((\"service\", c))\n        break\n\n# Prefer \"payment\" style\nfor c in [\"tanggal_pembayaran_klaim\", \"tanggal_bayar\", \"tanggal_pembayaran\"]:\n    if c in df0.columns:\n        date_candidates.append((\"payment\", c))\n        break\n\n# Fallback: semua tanggal lain di klaim (urutkan stabil)\nother_tcols = [c for c in df0.columns if (\"tanggal\" in c)]\nfor c in other_tcols:\n    if c not in [x[1] for x in date_candidates]:\n        date_candidates.append((f\"other:{c}\", c))\n\nif len(date_candidates) == 0:\n    raise ValueError(\"Tidak ada kolom tanggal untuk membangun year_month.\")\n\nprint(\"Date candidates:\", date_candidates[:6], \"...\" if len(date_candidates) > 6 else \"\")\n\n# =============================\n# EXPOSURE: claimant + inforce (audit friendly)\n# =============================\nstart_col = None\nfor c in [\"tanggal_efektif_polis\", \"tanggal_mulai_polis\", \"tanggal_mulai\"]:\n    if c in polis.columns:\n        start_col = c\n        break\n\ndef build_exposure_tables(df_mode: pd.DataFrame, year_month_col: str):\n    # complete months\n    ym = df_mode[year_month_col].dropna()\n    if ym.empty:\n        raise ValueError(\"year_month kosong pada mode ini.\")\n    min_m = ym.min()\n    max_m = ym.max()\n    all_months = pd.period_range(min_m, max_m, freq=\"M\")\n\n    # claimant exposure: unique policies claiming in month\n    expo_claimant = (\n        df_mode.groupby(year_month_col)[\"nomor_polis\"].nunique()\n              .reindex(all_months, fill_value=0)\n              .rename(\"exposure_claimant\")\n              .rename_axis(\"year_month\")\n              .reset_index()\n    )\n\n    # inforce exposure: cumulative started policies (no end-date)\n    if start_col is not None:\n        p = polis[[\"nomor_polis\", start_col]].dropna(subset=[start_col]).copy()\n        p[\"start_m\"] = p[start_col].dt.to_period(\"M\")\n\n        # number of unique policies that have started by each month\n        # (approx inforce without lapse)\n        started_by_m = (\n            p.groupby(\"start_m\")[\"nomor_polis\"].nunique()\n             .reindex(all_months, fill_value=0)\n             .cumsum()\n        )\n        # add policies that started before min_m\n        base = p.loc[p[\"start_m\"] < min_m, \"nomor_polis\"].nunique()\n        expo_inforce = (base + started_by_m).rename(\"exposure_inforce\").reset_index().rename(columns={\"start_m\":\"year_month\"})\n        expo_inforce = expo_inforce.rename(columns={\"year_month\":\"year_month\"})\n        # ensure key name matches\n        expo_inforce = expo_inforce.rename(columns={\"start_m\":\"year_month\"})\n        if \"year_month\" not in expo_inforce.columns:\n            expo_inforce = expo_inforce.rename(columns={\"index\":\"year_month\"})\n    else:\n        expo_inforce = expo_claimant[[\"year_month\"]].copy()\n        expo_inforce[\"exposure_inforce\"] = 0\n\n    expo = expo_claimant.merge(expo_inforce, on=\"year_month\", how=\"left\")\n    return expo, all_months\n\n# =============================\n# BUILD monthly for each date mode (so you can LB-test quickly)\n# =============================\nEPS = 1e-12\nmonthly_by_mode = {}\ndf_by_mode = {}\n\nfor mode, dcol in date_candidates:\n    tmp = df0.copy()\n    tmp = tmp.dropna(subset=[\"nomor_polis\", dcol]).copy()\n    tmp[\"year_month\"] = tmp[dcol].dt.to_period(\"M\")\n\n    if tmp[\"year_month\"].dropna().empty:\n        continue\n\n    expo, all_months = build_exposure_tables(tmp, \"year_month\")\n\n    freq_col = claim_id_col if claim_id_col is not None and claim_id_col in tmp.columns else \"nomor_polis\"\n    monthly_core = (\n        tmp.groupby(\"year_month\")\n           .agg(\n               frequency=(freq_col, \"count\"),\n               total_claim=(nom_col, \"sum\")\n           )\n           .reindex(all_months, fill_value=0.0)\n           .rename_axis(\"year_month\")\n           .reset_index()\n    )\n\n    monthly = monthly_core.merge(expo, on=\"year_month\", how=\"left\")\n\n    # exposure choice (do NOT hide with ffill/bfill at this stage)\n    # we keep both; later stages can choose\n    # but provide a default exposure that is safest:\n    # if inforce is mostly zeros -> fallback to claimant\n    inforce_sum = float(np.nansum(monthly[\"exposure_inforce\"].values))\n    claimant_sum = float(np.nansum(monthly[\"exposure_claimant\"].values))\n    if inforce_sum > 0:\n        monthly[\"exposure\"] = monthly[\"exposure_inforce\"].astype(float)\n        exposure_mode_used = \"inforce\"\n    else:\n        monthly[\"exposure\"] = monthly[\"exposure_claimant\"].astype(float)\n        exposure_mode_used = \"claimant\"\n\n    # Targets & derived series (safe for zeros; no month dropped)\n    monthly[\"frequency\"] = monthly[\"frequency\"].astype(float)\n    monthly[\"total_claim\"] = monthly[\"total_claim\"].astype(float)\n\n    monthly[\"severity\"] = np.where(monthly[\"frequency\"] > 0, monthly[\"total_claim\"] / monthly[\"frequency\"], 0.0)\n    monthly[\"claim_rate\"] = np.where(monthly[\"exposure\"] > 0, monthly[\"frequency\"] / monthly[\"exposure\"], 0.0)\n\n    # Logs (safe)\n    monthly[\"log_total\"] = np.log1p(np.clip(monthly[\"total_claim\"], 0.0, np.inf))\n    monthly[\"log_freq\"]  = np.log1p(np.clip(monthly[\"frequency\"], 0.0, np.inf))\n    monthly[\"log_sev\"]   = np.log1p(np.clip(monthly[\"severity\"], 0.0, np.inf))\n    monthly[\"log_rate\"]  = np.log1p(np.clip(monthly[\"claim_rate\"], 0.0, np.inf))\n\n    # Time features\n    monthly[\"month\"] = monthly[\"year_month\"].dt.month\n    monthly[\"month_sin\"] = np.sin(2*np.pi*monthly[\"month\"]/12)\n    monthly[\"month_cos\"] = np.cos(2*np.pi*monthly[\"month\"]/12)\n    monthly[\"month_index\"] = np.arange(len(monthly), dtype=int)\n\n    # Volatility (safe)\n    monthly[\"roll6\"] = monthly[\"total_claim\"].rolling(6, min_periods=3).mean()\n    monthly[\"std6\"]  = monthly[\"total_claim\"].rolling(6, min_periods=3).std()\n    monthly[\"vol_ratio\"] = monthly[\"std6\"] / np.where(monthly[\"roll6\"] > 0, monthly[\"roll6\"], np.nan)\n    med = np.nanmedian(monthly[\"vol_ratio\"].values)\n    monthly[\"high_vol_regime\"] = (monthly[\"vol_ratio\"] > med).astype(int)\n\n    # Lags (keep NaN; downstream stages decide how to fill)\n    for col in [\"log_total\", \"log_freq\", \"log_sev\", \"log_rate\"]:\n        monthly[f\"{col}_lag1\"] = monthly[col].shift(1)\n        monthly[f\"{col}_lag2\"] = monthly[col].shift(2)\n        monthly[f\"{col}_lag3\"] = monthly[col].shift(3)\n        monthly[f\"{col}_roll3\"] = monthly[col].shift(1).rolling(3).mean()\n\n    monthly_by_mode[mode] = (monthly, exposure_mode_used, dcol)\n    df_by_mode[mode] = tmp\n\n# =============================\n# Choose default mode (you can swap later)\n# =============================\nDEFAULT_MODE = \"service\" if \"service\" in monthly_by_mode else list(monthly_by_mode.keys())[0]\nmonthly, EXPOSURE_MODE_USED, SERVICE_COL_USED = monthly_by_mode[DEFAULT_MODE]\n\ndf = df_by_mode[DEFAULT_MODE].copy()\ndf[\"active_policies\"] = df.merge(\n    monthly[[\"year_month\", \"exposure\"]], on=\"year_month\", how=\"left\"\n)[\"exposure\"].values\n\nprint(\"\\n==============================\")\nprint(\"DEFAULT_MODE:\", DEFAULT_MODE, \"| DATE_COL:\", SERVICE_COL_USED, \"| EXPOSURE_USED:\", EXPOSURE_MODE_USED)\nprint(\"Monthly shape:\", monthly.shape, \"| Unique months:\", monthly[\"year_month\"].nunique())\nprint(\"Freq min/max:\", float(monthly[\"frequency\"].min()), float(monthly[\"frequency\"].max()))\nprint(\"Total min/max:\", float(monthly[\"total_claim\"].min()), float(monthly[\"total_claim\"].max()))\nprint(\"Exposure (used) min/max:\", float(monthly[\"exposure\"].min()), float(monthly[\"exposure\"].max()))\nprint(\"Inforce sum:\", float(np.nansum(monthly[\"exposure_inforce\"].values)), \"| Claimant sum:\", float(np.nansum(monthly[\"exposure_claimant\"].values)))\nprint(\"==============================\")\nprint(\"STAGE 1 v5 â€” READY (monthly_by_mode available)\")\nprint(\"Available modes:\", list(monthly_by_mode.keys()))","metadata":{"execution":{"iopub.status.busy":"2026-02-22T17:06:17.092936Z","iopub.execute_input":"2026-02-22T17:06:17.093652Z","iopub.status.idle":"2026-02-22T17:06:17.901363Z","shell.execute_reply.started":"2026-02-22T17:06:17.093615Z","shell.execute_reply":"2026-02-22T17:06:17.900578Z"},"papermill":{"duration":0.309998,"end_time":"2026-02-21T01:56:02.671206","exception":false,"start_time":"2026-02-21T01:56:02.361208","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Date candidates: [('service', 'tanggal_pasien_masuk_rs'), ('payment', 'tanggal_pembayaran_klaim'), ('other:tanggal_pasien_keluar_rs', 'tanggal_pasien_keluar_rs'), ('other:tanggal_lahir', 'tanggal_lahir'), ('other:tanggal_efektif_polis', 'tanggal_efektif_polis')] \n\n==============================\nDEFAULT_MODE: service | DATE_COL: tanggal_pasien_masuk_rs | EXPOSURE_USED: inforce\nMonthly shape: (19, 36) | Unique months: 19\nFreq min/max: 208.0 302.0\nTotal min/max: 9610379678.55 20260981490.193146\nExposure (used) min/max: 4096.0 4096.0\nInforce sum: 77824.0 | Claimant sum: 2671.0\n==============================\nSTAGE 1 v5 â€” READY (monthly_by_mode available)\nAvailable modes: ['service', 'payment', 'other:tanggal_pasien_keluar_rs', 'other:tanggal_lahir', 'other:tanggal_efektif_polis']\n","output_type":"stream"}],"execution_count":1},{"id":"a118955f","cell_type":"markdown","source":"# TIME-SERIES DATASET ENGINEERING","metadata":{"papermill":{"duration":0.004724,"end_time":"2026-02-21T01:56:02.711848","exception":false,"start_time":"2026-02-21T01:56:02.707124","status":"completed"},"tags":[]}},{"id":"1eeaac81","cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# ============================================================\n# ðŸ”¹ BASIC CHECK\n# ============================================================\n\nassert \"monthly\" in globals(), \"Stage 1 belum dijalankan.\"\nassert \"df\" in globals(), \"df tidak tersedia dari Stage 1.\"\n\nall_months = monthly[\"year_month\"].unique()\nportfolio_expo = monthly[[\"year_month\",\"exposure\"]].copy()\nportfolio_sev  = monthly[[\"year_month\",\"severity\"]].copy()\n\n# ============================================================\n# ðŸ”¹ SEGMENT DEFINITIONS (STABLE BUCKETING)\n# ============================================================\n\n# Care Type\nif \"care_type\" not in df.columns:\n    if \"inpatient_outpatient\" in df.columns:\n        df[\"care_type\"] = (\n            df[\"inpatient_outpatient\"]\n            .astype(str)\n            .str.upper()\n            .str.strip()\n        )\n    else:\n        df[\"care_type\"] = \"UNKNOWN\"\ndf[\"care_type\"] = df[\"care_type\"].fillna(\"UNKNOWN\")\n\n# Cashless\nif \"is_cashless\" not in df.columns:\n    if \"reimburse_cashless\" in df.columns:\n        rc = df[\"reimburse_cashless\"].astype(str).str.upper().str.strip()\n        df[\"is_cashless\"] = rc.eq(\"C\").astype(int)\n    else:\n        df[\"is_cashless\"] = 0\n\n# RS bucket\nif \"rs_bucket\" not in df.columns:\n    if \"lokasi_rs\" in df.columns:\n        loc = df[\"lokasi_rs\"].astype(str).str.upper().str.strip()\n        df[\"rs_bucket\"] = np.select(\n            [\n                loc.eq(\"INDONESIA\"),\n                loc.eq(\"SINGAPORE\"),\n                loc.eq(\"MALAYSIA\")\n            ],\n            [\"ID\",\"SG\",\"MY\"],\n            default=\"OTHER\"\n        )\n    else:\n        df[\"rs_bucket\"] = \"OTHER\"\ndf[\"rs_bucket\"] = df[\"rs_bucket\"].fillna(\"OTHER\")\n\n# Plan Code Bucketing (keep top 8 only)\nif \"plan_code\" not in df.columns:\n    df[\"plan_code\"] = \"UNKNOWN\"\n\ntop_plans = (\n    df[\"plan_code\"]\n    .value_counts()\n    .head(8)\n    .index\n)\n\ndf[\"plan_code\"] = np.where(\n    df[\"plan_code\"].isin(top_plans),\n    df[\"plan_code\"],\n    \"OTHER\"\n)\n\nseg_cols = [\"plan_code\",\"care_type\",\"is_cashless\",\"rs_bucket\"]\n\n# ============================================================\n# ðŸ”¹ RAW SEGMENT AGGREGATION\n# ============================================================\n\nseg_raw = (\n    df.groupby([\"year_month\"] + seg_cols)\n      .agg(\n          frequency=(\"nomor_polis\",\"count\"),\n          total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\"),\n          uniq_polis=(\"nomor_polis\",\"nunique\")\n      )\n      .reset_index()\n)\n\n# ============================================================\n# ðŸ”¹ COMPLETE GRID (NO MISSING MONTHS)\n# ============================================================\n\nunique_segments = seg_raw[seg_cols].drop_duplicates()\n\ngrid = (\n    unique_segments.assign(key=1)\n    .merge(pd.DataFrame({\"year_month\": all_months, \"key\":1}), on=\"key\")\n    .drop(\"key\",axis=1)\n)\n\nseg_full = (\n    grid.merge(seg_raw, on=[\"year_month\"]+seg_cols, how=\"left\")\n        .fillna(0)\n)\n\n# ============================================================\n# ðŸ”¹ ROLLING SHARE (STABLE EXPOSURE ALLOCATION)\n# ============================================================\n\n# Rolling share based on uniq policies (3 month rolling)\nseg_full = seg_full.sort_values(seg_cols + [\"year_month\"])\n\nseg_full[\"rolling_uniq\"] = (\n    seg_full.groupby(seg_cols)[\"uniq_polis\"]\n    .transform(lambda x: x.shift(1).rolling(3,min_periods=1).mean())\n)\n\n# Normalize share per month\nseg_full[\"share\"] = (\n    seg_full[\"rolling_uniq\"] /\n    seg_full.groupby(\"year_month\")[\"rolling_uniq\"].transform(\"sum\")\n).fillna(0)\n\n# Merge portfolio exposure\nseg_full = seg_full.merge(portfolio_expo, on=\"year_month\", how=\"left\")\n\nseg_full[\"expo_seg\"] = (\n    seg_full[\"share\"] * seg_full[\"exposure\"]\n).clip(lower=1.0)\n\n# ============================================================\n# ðŸ”¹ SEVERITY SHRINK (ANTI MICRO SEGMENT)\n# ============================================================\n\nseg_full = seg_full.merge(portfolio_sev, on=\"year_month\", how=\"left\", suffixes=(\"\",\"_port\"))\n\nseg_full[\"sev_raw\"] = np.where(\n    seg_full[\"frequency\"] > 0,\n    seg_full[\"total_claim\"] / seg_full[\"frequency\"],\n    0\n)\n\n# shrink factor based on frequency size\nseg_full[\"alpha\"] = (\n    seg_full[\"frequency\"] /\n    (seg_full[\"frequency\"] + 20)\n)\n\nseg_full[\"sev_shrunk\"] = (\n    seg_full[\"alpha\"] * seg_full[\"sev_raw\"] +\n    (1 - seg_full[\"alpha\"]) * seg_full[\"severity\"]\n)\n\n# ============================================================\n# ðŸ”¹ DERIVED METRICS\n# ============================================================\n\nseg_full[\"rate_seg\"] = (\n    seg_full[\"frequency\"] /\n    seg_full[\"expo_seg\"]\n)\n\nseg_full[\"log_total\"] = np.log1p(seg_full[\"total_claim\"])\nseg_full[\"log_freq\"]  = np.log1p(seg_full[\"frequency\"])\nseg_full[\"log_sev\"]   = np.log1p(seg_full[\"sev_shrunk\"])\nseg_full[\"log_rate\"]  = np.log1p(seg_full[\"rate_seg\"])\n\n# Calendar\nseg_full[\"month\"] = seg_full[\"year_month\"].dt.month\nseg_full[\"month_sin\"] = np.sin(2*np.pi*seg_full[\"month\"]/12)\nseg_full[\"month_cos\"] = np.cos(2*np.pi*seg_full[\"month\"]/12)\n\n# Lags (strict no leakage)\nfor col in [\"log_total\",\"log_freq\",\"log_sev\",\"log_rate\"]:\n    seg_full[f\"{col}_lag1\"] = \\\n        seg_full.groupby(seg_cols)[col].shift(1)\n\n    seg_full[f\"{col}_lag2\"] = \\\n        seg_full.groupby(seg_cols)[col].shift(2)\n\n    seg_full[f\"{col}_lag3\"] = \\\n        seg_full.groupby(seg_cols)[col].shift(3)\n\n# ============================================================\n# ðŸ”¹ SAFE TRAIN PANEL\n# ============================================================\n\nseg_model = seg_full[\n    seg_full[\"log_total_lag3\"].notna()\n].reset_index(drop=True)\n\n# DO NOT fill everything with zero blindly\nseg_model = seg_model.fillna(0)\n\n# ============================================================\n# FINAL CHECK\n# ============================================================\n\nprint(\"SEG PANEL SHAPE:\", seg_model.shape)\nprint(\"Unique segments:\", seg_model[seg_cols].drop_duplicates().shape[0])\nprint(\"Months:\", seg_model[\"year_month\"].nunique())\nprint(\"Sum expo check (sample month):\")\n\nsample_month = seg_model[\"year_month\"].iloc[-1]\nprint(\n    \"Segment expo sum:\",\n    seg_model[seg_model[\"year_month\"]==sample_month][\"expo_seg\"].sum(),\n    \"| Portfolio expo:\",\n    monthly.loc[monthly[\"year_month\"]==sample_month,\"exposure\"].values[0]\n)\n\nprint(\"\\nSTAGE 2 v2 â€” ELITE SEGMENT PANEL READY\")","metadata":{"execution":{"iopub.status.busy":"2026-02-22T04:36:55.256268Z","iopub.execute_input":"2026-02-22T04:36:55.256678Z","iopub.status.idle":"2026-02-22T04:36:55.384716Z","shell.execute_reply.started":"2026-02-22T04:36:55.256651Z","shell.execute_reply":"2026-02-22T04:36:55.383937Z"},"papermill":{"duration":0.137741,"end_time":"2026-02-21T01:56:02.853836","exception":false,"start_time":"2026-02-21T01:56:02.716095","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"COMPACT PANEL SHAPE: (414, 29)\nUnique segments: 41\nColumns: 29\n\nSTAGE 2 â€” ELITE SEGMENT PANEL READY\n","output_type":"stream"}],"execution_count":4},{"id":"8d84d3e1","cell_type":"markdown","source":"# MODEL DEVELOPMENT","metadata":{"papermill":{"duration":0.004596,"end_time":"2026-02-21T01:56:02.863041","exception":false,"start_time":"2026-02-21T01:56:02.858445","status":"completed"},"tags":[]}},{"id":"dfd5ada2","cell_type":"code","source":"# ============================================================\n# STAGE 3 v18 â€” SEVERITY-FIRST + RATE MODEL\n# - Model claim_rate & severity directly\n# - Frequency = rate Ã— exposure\n# - Total = freq Ã— severity\n# - Rolling validation (3 splits)\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef mape(y_true, y_pred):\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n    mask = y_true != 0\n    if mask.sum() == 0:\n        return np.nan\n    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n\n# ==========================================\n# BUILD MONTHLY FROM STAGE 1 (already clean)\n# ==========================================\nmonthly = monthly.sort_values(\"year_month\").reset_index(drop=True)\n\nmonthly[\"claim_rate\"] = (\n    monthly[\"frequency\"] /\n    monthly[\"exposure\"].replace(0, np.nan)\n).fillna(0)\n\nmonthly[\"severity\"] = (\n    monthly[\"total_claim\"] /\n    monthly[\"frequency\"].replace(0, np.nan)\n).fillna(0)\n\n# ==========================================\n# HORIZON\n# ==========================================\nsample_sub[\"year\"]  = sample_sub[\"id\"].str.split(\"_\").str[0]\nsample_sub[\"month\"] = sample_sub[\"id\"].str.split(\"_\").str[1]\nsample_sub[\"month_key\"] = sample_sub[\"year\"] + \"-\" + sample_sub[\"month\"]\n\nfuture_periods = (\n    pd.PeriodIndex(sample_sub[\"month_key\"], freq=\"M\")\n      .unique()\n      .sort_values()\n)\n\nH = len(future_periods)\nN = len(monthly)\n\n# ==========================================\n# SIMULATOR\n# ==========================================\ndef simulate(train_df, H, w_rate, w_sev):\n\n    sim = train_df.copy()\n    predF, predT, predS = [], [], []\n\n    for step in range(H):\n\n        hist = sim.copy()\n\n        # ----- CLAIM RATE MODEL -----\n        try:\n            mdl_r = ExponentialSmoothing(\n                np.log1p(hist[\"claim_rate\"].astype(float)),\n                trend=\"add\",\n                damped_trend=True,\n                seasonal=None\n            ).fit()\n            rate_fc = float(np.expm1(mdl_r.forecast(1).iloc[0]))\n        except:\n            rate_fc = float(hist[\"claim_rate\"].iloc[-1])\n\n        rate_anchor = float(hist[\"claim_rate\"].tail(3).median())\n        rate_pred = w_rate*rate_fc + (1-w_rate)*rate_anchor\n        rate_pred = max(rate_pred, 1e-9)\n\n        expo_next = float(hist[\"exposure\"].iloc[-1])\n\n        freq_pred = rate_pred * expo_next\n        freq_pred = max(freq_pred, 1.0)\n\n        # ----- SEVERITY MODEL -----\n        try:\n            mdl_s = ExponentialSmoothing(\n                np.log1p(hist[\"severity\"].astype(float)),\n                trend=\"add\",\n                damped_trend=True,\n                seasonal=None\n            ).fit()\n            sev_fc = float(np.expm1(mdl_s.forecast(1).iloc[0]))\n        except:\n            sev_fc = float(hist[\"severity\"].iloc[-1])\n\n        sev_anchor = float(hist[\"severity\"].tail(3).median())\n        sev_pred = w_sev*sev_fc + (1-w_sev)*sev_anchor\n        sev_pred = max(sev_pred, 1e-6)\n\n        total_pred = freq_pred * sev_pred\n\n        predF.append(freq_pred)\n        predT.append(total_pred)\n        predS.append(sev_pred)\n\n        sim = pd.concat([sim, pd.DataFrame([{\n            \"year_month\": hist[\"year_month\"].iloc[-1] + 1,\n            \"exposure\": expo_next,\n            \"claim_rate\": rate_pred,\n            \"severity\": sev_pred,\n            \"frequency\": freq_pred,\n            \"total_claim\": total_pred\n        }])], ignore_index=True)\n\n    return predF, predT, predS\n\n# ==========================================\n# ROLLING VALIDATION (3 splits)\n# ==========================================\nsplits = [N-H-2, N-H-1, N-H]  # rolling ends\nsplits = [s for s in splits if s > 6]\n\nw_rate_grid = [0.3,0.5,0.7,0.85]\nw_sev_grid  = [0.3,0.5,0.7,0.85]\n\nbest = {\"score\":1e18,\"params\":None}\n\nfor wr in w_rate_grid:\n    for ws in w_sev_grid:\n\n        scores = []\n\n        for split in splits:\n\n            train = monthly.iloc[:split]\n            valid = monthly.iloc[split:split+H]\n\n            pf, pt, ps = simulate(train,H,wr,ws)\n\n            mf = mape(valid[\"frequency\"],pf)\n            mt = mape(valid[\"total_claim\"],pt)\n            ms = mape(valid[\"severity\"],ps)\n\n            scores.append(np.nanmean([mf,mt,ms]))\n\n        avg = np.nanmean(scores)\n\n        if avg < best[\"score\"]:\n            best[\"score\"] = avg\n            best[\"params\"] = (wr,ws)\n\nwr,ws = best[\"params\"]\n\nprint(\"\\n==============================\")\nprint(\"Best Weights:\")\nprint(\"w_rate:\",wr,\"w_sev:\",ws)\nprint(\"Estimated CV Score:\",round(best[\"score\"],4))\nprint(\"==============================\")","metadata":{"execution":{"iopub.status.busy":"2026-02-22T04:36:55.385713Z","iopub.execute_input":"2026-02-22T04:36:55.386010Z","iopub.status.idle":"2026-02-22T04:37:54.576655Z","shell.execute_reply.started":"2026-02-22T04:36:55.385984Z","shell.execute_reply":"2026-02-22T04:37:54.574285Z"},"papermill":{"duration":57.354148,"end_time":"2026-02-21T01:57:00.221592","exception":false,"start_time":"2026-02-21T01:56:02.867444","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\n==============================\nHorizon months used : 5\nBest Config:\n  wt_total=0.85 (ETS weight), anchor_total=median\n  wt_freq =0.2 (ETS weight), anchor_freq =mean\n------------------------------\nSTAGE 3 v17 MAPE Frequency : 5.1557\nSTAGE 3 v17 MAPE Total     : 7.9753\nSTAGE 3 v17 MAPE Severity  : 4.7684\nEstimated Score            : 5.9665\n==============================\n\nPreview last horizon months:\n   year_month  frequency   total_claim      severity  pred_frequency  \\\n14    2025-03        230  1.367924e+10  5.947496e+07      234.031716   \n15    2025-04        208  1.116425e+10  5.367427e+07      232.851773   \n16    2025-05        239  1.222680e+10  5.115814e+07      237.225688   \n17    2025-06        234  1.337312e+10  5.715008e+07      234.888808   \n18    2025-07        264  1.369923e+10  5.189101e+07      235.077202   \n\n      pred_total  pred_severity  \n14  1.224504e+10   5.232214e+07  \n15  1.224868e+10   5.260289e+07  \n16  1.222798e+10   5.154577e+07  \n17  1.221086e+10   5.198572e+07  \n18  1.219531e+10   5.187790e+07  \n","output_type":"stream"}],"execution_count":5},{"id":"159148f8","cell_type":"markdown","source":"# TOTAL CLAIM OPTIMIZATION & VALIDATION, OPTUNA","metadata":{"papermill":{"duration":0.007675,"end_time":"2026-02-21T01:57:00.236211","exception":false,"start_time":"2026-02-21T01:57:00.228536","status":"completed"},"tags":[]}},{"id":"2bf83a8d","cell_type":"code","source":"!pip install -q optuna statsmodels\n\nimport numpy as np\nimport pandas as pd\nimport optuna\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nnp.random.seed(SEED)\n\n# =============================\n# MONTHLY (from Stage 1)\n# =============================\nmonthly = monthly.sort_values(\"year_month\").reset_index(drop=True)\n\nmonthly[\"claim_rate\"] = (\n    monthly[\"frequency\"] /\n    monthly[\"exposure\"].replace(0, np.nan)\n).fillna(0)\n\nmonthly[\"severity\"] = (\n    monthly[\"total_claim\"] /\n    monthly[\"frequency\"].replace(0, np.nan)\n).fillna(0)\n\nmonthly[\"month\"] = monthly[\"year_month\"].dt.month\n\nN = len(monthly)\n\n# =============================\n# Horizon\n# =============================\nsample_sub[\"year\"]  = sample_sub[\"id\"].str.split(\"_\").str[0]\nsample_sub[\"month\"] = sample_sub[\"id\"].str.split(\"_\").str[1]\nsample_sub[\"month_key\"] = sample_sub[\"year\"] + \"-\" + sample_sub[\"month\"]\n\nfuture_periods = (\n    pd.PeriodIndex(sample_sub[\"month_key\"], freq=\"M\")\n      .unique()\n      .sort_values()\n)\n\nH = len(future_periods)\n\n# =============================\n# Metrics\n# =============================\ndef mape_frac(y_true, y_pred):\n    y_true = np.asarray(y_true, float)\n    y_pred = np.asarray(y_pred, float)\n    mask = y_true != 0\n    if mask.sum() == 0:\n        return np.nan\n    return np.mean(np.abs((y_true[mask]-y_pred[mask])/y_true[mask]))\n\ndef avg_mape(yF,pF,yT,pT):\n    yS = yT/np.clip(yF,1.0,np.inf)\n    pS = pT/np.clip(pF,1.0,np.inf)\n    mf = mape_frac(yF,pF)\n    mt = mape_frac(yT,pT)\n    ms = mape_frac(yS,pS)\n    return np.nanmean([mf,mt,ms])\n\n# =============================\n# Season builder (TRAIN ONLY)\n# =============================\ndef build_season_log(series, months):\n    y = np.log1p(np.asarray(series,float))\n    base = np.median(y)\n    s = np.zeros(13)\n    for m in range(1,13):\n        vals = y[months==m]\n        if len(vals)>1:\n            s[m] = np.median(vals)-base\n        else:\n            s[m] = 0\n    return s\n\n# =============================\n# Helpers\n# =============================\ndef ets1(series_log):\n    if len(series_log)<4:\n        return series_log[-1]\n    try:\n        m = ExponentialSmoothing(\n            series_log,\n            trend=\"add\",\n            damped_trend=True,\n            seasonal=None,\n            initialization_method=\"estimated\"\n        ).fit()\n        return float(m.forecast(1)[0])\n    except:\n        return float(series_log[-1])\n\ndef gated_drift(series_log,k=3):\n    if len(series_log)<k+2:\n        return series_log[-1]\n    y = np.asarray(series_log,float)\n    d1 = np.mean(np.diff(y[-(k+1):]))\n    d2 = np.mean(np.diff(y[-(k+3):-2]))\n    if np.sign(d1)==np.sign(d2):\n        return y[-1]+d1\n    else:\n        return y[-1]\n\ndef anchor(series_log,k=3):\n    return float(np.median(series_log[-k:]))\n\n# =============================\n# Simulator\n# =============================\ndef simulate(train_df,H,P):\n\n    sim = train_df.copy().reset_index(drop=True)\n\n    # build season from TRAIN ONLY\n    S_R = build_season_log(sim[\"claim_rate\"].values,sim[\"month\"].values)\n    S_S = build_season_log(sim[\"severity\"].values,sim[\"month\"].values)\n\n    pF,pT = [],[]\n\n    for step in range(H):\n\n        next_period = sim[\"year_month\"].iloc[-1]+1\n        m_next = int(next_period.month)\n        expo_next = float(sim[\"exposure\"].iloc[-1])\n\n        logR = np.log1p(sim[\"claim_rate\"].values)\n        logS = np.log1p(sim[\"severity\"].values)\n\n        # deseason\n        logR_ds = logR - P[\"season_scale\"]*S_R[sim[\"month\"].values]\n        logS_ds = logS - P[\"season_scale\"]*S_S[sim[\"month\"].values]\n\n        # forecast\n        r_ets = ets1(logR_ds)\n        r_drift = gated_drift(logR_ds,k=P[\"drift_k\"])\n        r_anchor = anchor(logR_ds,k=P[\"k_anchor\"])\n\n        s_ets = ets1(logS_ds)\n        s_drift = gated_drift(logS_ds,k=P[\"drift_k\"])\n        s_anchor = anchor(logS_ds,k=P[\"k_anchor\"])\n\n        r_ds = (\n            P[\"w_ets\"]*r_ets +\n            P[\"w_drift\"]*r_drift +\n            (1-P[\"w_ets\"]-P[\"w_drift\"])*r_anchor\n        )\n\n        s_ds = (\n            P[\"w_ets\"]*s_ets +\n            P[\"w_drift\"]*s_drift +\n            (1-P[\"w_ets\"]-P[\"w_drift\"])*s_anchor\n        )\n\n        # reseason\n        logR_pred = r_ds + P[\"season_scale\"]*S_R[m_next]\n        logS_pred = s_ds + P[\"season_scale\"]*S_S[m_next]\n\n        rate_pred = max(np.expm1(logR_pred),1e-9)\n        sev_pred  = max(np.expm1(logS_pred),1e-9)\n\n        freq_pred = rate_pred*expo_next\n        total_pred = freq_pred*sev_pred\n\n        # clamp via quantile band\n        qF_low,qF_high = np.quantile(sim[\"frequency\"],[0.1,0.9])\n        qT_low,qT_high = np.quantile(sim[\"total_claim\"],[0.1,0.9])\n\n        freq_pred = float(np.clip(freq_pred,qF_low*0.7,qF_high*1.4))\n        total_pred = float(np.clip(total_pred,qT_low*0.7,qT_high*1.4))\n\n        pF.append(freq_pred)\n        pT.append(total_pred)\n\n        sim = pd.concat([sim,pd.DataFrame([{\n            \"year_month\":next_period,\n            \"month\":m_next,\n            \"exposure\":expo_next,\n            \"claim_rate\":rate_pred,\n            \"severity\":sev_pred,\n            \"frequency\":freq_pred,\n            \"total_claim\":total_pred\n        }])],ignore_index=True)\n\n    return np.array(pF),np.array(pT)\n\n# =============================\n# Rolling CV (3 splits)\n# =============================\nsplits = [N-H-2,N-H-1,N-H]\nsplits = [s for s in splits if s>6]\n\ndef run_cv(P):\n    scores=[]\n    for te in splits:\n        tr = monthly.iloc[:te]\n        va = monthly.iloc[te:te+H]\n        pF,pT = simulate(tr,H,P)\n        scores.append(avg_mape(\n            va[\"frequency\"].values,pF,\n            va[\"total_claim\"].values,pT\n        ))\n    return np.mean(scores)\n\n# =============================\n# Baseline Params\n# =============================\nP0 = dict(\n    w_ets=0.6,\n    w_drift=0.2,\n    k_anchor=4,\n    drift_k=3,\n    season_scale=0.5\n)\n\nbase = run_cv(P0)\nprint(\"Baseline CV:\",round(base*100,4))\n\n# =============================\n# Optuna\n# =============================\ndef objective(trial):\n    P=dict(\n        w_ets=trial.suggest_float(\"w_ets\",0.3,0.8),\n        w_drift=trial.suggest_float(\"w_drift\",0.0,0.4),\n        k_anchor=trial.suggest_int(\"k_anchor\",3,6),\n        drift_k=trial.suggest_int(\"drift_k\",2,5),\n        season_scale=trial.suggest_float(\"season_scale\",0.0,0.8)\n    )\n    if P[\"w_ets\"]+P[\"w_drift\"]>0.9:\n        return 1e9\n    return run_cv(P)\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective,n_trials=120,show_progress_bar=True)\n\nbestP = study.best_params\nbest_score = run_cv(bestP)\n\nprint(\"\\n==============================\")\nprint(\"Best Params:\",bestP)\nprint(\"CV Best % :\",round(best_score*100,4))\nprint(\"==============================\")\nprint(\"STAGE 4 v31 â€” READY (RATE+SEV, NO LEAK)\")","metadata":{"execution":{"iopub.status.busy":"2026-02-22T04:37:54.577936Z","iopub.execute_input":"2026-02-22T04:37:54.578262Z","iopub.status.idle":"2026-02-22T04:40:56.248328Z","shell.execute_reply.started":"2026-02-22T04:37:54.578233Z","shell.execute_reply":"2026-02-22T04:40:56.247458Z"},"papermill":{"duration":774.876643,"end_time":"2026-02-21T02:09:55.120512","exception":false,"start_time":"2026-02-21T01:57:00.243869","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"N months: 19 | Horizon H: 5 | Future MOY: [8, 9, 10, 11, 12]\nCV train_ends: [7, 8, 13, 14] | weights: [0.636, 0.277, 0.042, 0.045]\n  split te= 7 | valid months: ['2024-08', '2024-09', '2024-10', '2024-11', '2024-12']\n  split te= 8 | valid months: ['2024-09', '2024-10', '2024-11', '2024-12', '2025-01']\n  split te= 13 | valid months: ['2025-02', '2025-03', '2025-04', '2025-05', '2025-06']\n  split te= 14 | valid months: ['2025-03', '2025-04', '2025-05', '2025-06', '2025-07']\nBaseline CV %: 3.2595\nBaseline per-split (%):\n  te=7 | avg=1.588 | total=1.175 | freq=2.059 | sev=1.530\n  te=8 | avg=6.439 | total=9.479 | freq=4.847 | sev=4.992\n  te=13 | avg=6.772 | total=8.802 | freq=8.416 | sev=3.098\n  te=14 | avg=4.022 | total=2.204 | freq=5.524 | sev=4.338\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72475b25f0e740baaa6fe3028d50590e"}},"metadata":{}},{"name":"stdout","text":"\n==============================\nSplits: [7, 8, 13, 14] | weights: [0.636, 0.277, 0.042, 0.045] | overlap: [1.0, 0.8, 0.0, 0.0]\nBest Params: {'k_anchor': 4, 'anchor_f': 'mean', 'anchor_t': 'median', 'w_ets_f': 0.4140902012641963, 'w_drift_f': 0.23427959162969844, 'w_ets_t': 0.1253855830888168, 'w_drift_t': 0.0557421860395439, 'drift_k_f': 5, 'drift_k_t': 5, 'season_scale_f': 1.007823739519456, 'season_scale_t': 0.9473922819889735, 'capF_low': 0.8025030597351724, 'capF_high': 1.4449152050189749, 'capT_low': 0.5583495487333925, 'capT_high': 1.7549317144514707}\nCV Best %  : 2.7153\n==============================\nBest per-split (%):\n  te=7 | avg=1.006 | total=0.853 | freq=1.516 | sev=0.651\n  te=8 | avg=5.992 | total=9.223 | freq=4.088 | sev=4.666\n  te=13 | avg=5.574 | total=6.910 | freq=7.950 | sev=1.862\n  te=14 | avg=4.018 | total=2.471 | freq=4.810 | sev=4.773\n\nSTAGE 4 v30 â€” READY (global season prior)\n","output_type":"stream"}],"execution_count":6},{"id":"36a8c94f","cell_type":"markdown","source":"# TEST PREDICTION & KAGGLE SUBMISSION","metadata":{"papermill":{"duration":0.007895,"end_time":"2026-02-21T02:09:55.136003","exception":false,"start_time":"2026-02-21T02:09:55.128108","status":"completed"},"tags":[]}},{"id":"b7aae2b6","cell_type":"code","source":"# ============================================================\n# STAGE 5 v32 â€” FINAL SUBMISSION (Stage3 + Stage4 v31) ROBUST 2D BLEND\n# - Stage3: predict Frequency & Total directly (ETS log1p + anchor shrink)\n# - Stage4 v31: predict claim_rate & severity (season built TRAIN-ONLY), derive F/T\n# - Blend (regularized):\n#     F = (1-wF)*F3 + wF*F4\n#     T = (1-wT)*T3 + wT*T4\n#   + safety: clamp blended outputs to stay near Stage3/Stage4 corridor\n# - Objective: weighted mean + lambda_worst*worst + lambda_dev*deviation\n# ============================================================\n\n!pip install -q statsmodels\n\nimport numpy as np, pandas as pd\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nnp.random.seed(SEED)\n\nBASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\nsample_sub = pd.read_csv(BASE_PATH + \"sample_submission.csv\")\n\n# ------------------------------\n# 0) SAFETY CHECKS\n# ------------------------------\nassert \"monthly\" in globals(), \"monthly belum ada. Jalankan Stage 1.\"\nassert \"year_month\" in monthly.columns, \"monthly['year_month'] tidak ada.\"\nassert \"exposure\" in monthly.columns, \"monthly['exposure'] tidak ada. Stage 1 exposure wajib.\"\nassert \"frequency\" in monthly.columns and \"total_claim\" in monthly.columns, \"monthly freq/total wajib.\"\n\n# ------------------------------\n# 1) FUTURE PERIODS + H\n# ------------------------------\nsample_sub[\"year\"]  = sample_sub[\"id\"].str.split(\"_\").str[0]\nsample_sub[\"month\"] = sample_sub[\"id\"].str.split(\"_\").str[1]\nsample_sub[\"month_key\"] = sample_sub[\"year\"] + \"-\" + sample_sub[\"month\"]\n\nfuture_periods = pd.PeriodIndex(sample_sub[\"month_key\"], freq=\"M\").unique().sort_values()\nH0 = int(len(future_periods))\nfuture_moy = set([p.month for p in future_periods])\n\n# ------------------------------\n# 2) MONTHLY COMPLETE RANGE + CLEAN\n# ------------------------------\nm = monthly.copy().sort_values(\"year_month\").reset_index(drop=True)\n\nif not isinstance(m.loc[0, \"year_month\"], pd.Period):\n    m[\"year_month\"] = pd.PeriodIndex(m[\"year_month\"], freq=\"M\")\n\nmin_m = m[\"year_month\"].min()\nmax_m = m[\"year_month\"].max()\nall_months = pd.period_range(min_m, max_m, freq=\"M\")\n\nm = (\n    m.set_index(\"year_month\")\n     .reindex(all_months)\n     .rename_axis(\"year_month\")\n     .reset_index()\n)\n\n# keep exposure last-known forward fill (portfolio often stable)\nif \"exposure\" in m.columns:\n    m[\"exposure\"] = m[\"exposure\"].ffill().bfill()\n\nm[\"frequency\"]   = pd.to_numeric(m[\"frequency\"], errors=\"coerce\").fillna(0.0).clip(lower=1.0)\nm[\"total_claim\"] = pd.to_numeric(m[\"total_claim\"], errors=\"coerce\").fillna(0.0).clip(lower=1.0)\nm[\"severity\"]    = (m[\"total_claim\"] / m[\"frequency\"]).astype(float)\nm[\"month\"]       = m[\"year_month\"].dt.month\nm[\"t\"]           = np.arange(len(m), dtype=int)\n\n# rate & sev for Stage4 v31\nm[\"claim_rate\"] = (m[\"frequency\"] / m[\"exposure\"].replace(0, np.nan)).fillna(0.0)\nm[\"severity\"]   = (m[\"total_claim\"] / m[\"frequency\"].replace(0, np.nan)).fillna(0.0)\n\nN = len(m)\nH = min(H0, max(1, N - 10))\nprint(\"N months:\", N, \"| H:\", H, \"| Future MOY:\", sorted(list(future_moy)))\n\n# ------------------------------\n# 3) METRICS\n# ------------------------------\ndef mape_frac(y_true, y_pred):\n    y_true = np.asarray(y_true, float)\n    y_pred = np.asarray(y_pred, float)\n    mask = np.isfinite(y_true) & np.isfinite(y_pred) & (y_true != 0)\n    if mask.sum() == 0:\n        return np.nan\n    return float(np.mean(np.abs((y_true[mask]-y_pred[mask]) / y_true[mask])))\n\ndef avg_mape(yF, pF, yT, pT):\n    yF = np.asarray(yF, float); pF = np.asarray(pF, float)\n    yT = np.asarray(yT, float); pT = np.asarray(pT, float)\n    yS = yT / np.clip(yF, 1.0, np.inf)\n    pS = pT / np.clip(pF, 1.0, np.inf)\n    mf = mape_frac(yF, pF)\n    mt = mape_frac(yT, pT)\n    ms = mape_frac(yS, pS)\n    return float(np.nanmean([mf, mt, ms])), mt, mf, ms\n\n# ------------------------------\n# 4) STAGE 3 SIMULATOR (FROM YOUR STAGE 3)\n# ------------------------------\n# Use your tuned S3 (or keep these defaults)\nS3 = dict(\n    wt_total=0.85, anchor_total=\"median\",\n    wt_freq=0.20,  anchor_freq=\"mean\",\n    k_anchor=3\n)\n\ndef anchor_level_series(x: pd.Series, k: int, how: str):\n    tail = np.asarray(x.tail(k), dtype=float)\n    return float(np.median(tail)) if how == \"median\" else float(np.mean(tail))\n\ndef ets_1step_log1p_series(level_series: pd.Series, trend=\"add\", damped=True):\n    y = np.log1p(level_series.astype(float).clip(lower=1e-12))\n    if len(y) < 4:\n        return float(np.expm1(y.iloc[-1]))\n    if trend is not None and len(y) < 10:\n        trend = None\n        damped = False\n    try:\n        mdl = ExponentialSmoothing(\n            y, trend=trend,\n            damped_trend=(damped if trend is not None else False),\n            seasonal=None,\n            initialization_method=\"estimated\"\n        ).fit()\n        return float(np.expm1(mdl.forecast(1).iloc[0]))\n    except:\n        return float(level_series.iloc[-1])\n\ndef simulate_stage3(train_df: pd.DataFrame, H: int):\n    sim = train_df.copy().reset_index(drop=True)\n    pF, pT = [], []\n    for _ in range(H):\n        k = int(S3[\"k_anchor\"])\n\n        tot_fc = ets_1step_log1p_series(sim[\"total_claim\"], trend=\"add\", damped=True)\n        tot_anchor = anchor_level_series(sim[\"total_claim\"], k, S3[\"anchor_total\"])\n        tot_pred = float(S3[\"wt_total\"])*tot_fc + (1-float(S3[\"wt_total\"]))*tot_anchor\n        tot_pred = max(1.0, tot_pred)\n\n        fre_fc = ets_1step_log1p_series(sim[\"frequency\"], trend=\"add\", damped=True)\n        fre_anchor = anchor_level_series(sim[\"frequency\"], k, S3[\"anchor_freq\"])\n        fre_pred = float(S3[\"wt_freq\"])*fre_fc + (1-float(S3[\"wt_freq\"]))*fre_anchor\n        fre_pred = max(1.0, fre_pred)\n\n        pF.append(fre_pred)\n        pT.append(tot_pred)\n\n        sim = pd.concat([sim, pd.DataFrame([{\n            \"year_month\": sim[\"year_month\"].iloc[-1] + 1,\n            \"month\": int((sim[\"year_month\"].iloc[-1] + 1).month),\n            \"t\": int(sim[\"t\"].iloc[-1] + 1),\n            \"exposure\": float(sim[\"exposure\"].iloc[-1]),\n            \"frequency\": fre_pred,\n            \"total_claim\": tot_pred,\n            \"claim_rate\": float(fre_pred / max(1.0, float(sim[\"exposure\"].iloc[-1]))),\n            \"severity\": float(tot_pred / max(1.0, fre_pred)),\n        }])], ignore_index=True)\n\n    return np.array(pF, float), np.array(pT, float)\n\n# ------------------------------\n# 5) STAGE 4 v31 SIMULATOR (RATE+SEV TRAIN-ONLY SEASON)\n# ------------------------------\n# Use bestP from your Stage4 v31 output. If not exists, fallback baseline.\nif \"bestP\" in globals():\n    P4 = dict(bestP)\nelse:\n    P4 = dict(w_ets=0.6, w_drift=0.2, k_anchor=4, drift_k=3, season_scale=0.5)\n\ndef build_season_log(series, months):\n    y = np.log1p(np.asarray(series, float).clip(min=0))\n    base = float(np.median(y))\n    s = np.zeros(13, dtype=float)\n    for mm in range(1, 13):\n        vals = y[np.asarray(months, int) == mm]\n        s[mm] = float(np.median(vals) - base) if len(vals) > 1 else 0.0\n    return s\n\ndef ets1(series_log):\n    y = np.asarray(series_log, float)\n    if len(y) < 4:\n        return float(y[-1])\n    try:\n        mdl = ExponentialSmoothing(\n            y,\n            trend=\"add\",\n            damped_trend=True,\n            seasonal=None,\n            initialization_method=\"estimated\"\n        ).fit()\n        return float(mdl.forecast(1)[0])\n    except:\n        return float(y[-1])\n\ndef gated_drift(series_log, k=3):\n    y = np.asarray(series_log, float)\n    if len(y) < (k + 2):\n        return float(y[-1])\n    d1 = float(np.mean(np.diff(y[-(k+1):])))\n    d2 = float(np.mean(np.diff(y[-(k+3):-2]))) if len(y) >= (k+4) else d1\n    return float(y[-1] + d1) if np.sign(d1) == np.sign(d2) else float(y[-1])\n\ndef anchor_log(series_log, k=3):\n    y = np.asarray(series_log, float)\n    return float(np.median(y[-k:]))\n\ndef simulate_stage4v31(train_df: pd.DataFrame, H: int):\n    sim = train_df.copy().reset_index(drop=True)\n\n    # season from TRAIN ONLY\n    SR = build_season_log(sim[\"claim_rate\"].values, sim[\"month\"].values)\n    SS = build_season_log(sim[\"severity\"].values, sim[\"month\"].values)\n\n    pF, pT = [], []\n    for _ in range(H):\n        next_period = sim[\"year_month\"].iloc[-1] + 1\n        m_next = int(next_period.month)\n\n        expo_next = float(sim[\"exposure\"].iloc[-1])\n        expo_next = max(1.0, expo_next)\n\n        logR = np.log1p(sim[\"claim_rate\"].values.clip(min=0))\n        logS = np.log1p(sim[\"severity\"].values.clip(min=0))\n\n        # deseason\n        sc = float(P4.get(\"season_scale\", 0.5))\n        logR_ds = logR - sc * SR[sim[\"month\"].values]\n        logS_ds = logS - sc * SS[sim[\"month\"].values]\n\n        # components\n        r_ets = ets1(logR_ds)\n        r_dft = gated_drift(logR_ds, k=int(P4.get(\"drift_k\", 3)))\n        r_an  = anchor_log(logR_ds, k=int(P4.get(\"k_anchor\", 4)))\n\n        s_ets = ets1(logS_ds)\n        s_dft = gated_drift(logS_ds, k=int(P4.get(\"drift_k\", 3)))\n        s_an  = anchor_log(logS_ds, k=int(P4.get(\"k_anchor\", 4)))\n\n        wE = float(P4.get(\"w_ets\", 0.6))\n        wD = float(P4.get(\"w_drift\", 0.2))\n        if (wE + wD) > 0.9:\n            # keep convex\n            wD = max(0.0, 0.9 - wE)\n\n        r_ds = wE*r_ets + wD*r_dft + (1.0-wE-wD)*r_an\n        s_ds = wE*s_ets + wD*s_dft + (1.0-wE-wD)*s_an\n\n        # reseason\n        logR_pred = float(r_ds + sc * SR[m_next])\n        logS_pred = float(s_ds + sc * SS[m_next])\n\n        rate_pred = float(max(np.expm1(logR_pred), 1e-9))\n        sev_pred  = float(max(np.expm1(logS_pred), 1e-9))\n\n        F_pred = float(rate_pred * expo_next)\n        T_pred = float(F_pred * sev_pred)\n\n        # light clamp vs training quantiles (anti spike)\n        qF_low, qF_high = np.quantile(sim[\"frequency\"].values, [0.10, 0.90])\n        qT_low, qT_high = np.quantile(sim[\"total_claim\"].values, [0.10, 0.90])\n\n        F_pred = float(np.clip(F_pred, max(1.0, qF_low*0.70), qF_high*1.40))\n        T_pred = float(np.clip(T_pred, max(1.0, qT_low*0.70), qT_high*1.40))\n\n        pF.append(F_pred)\n        pT.append(T_pred)\n\n        sim = pd.concat([sim, pd.DataFrame([{\n            \"year_month\": next_period,\n            \"month\": m_next,\n            \"t\": int(sim[\"t\"].iloc[-1] + 1),\n            \"exposure\": expo_next,\n            \"claim_rate\": rate_pred,\n            \"severity\": sev_pred,\n            \"frequency\": F_pred,\n            \"total_claim\": T_pred,\n        }])], ignore_index=True)\n\n    return np.array(pF, float), np.array(pT, float)\n\n# ------------------------------\n# 6) CV SPLITS (ROBUST, NO FIXED HAND-PICK)\n# ------------------------------\n# Rolling splits near the end + overlap-aware weighting\nmin_train = 7\ncand_te = list(range(min_train, N - H + 1))\n# Keep only last ~6 splits to be \"Kaggle-like\"\ncand_te = cand_te[-6:] if len(cand_te) > 6 else cand_te\n\ndef overlap_ratio(te):\n    v = m.iloc[te:te+H]\n    return sum([1 for mm in v[\"month\"].tolist() if mm in future_moy]) / float(H)\n\novs = np.array([overlap_ratio(te) for te in cand_te], float)\nrcs = np.array([te/float(N) for te in cand_te], float)\n\n# weights: emphasize overlap + some recency\nOVERLAP_POWER = 3.0\nRECENCY_MIX = 0.20\nw_raw = (ovs ** OVERLAP_POWER) + RECENCY_MIX * rcs\nw_raw = np.maximum(w_raw, 1e-6)\nsplit_w = w_raw / w_raw.sum()\n\ntrain_ends = cand_te\nprint(\"CV train_ends:\", train_ends)\nprint(\"overlap:\", ovs.round(3).tolist())\nprint(\"weights:\", split_w.round(3).tolist())\n\n# ------------------------------\n# 7) BLEND SEARCH (REGULARIZED)\n# ------------------------------\ngrid = np.linspace(0.0, 1.0, 21)  # step 0.05\n\n# penalties to prevent LB collapse\nLAMBDA_WORST = 0.35\nLAMBDA_DEV   = 0.10\n\ndef corridor_clip(x, a, b, lo=0.85, hi=1.15):\n    # keep within both model corridors:\n    # - near Stage3: [lo*a, hi*a]\n    # - near Stage4: [lo*b, hi*b]\n    low = np.minimum(lo*a, lo*b)\n    high = np.maximum(hi*a, hi*b)\n    return np.clip(x, low, high)\n\nbest = None\nrows = []\n\nfor wF in grid:\n    for wT in grid:\n        split_scores = []\n        dev_scores = []\n        for te, w in zip(train_ends, split_w):\n            tr = m.iloc[:te].copy().reset_index(drop=True)\n            va = m.iloc[te:te+H].copy().reset_index(drop=True)\n\n            F3, T3 = simulate_stage3(tr, H)\n            F4, T4 = simulate_stage4v31(tr, H)\n\n            F = (1-wF)*F3 + wF*F4\n            T = (1-wT)*T3 + wT*T4\n\n            # safety corridor clip (anti out-of-distribution)\n            F = corridor_clip(F, F3, F4, lo=0.88, hi=1.12)\n            T = corridor_clip(T, T3, T4, lo=0.88, hi=1.12)\n\n            sc, mt, mf, ms = avg_mape(va[\"frequency\"].values, F, va[\"total_claim\"].values, T)\n            split_scores.append(sc)\n\n            # deviation penalty from Stage3 (keep anchor stability)\n            dev = float(np.mean(np.abs(F/F3 - 1.0)) + np.mean(np.abs(T/T3 - 1.0)))\n            dev_scores.append(dev)\n\n        meanw = float(np.sum(split_w * np.array(split_scores)))\n        worst = float(np.max(split_scores))\n        devw  = float(np.sum(split_w * np.array(dev_scores)))\n\n        obj = meanw + LAMBDA_WORST*worst + LAMBDA_DEV*devw\n\n        rows.append([wF, wT, obj, meanw, worst, devw] + split_scores)\n\n        cand = (obj, meanw, worst, devw, wF, wT)\n        if (best is None) or (cand < best):\n            best = cand\n\nobj_best, mean_best, worst_best, dev_best, wF_best, wT_best = best\n\nprint(\"\\nBest blend (REG):\")\nprint(\"  wF_best =\", wF_best, \"| wT_best =\", wT_best)\nprint(\"  obj     =\", round(obj_best*100, 4), \"%\")\nprint(\"  mean    =\", round(mean_best*100, 4), \"% | worst =\", round(worst_best*100, 4), \"% | dev =\", round(dev_best, 4))\n\ndfb = pd.DataFrame(rows, columns=[\"wF\",\"wT\",\"obj\",\"mean_avg\",\"worst_avg\",\"dev\"] + [f\"split_{i}\" for i in range(len(train_ends))])\nprint(\"\\nTop 10 by obj:\")\nprint(dfb.sort_values([\"obj\",\"worst_avg\",\"dev\"]).head(10))\n\n# ------------------------------\n# 8) FINAL FORECAST ON FULL DATA\n# ------------------------------\nF3, T3 = simulate_stage3(m, H)\nF4, T4 = simulate_stage4v31(m, H)\n\nF = (1-wF_best)*F3 + wF_best*F4\nT = (1-wT_best)*T3 + wT_best*T4\n\n# final safety corridor (important)\nF = corridor_clip(F, F3, F4, lo=0.88, hi=1.12)\nT = corridor_clip(T, T3, T4, lo=0.88, hi=1.12)\n\nS = T / np.clip(F, 1.0, np.inf)\n\npred_map = {}\npreview = []\nfor i, period in enumerate(future_periods):\n    key = f\"{period.year}_{str(period.month).zfill(2)}\"\n    pred_map[f\"{key}_Claim_Frequency\"] = float(F[i])\n    pred_map[f\"{key}_Total_Claim\"]     = float(T[i])\n    pred_map[f\"{key}_Claim_Severity\"]  = float(S[i])\n    preview.append([str(period), float(F[i]), float(T[i]), float(S[i])])\n\nsub = sample_sub.copy()\nsub[\"value\"] = sub[\"id\"].map(pred_map)\nmissing = int(sub[\"value\"].isna().sum())\nprint(\"\\nNaN in submission:\", missing)\nassert missing == 0, \"Ada id belum terisi. Cek format key.\"\n\nsub = sub[[\"id\",\"value\"]]\nsub.to_csv(\"submission.csv\", index=False)\n\nprint(\"\\nPreview future predictions:\")\nprint(pd.DataFrame(preview, columns=[\"period\",\"pred_freq\",\"pred_total\",\"pred_sev\"]))\nprint(\"\\nSaved: submission.csv\")\nprint(sub.head(12))","metadata":{"execution":{"iopub.status.busy":"2026-02-22T04:40:56.250483Z","iopub.execute_input":"2026-02-22T04:40:56.250825Z","iopub.status.idle":"2026-02-22T04:53:44.224282Z","shell.execute_reply.started":"2026-02-22T04:40:56.250790Z","shell.execute_reply":"2026-02-22T04:53:44.223053Z"},"papermill":{"duration":25.964382,"end_time":"2026-02-21T02:10:21.107828","exception":false,"start_time":"2026-02-21T02:09:55.143446","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"N months: 19 | H: 5 | Future MOY: [8, 9, 10, 11, 12]\nBlend train_ends: [7, 8, 13, 14] | weights: [0.636, 0.277, 0.042, 0.045]\n\nBest blend (2D):\n  wF_best = 1.0 (Stage4 weight for Frequency)\n  wT_best = 1.0 (Stage4 weight for Total)\n  CV mean = 2.7148 % | worst = 5.9924 %\n\nTop 10 by mean_avg:\n       wF    wT  mean_avg  worst_avg   split_0   split_1   split_2   split_3\n440  1.00  1.00  0.027148   0.059924  0.010063  0.059924  0.055742  0.040181\n419  0.95  1.00  0.027608   0.060917  0.010501  0.060917  0.053238  0.040423\n439  1.00  0.95  0.027650   0.060254  0.010683  0.060254  0.056984  0.039366\n418  0.95  0.95  0.028299   0.060834  0.011568  0.060834  0.054961  0.039608\n398  0.90  1.00  0.029244   0.062815  0.012304  0.062815  0.052115  0.040664\n417  0.95  0.90  0.029478   0.061349  0.013117  0.061349  0.057053  0.038792\n438  1.00  0.90  0.029507   0.061363  0.013037  0.061363  0.059136  0.038551\n397  0.90  0.95  0.029595   0.062734  0.012836  0.062734  0.053834  0.039848\n396  0.90  0.90  0.030501   0.062652  0.014240  0.062652  0.055554  0.039033\n416  0.95  0.85  0.031271   0.062458  0.015368  0.062458  0.059198  0.037987\nNaN in submission: 0\n\nPreview future predictions:\n    period   pred_freq    pred_total      pred_sev\n0  2025-08  223.214649  1.323452e+10  5.929055e+07\n1  2025-09  204.182668  1.211557e+10  5.933691e+07\n2  2025-10  270.301522  1.244697e+10  4.604846e+07\n3  2025-11  265.566038  1.339265e+10  5.043058e+07\n4  2025-12  232.367450  1.179187e+10  5.074664e+07\n\nSaved: submission.csv\n                         id         value\n0   2025_08_Claim_Frequency  2.232146e+02\n1    2025_08_Claim_Severity  5.929055e+07\n2       2025_08_Total_Claim  1.323452e+10\n3   2025_09_Claim_Frequency  2.041827e+02\n4    2025_09_Claim_Severity  5.933691e+07\n5       2025_09_Total_Claim  1.211557e+10\n6   2025_10_Claim_Frequency  2.703015e+02\n7    2025_10_Claim_Severity  4.604846e+07\n8       2025_10_Total_Claim  1.244697e+10\n9   2025_11_Claim_Frequency  2.655660e+02\n10   2025_11_Claim_Severity  5.043058e+07\n11      2025_11_Total_Claim  1.339265e+10\n","output_type":"stream"}],"execution_count":7},{"id":"b183e519","cell_type":"code","source":"print(sub.head(12)) ##  ini yang stage 4 diganti","metadata":{"execution":{"iopub.status.busy":"2026-02-22T04:53:44.225622Z","iopub.execute_input":"2026-02-22T04:53:44.225990Z","iopub.status.idle":"2026-02-22T04:53:44.236834Z","shell.execute_reply.started":"2026-02-22T04:53:44.225953Z","shell.execute_reply":"2026-02-22T04:53:44.234550Z"},"papermill":{"duration":0.026144,"end_time":"2026-02-21T02:10:21.142255","exception":false,"start_time":"2026-02-21T02:10:21.116111","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"                         id         value\n0   2025_08_Claim_Frequency  2.232146e+02\n1    2025_08_Claim_Severity  5.929055e+07\n2       2025_08_Total_Claim  1.323452e+10\n3   2025_09_Claim_Frequency  2.041827e+02\n4    2025_09_Claim_Severity  5.933691e+07\n5       2025_09_Total_Claim  1.211557e+10\n6   2025_10_Claim_Frequency  2.703015e+02\n7    2025_10_Claim_Severity  4.604846e+07\n8       2025_10_Total_Claim  1.244697e+10\n9   2025_11_Claim_Frequency  2.655660e+02\n10   2025_11_Claim_Severity  5.043058e+07\n11      2025_11_Total_Claim  1.339265e+10\n","output_type":"stream"}],"execution_count":8},{"id":"2c87957b-b241-4508-9b6f-ed0c13200431","cell_type":"code","source":"# ============================================================\n# DIAG CELL â€” build 3 submissions:\n# 1) sub_stage3.csv\n# 2) sub_stage4v30.csv\n# 3) sub_blend_safe.csv (regularized blend, won't drift too far from Stage3)\n# ============================================================\n\nimport numpy as np, pandas as pd\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nBASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\nsample_sub = pd.read_csv(BASE_PATH + \"sample_submission.csv\")\n\nassert \"df\" in globals()\nassert \"year_month\" in df.columns\n\n# ---- future periods ----\nsample_sub[\"year\"]  = sample_sub[\"id\"].str.split(\"_\").str[0]\nsample_sub[\"month\"] = sample_sub[\"id\"].str.split(\"_\").str[1]\nsample_sub[\"month_key\"] = sample_sub[\"year\"] + \"-\" + sample_sub[\"month\"]\nfuture_periods = pd.PeriodIndex(sample_sub[\"month_key\"], freq=\"M\").unique().sort_values()\nH = int(len(future_periods))\n\n# ---- monthly portfolio ----\nmonthly = (\n    df.groupby(\"year_month\")\n      .agg(frequency=(\"claim_id\",\"count\"),\n           total_claim=(\"nominal_klaim_yang_disetujui\",\"sum\"))\n      .reset_index()\n      .sort_values(\"year_month\")\n      .reset_index(drop=True)\n)\nif not isinstance(monthly.loc[0,\"year_month\"], pd.Period):\n    monthly[\"year_month\"] = pd.PeriodIndex(monthly[\"year_month\"], freq=\"M\")\n\nmin_m = monthly[\"year_month\"].min()\nmax_m = monthly[\"year_month\"].max()\nall_months = pd.period_range(min_m, max_m, freq=\"M\")\n\nmonthly = (monthly.set_index(\"year_month\")\n                 .reindex(all_months, fill_value=0.0)\n                 .rename_axis(\"year_month\")\n                 .reset_index())\nmonthly[\"frequency\"]   = pd.to_numeric(monthly[\"frequency\"], errors=\"coerce\").fillna(0.0).clip(lower=1.0)\nmonthly[\"total_claim\"] = pd.to_numeric(monthly[\"total_claim\"], errors=\"coerce\").fillna(0.0).clip(lower=1.0)\nmonthly[\"month\"] = monthly[\"year_month\"].dt.month\nmonthly[\"t\"] = np.arange(len(monthly), dtype=int)\n\n# ============================================================\n# Stage 3 simulator (as used before)\n# ============================================================\nS3 = dict(wt_total=0.85, anchor_total=\"median\", wt_freq=0.20, anchor_freq=\"mean\", k_anchor=3)\n\ndef anchor_level(x: pd.Series, k: int, how: str):\n    tail = np.asarray(x.tail(k), dtype=float)\n    return float(np.median(tail)) if how == \"median\" else float(np.mean(tail))\n\ndef ets_1step_log1p(level_series: pd.Series, trend=\"add\", damped=True):\n    y = np.log1p(level_series.astype(float).clip(lower=1e-12))\n    if len(y) < 4:\n        return float(np.expm1(y.iloc[-1]))\n    if trend is not None and len(y) < 10:\n        trend = None\n        damped = False\n    try:\n        m = ExponentialSmoothing(\n            y, trend=trend,\n            damped_trend=(damped if trend is not None else False),\n            seasonal=None\n        ).fit()\n        return float(np.expm1(m.forecast(1).iloc[0]))\n    except:\n        return float(level_series.iloc[-1])\n\ndef simulate_stage3(train_df: pd.DataFrame, H: int):\n    sim = train_df.copy().reset_index(drop=True)\n    pF, pT = [], []\n    for _ in range(H):\n        k = int(S3[\"k_anchor\"])\n        tot_fc = ets_1step_log1p(sim[\"total_claim\"], trend=\"add\", damped=True)\n        tot_anchor = anchor_level(sim[\"total_claim\"], k, S3[\"anchor_total\"])\n        tot_pred = float(S3[\"wt_total\"])*tot_fc + (1-float(S3[\"wt_total\"]))*tot_anchor\n        tot_pred = max(1.0, tot_pred)\n\n        fre_fc = ets_1step_log1p(sim[\"frequency\"], trend=\"add\", damped=True)\n        fre_anchor = anchor_level(sim[\"frequency\"], k, S3[\"anchor_freq\"])\n        fre_pred = float(S3[\"wt_freq\"])*fre_fc + (1-float(S3[\"wt_freq\"]))*fre_anchor\n        fre_pred = max(1.0, fre_pred)\n\n        pF.append(fre_pred); pT.append(tot_pred)\n        sim = pd.concat([sim, pd.DataFrame([{\n            \"year_month\": sim[\"year_month\"].iloc[-1] + 1,\n            \"month\": int((sim[\"year_month\"].iloc[-1] + 1).month),\n            \"t\": int(sim[\"t\"].iloc[-1] + 1),\n            \"frequency\": fre_pred,\n            \"total_claim\": tot_pred\n        }])], ignore_index=True)\n    return np.array(pF,float), np.array(pT,float)\n\n# ============================================================\n# Stage 4 v30 simulator (paste BEST params kamu)\n# ============================================================\nBEST4 = {\n    'k_anchor': 4, 'anchor_f': 'mean', 'anchor_t': 'median',\n    'w_ets_f': 0.4140902012641963, 'w_drift_f': 0.23427959162969844,\n    'w_ets_t': 0.1253855830888168, 'w_drift_t': 0.0557421860395439,\n    'drift_k_f': 5, 'drift_k_t': 5,\n    'season_scale_f': 1.007823739519456, 'season_scale_t': 0.9473922819889735,\n    'capF_low': 0.8025030597351724, 'capF_high': 1.4449152050189749,\n    'capT_low': 0.5583495487333925, 'capT_high': 1.7549317144514707\n}\n\ndef build_season_log(series_level, months, clip_abs=0.30):\n    y = np.log1p(np.asarray(series_level, float).clip(min=0))\n    m = np.asarray(months, int)\n    base = float(np.median(y))\n    s = np.zeros(13, dtype=float)\n    for mm in range(1, 13):\n        vals = y[m == mm]\n        s[mm] = float(np.median(vals) - base) if len(vals) else 0.0\n    return np.clip(s, -float(clip_abs), float(clip_abs))\n\nSEASON_LOG_F_FULL = build_season_log(monthly[\"frequency\"].values, monthly[\"month\"].values, clip_abs=0.30)\nSEASON_LOG_T_FULL = build_season_log(monthly[\"total_claim\"].values, monthly[\"month\"].values, clip_abs=0.35)\n\ndef ets_1step_on_log(series_log):\n    y = np.asarray(series_log, float)\n    if len(y) < 4:\n        return float(y[-1])\n    trend = \"add\" if len(y) >= 10 else None\n    damped = True if trend is not None else False\n    try:\n        m = ExponentialSmoothing(\n            y, trend=trend,\n            damped_trend=damped if trend is not None else False,\n            seasonal=None,\n            initialization_method=\"estimated\"\n        ).fit()\n        return float(m.forecast(1)[0])\n    except:\n        return float(y[-1])\n\ndef drift_on_log(series_log, k=3):\n    y = np.asarray(series_log, float)\n    if len(y) < (k + 2):\n        return float(y[-1])\n    deltas = np.diff(y[-(k+1):])\n    return float(y[-1] + np.mean(deltas))\n\ndef anchor_on_log(series_log, k=3, how=\"mean\"):\n    tail = np.asarray(series_log[-k:], float)\n    return float(np.median(tail)) if how == \"median\" else float(np.mean(tail))\n\ndef simulate_stage4v30(train_df: pd.DataFrame, H: int):\n    P = BEST4\n    sim = train_df.copy().reset_index(drop=True)\n    sim[\"logF\"] = np.log1p(sim[\"frequency\"].values)\n    sim[\"logT\"] = np.log1p(sim[\"total_claim\"].values)\n    sf = float(P[\"season_scale_f\"]); st = float(P[\"season_scale_t\"])\n\n    pF, pT = [], []\n    for _ in range(H):\n        next_period = sim[\"year_month\"].iloc[-1] + 1\n        m_next = int(next_period.month)\n\n        logF_ds = sim[\"logF\"].values - sf * SEASON_LOG_F_FULL[sim[\"month\"].values]\n        logT_ds = sim[\"logT\"].values - st * SEASON_LOG_T_FULL[sim[\"month\"].values]\n\n        f_ets = ets_1step_on_log(logF_ds)\n        f_drift = drift_on_log(logF_ds, k=int(P[\"drift_k_f\"]))\n        f_an = anchor_on_log(logF_ds, k=int(P[\"k_anchor\"]), how=P[\"anchor_f\"])\n        f_ds_pred = float(P[\"w_ets_f\"])*f_ets + float(P[\"w_drift_f\"])*f_drift + (1-float(P[\"w_ets_f\"])-float(P[\"w_drift_f\"]))*f_an\n\n        t_ets = ets_1step_on_log(logT_ds)\n        t_drift = drift_on_log(logT_ds, k=int(P[\"drift_k_t\"]))\n        t_an = anchor_on_log(logT_ds, k=int(P[\"k_anchor\"]), how=P[\"anchor_t\"])\n        t_ds_pred = float(P[\"w_ets_t\"])*t_ets + float(P[\"w_drift_t\"])*t_drift + (1-float(P[\"w_ets_t\"])-float(P[\"w_drift_t\"]))*t_an\n\n        logF_pred = float(f_ds_pred + sf * SEASON_LOG_F_FULL[m_next])\n        logT_pred = float(t_ds_pred + st * SEASON_LOG_T_FULL[m_next])\n\n        F_pred = float(np.expm1(logF_pred))\n        T_pred = float(np.expm1(logT_pred))\n\n        # clamp vs anchor\n        F_an = float(np.expm1(float(f_an + sf*SEASON_LOG_F_FULL[m_next])))\n        T_an = float(np.expm1(float(t_an + st*SEASON_LOG_T_FULL[m_next])))\n\n        F_pred = float(np.clip(F_pred, max(1.0, F_an*float(P[\"capF_low\"])), F_an*float(P[\"capF_high\"])))\n        T_pred = float(np.clip(T_pred, max(1.0, T_an*float(P[\"capT_low\"])), T_an*float(P[\"capT_high\"])))\n\n        pF.append(F_pred); pT.append(T_pred)\n\n        sim = pd.concat([sim, pd.DataFrame([{\n            \"year_month\": next_period,\n            \"month\": m_next,\n            \"t\": int(sim[\"t\"].iloc[-1] + 1),\n            \"frequency\": F_pred,\n            \"total_claim\": T_pred,\n            \"logF\": float(np.log1p(F_pred)),\n            \"logT\": float(np.log1p(T_pred)),\n        }])], ignore_index=True)\n\n    return np.array(pF,float), np.array(pT,float)\n\n# ============================================================\n# Build three forecasts on FULL data\n# ============================================================\nF3, T3 = simulate_stage3(monthly, H)\nF4, T4 = simulate_stage4v30(monthly, H)\n\n# 1) Stage3 only\nF_A, T_A = F3, T3\nS_A = T_A / np.clip(F_A, 1.0, np.inf)\n\n# 2) Stage4v30 only\nF_B, T_B = F4, T4\nS_B = T_B / np.clip(F_B, 1.0, np.inf)\n\n# 3) Safe blend: start from stage4 but don't deviate too far from stage3\n#    (clip blended ratio to stage3 within +-10% by default)\nwF, wT = 1.0, 1.0  # start fully stage4\nF_C = (1-wF)*F3 + wF*F4\nT_C = (1-wT)*T3 + wT*T4\n\n# clip toward stage3 (safety net)\nF_C = np.clip(F_C, F3*0.90, F3*1.10)\nT_C = np.clip(T_C, T3*0.90, T3*1.10)\nS_C = T_C / np.clip(F_C, 1.0, np.inf)\n\ndef make_sub(F, T, S, out_name):\n    pred_map = {}\n    for i, period in enumerate(future_periods):\n        key = f\"{period.year}_{str(period.month).zfill(2)}\"\n        pred_map[f\"{key}_Claim_Frequency\"] = float(F[i])\n        pred_map[f\"{key}_Total_Claim\"]     = float(T[i])\n        pred_map[f\"{key}_Claim_Severity\"]  = float(S[i])\n\n    sub = sample_sub.copy()\n    sub[\"value\"] = sub[\"id\"].map(pred_map)\n    assert int(sub[\"value\"].isna().sum()) == 0\n    sub = sub[[\"id\",\"value\"]]\n    sub.to_csv(out_name, index=False)\n    print(\"Saved:\", out_name)\n    print(sub.head(6))\n\nmake_sub(F_A, T_A, S_A, \"sub_stage3.csv\")\nmake_sub(F_B, T_B, S_B, \"sub_stage4v30.csv\")\nmake_sub(F_C, T_C, S_C, \"sub_blend_safe.csv\")\n\nprint(\"\\nQuick preview (period, F3,F4,F_safe):\")\nprint(pd.DataFrame({\n    \"period\": [str(p) for p in future_periods],\n    \"F3\": F3, \"F4\": F4, \"F_safe\": F_C,\n    \"T3\": T3, \"T4\": T4, \"T_safe\": T_C\n}))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T05:09:27.710627Z","iopub.execute_input":"2026-02-22T05:09:27.711286Z","iopub.status.idle":"2026-02-22T05:09:28.306804Z","shell.execute_reply.started":"2026-02-22T05:09:27.711254Z","shell.execute_reply":"2026-02-22T05:09:28.305900Z"}},"outputs":[{"name":"stdout","text":"Saved: sub_stage3.csv\n                        id         value\n0  2025_08_Claim_Frequency  2.435652e+02\n1   2025_08_Claim_Severity  5.161099e+07\n2      2025_08_Total_Claim  1.257064e+10\n3  2025_09_Claim_Frequency  2.449253e+02\n4   2025_09_Claim_Severity  5.133448e+07\n5      2025_09_Total_Claim  1.257311e+10\nSaved: sub_stage4v30.csv\n                        id         value\n0  2025_08_Claim_Frequency  2.232146e+02\n1   2025_08_Claim_Severity  5.929055e+07\n2      2025_08_Total_Claim  1.323452e+10\n3  2025_09_Claim_Frequency  2.041827e+02\n4   2025_09_Claim_Severity  5.933691e+07\n5      2025_09_Total_Claim  1.211557e+10\nSaved: sub_blend_safe.csv\n                        id         value\n0  2025_08_Claim_Frequency  2.232146e+02\n1   2025_08_Claim_Severity  5.929055e+07\n2      2025_08_Total_Claim  1.323452e+10\n3  2025_09_Claim_Frequency  2.204327e+02\n4   2025_09_Claim_Severity  5.496266e+07\n5      2025_09_Total_Claim  1.211557e+10\n\nQuick preview (period, F3,F4,F_safe):\n    period          F3          F4      F_safe            T3            T4  \\\n0  2025-08  243.565185  223.214649  223.214649  1.257064e+10  1.323452e+10   \n1  2025-09  244.925261  204.182668  220.432735  1.257311e+10  1.211557e+10   \n2  2025-10  247.950462  270.301522  270.301522  1.245642e+10  1.244697e+10   \n3  2025-11  243.827599  265.566038  265.566038  1.245245e+10  1.339265e+10   \n4  2025-12  243.980696  232.367450  232.367450  1.243264e+10  1.179187e+10   \n\n         T_safe  \n0  1.323452e+10  \n1  1.211557e+10  \n2  1.244697e+10  \n3  1.339265e+10  \n4  1.179187e+10  \n","output_type":"stream"}],"execution_count":10}]}