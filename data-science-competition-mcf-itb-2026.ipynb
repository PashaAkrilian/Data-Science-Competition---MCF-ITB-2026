{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e905cdda",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-14T09:39:57.997142Z",
     "iopub.status.busy": "2026-02-14T09:39:57.996279Z",
     "iopub.status.idle": "2026-02-14T09:39:59.369835Z",
     "shell.execute_reply": "2026-02-14T09:39:59.368302Z"
    },
    "papermill": {
     "duration": 1.381883,
     "end_time": "2026-02-14T09:39:59.372661",
     "exception": false,
     "start_time": "2026-02-14T09:39:57.990778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/Data_Klaim.csv\n",
      "/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/sample_submission.csv\n",
      "/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/Data_Polis.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e5be05",
   "metadata": {
    "papermill": {
     "duration": 0.003308,
     "end_time": "2026-02-14T09:39:59.380003",
     "exception": false,
     "start_time": "2026-02-14T09:39:59.376695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DATA FOUNDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83640ccc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T09:39:59.389678Z",
     "iopub.status.busy": "2026-02-14T09:39:59.389081Z",
     "iopub.status.idle": "2026-02-14T09:39:59.678678Z",
     "shell.execute_reply": "2026-02-14T09:39:59.677141Z"
    },
    "papermill": {
     "duration": 0.297555,
     "end_time": "2026-02-14T09:39:59.681099",
     "exception": false,
     "start_time": "2026-02-14T09:39:59.383544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Klaim shape : (4627, 13)\n",
      "Initial Polis shape : (4096, 6)\n",
      "\n",
      "Tanggal efektif polis range:\n",
      "2011-12-05 00:00:00\n",
      "2018-02-20 00:00:00\n",
      "Klaim shape after cleaning : (4579, 13)\n",
      "Merged shape : (4579, 18)\n",
      "\n",
      "Final Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4579 entries, 0 to 4578\n",
      "Data columns (total 26 columns):\n",
      " #   Column                         Non-Null Count  Dtype         \n",
      "---  ------                         --------------  -----         \n",
      " 0   claim_id                       4579 non-null   object        \n",
      " 1   nomor_polis                    4579 non-null   object        \n",
      " 2   reimburse_cashless             4579 non-null   object        \n",
      " 3   inpatient_outpatient           4544 non-null   object        \n",
      " 4   icd_diagnosis                  4575 non-null   object        \n",
      " 5   icd_description                4575 non-null   object        \n",
      " 6   status_klaim                   4579 non-null   object        \n",
      " 7   tanggal_pembayaran_klaim       4579 non-null   datetime64[ns]\n",
      " 8   tanggal_pasien_masuk_rs        4579 non-null   datetime64[ns]\n",
      " 9   tanggal_pasien_keluar_rs       4579 non-null   datetime64[ns]\n",
      " 10  nominal_klaim_yang_disetujui   4579 non-null   float64       \n",
      " 11  nominal_biaya_rs_yang_terjadi  4579 non-null   float64       \n",
      " 12  lokasi_rs                      4572 non-null   object        \n",
      " 13  plan_code                      4579 non-null   object        \n",
      " 14  gender                         4579 non-null   object        \n",
      " 15  tanggal_lahir                  4579 non-null   datetime64[ns]\n",
      " 16  tanggal_efektif_polis          4579 non-null   datetime64[ns]\n",
      " 17  domisili                       4579 non-null   object        \n",
      " 18  age                            4579 non-null   float64       \n",
      " 19  tenure_days                    4579 non-null   int64         \n",
      " 20  los                            4579 non-null   int64         \n",
      " 21  claim_ratio                    4579 non-null   float64       \n",
      " 22  year_month                     4579 non-null   period[M]     \n",
      " 23  exposure                       4579 non-null   int64         \n",
      " 24  age_bucket                     4579 non-null   category      \n",
      " 25  is_inpatient                   4579 non-null   int64         \n",
      "dtypes: category(1), datetime64[ns](5), float64(4), int64(4), object(11), period[M](1)\n",
      "memory usage: 899.1+ KB\n",
      "None\n",
      "\n",
      "Unique Months: 24\n",
      "\n",
      "Exposure Sample:\n",
      "  year_month  exposure\n",
      "0    2024-07      4096\n",
      "1    2024-08      4096\n",
      "2    2024-10      4096\n",
      "3    2024-09      4096\n",
      "7    2024-05      4096\n",
      "\n",
      "STAGE 1 COMPLETE â€” SAFE & FULLY FIXED\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 1 â€” DATA FOUNDATION (FINAL SAFE VERSION)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\n",
    "\n",
    "klaim = pd.read_csv(BASE_PATH + \"Data_Klaim.csv\")\n",
    "polis = pd.read_csv(BASE_PATH + \"Data_Polis.csv\")\n",
    "\n",
    "print(\"Initial Klaim shape :\", klaim.shape)\n",
    "print(\"Initial Polis shape :\", polis.shape)\n",
    "\n",
    "# ============================================================\n",
    "# CLEAN COLUMN NAMES\n",
    "# ============================================================\n",
    "\n",
    "def clean_columns(df):\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"_\", regex=False)\n",
    "        .str.replace(\"/\", \"_\", regex=False)\n",
    "        .str.replace(\"-\", \"_\", regex=False)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "klaim = clean_columns(klaim)\n",
    "polis = clean_columns(polis)\n",
    "\n",
    "klaim = klaim.drop_duplicates().reset_index(drop=True)\n",
    "polis = polis.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# ============================================================\n",
    "# DATE PARSING (SAFE & FLEXIBLE)\n",
    "# ============================================================\n",
    "\n",
    "# Parse all possible date columns safely\n",
    "for col in klaim.columns:\n",
    "    if \"tanggal\" in col:\n",
    "        klaim[col] = pd.to_datetime(klaim[col], errors=\"coerce\")\n",
    "\n",
    "for col in polis.columns:\n",
    "    if \"tanggal\" in col:\n",
    "        # Try YYYYMMDD first\n",
    "        try:\n",
    "            polis[col] = pd.to_datetime(\n",
    "                polis[col].astype(str),\n",
    "                format=\"%Y%m%d\",\n",
    "                errors=\"raise\"\n",
    "            )\n",
    "        except:\n",
    "            polis[col] = pd.to_datetime(polis[col], errors=\"coerce\")\n",
    "\n",
    "# Check tanggal efektif polis range\n",
    "if \"tanggal_efektif_polis\" in polis.columns:\n",
    "    print(\"\\nTanggal efektif polis range:\")\n",
    "    print(polis[\"tanggal_efektif_polis\"].min())\n",
    "    print(polis[\"tanggal_efektif_polis\"].max())\n",
    "\n",
    "# ============================================================\n",
    "# CLEANING\n",
    "# ============================================================\n",
    "\n",
    "klaim = klaim.dropna(subset=[\"nomor_polis\"])\n",
    "\n",
    "if \"tanggal_pembayaran_klaim\" in klaim.columns:\n",
    "    klaim = klaim.dropna(subset=[\"tanggal_pembayaran_klaim\"])\n",
    "\n",
    "if \"nominal_klaim_yang_disetujui\" in klaim.columns:\n",
    "    klaim = klaim[klaim[\"nominal_klaim_yang_disetujui\"] > 0]\n",
    "\n",
    "if (\n",
    "    \"tanggal_pasien_masuk_rs\" in klaim.columns and\n",
    "    \"tanggal_pasien_keluar_rs\" in klaim.columns\n",
    "):\n",
    "    klaim = klaim[\n",
    "        klaim[\"tanggal_pasien_keluar_rs\"] >=\n",
    "        klaim[\"tanggal_pasien_masuk_rs\"]\n",
    "    ]\n",
    "\n",
    "print(\"Klaim shape after cleaning :\", klaim.shape)\n",
    "\n",
    "# ============================================================\n",
    "# MERGE\n",
    "# ============================================================\n",
    "\n",
    "df = klaim.merge(polis, on=\"nomor_polis\", how=\"left\")\n",
    "print(\"Merged shape :\", df.shape)\n",
    "\n",
    "# ============================================================\n",
    "# FEATURE ENGINEERING (SAFE)\n",
    "# ============================================================\n",
    "\n",
    "if \"tanggal_pasien_masuk_rs\" in df.columns and \"tanggal_lahir\" in df.columns:\n",
    "    df[\"age\"] = (\n",
    "        (df[\"tanggal_pasien_masuk_rs\"] - df[\"tanggal_lahir\"]).dt.days / 365\n",
    "    )\n",
    "\n",
    "if \"tanggal_pasien_masuk_rs\" in df.columns and \"tanggal_efektif_polis\" in df.columns:\n",
    "    df[\"tenure_days\"] = (\n",
    "        (df[\"tanggal_pasien_masuk_rs\"] - df[\"tanggal_efektif_polis\"]).dt.days\n",
    "    )\n",
    "\n",
    "if \"tanggal_pasien_keluar_rs\" in df.columns and \"tanggal_pasien_masuk_rs\" in df.columns:\n",
    "    df[\"los\"] = (\n",
    "        (df[\"tanggal_pasien_keluar_rs\"] - df[\"tanggal_pasien_masuk_rs\"]).dt.days\n",
    "    )\n",
    "\n",
    "if (\n",
    "    \"nominal_klaim_yang_disetujui\" in df.columns and\n",
    "    \"nominal_biaya_rs_yang_terjadi\" in df.columns\n",
    "):\n",
    "    df[\"claim_ratio\"] = (\n",
    "        df[\"nominal_klaim_yang_disetujui\"] /\n",
    "        df[\"nominal_biaya_rs_yang_terjadi\"]\n",
    "    )\n",
    "\n",
    "if \"tanggal_pembayaran_klaim\" in df.columns:\n",
    "    df[\"year_month\"] = df[\"tanggal_pembayaran_klaim\"].dt.to_period(\"M\")\n",
    "\n",
    "# ============================================================\n",
    "# TRUE EXPOSURE CALCULATION\n",
    "# ============================================================\n",
    "\n",
    "if \"tanggal_efektif_polis\" in polis.columns:\n",
    "\n",
    "    all_months = pd.period_range(\n",
    "        df[\"year_month\"].min(),\n",
    "        df[\"year_month\"].max(),\n",
    "        freq=\"M\"\n",
    "    )\n",
    "\n",
    "    exposure_list = []\n",
    "\n",
    "    for month in all_months:\n",
    "        active_policies = polis[\n",
    "            polis[\"tanggal_efektif_polis\"].dt.to_period(\"M\") <= month\n",
    "        ][\"nomor_polis\"].nunique()\n",
    "\n",
    "        exposure_list.append({\n",
    "            \"year_month\": month,\n",
    "            \"exposure\": active_policies\n",
    "        })\n",
    "\n",
    "    exposure_monthly = pd.DataFrame(exposure_list)\n",
    "\n",
    "    df = df.merge(exposure_monthly, on=\"year_month\", how=\"left\")\n",
    "\n",
    "# ============================================================\n",
    "# STABILITY FEATURES\n",
    "# ============================================================\n",
    "\n",
    "if \"age\" in df.columns:\n",
    "    df[\"age_bucket\"] = pd.cut(\n",
    "        df[\"age\"],\n",
    "        bins=[0,30,45,60,100],\n",
    "        labels=[\"young\",\"adult\",\"mature\",\"senior\"]\n",
    "    )\n",
    "\n",
    "if \"inpatient_outpatient\" in df.columns:\n",
    "    df[\"is_inpatient\"] = (\n",
    "        df[\"inpatient_outpatient\"]\n",
    "        .astype(str)\n",
    "        .str.lower()\n",
    "        .str.contains(\"in\")\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# FINAL CHECK\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nFinal Data Info:\")\n",
    "print(df.info())\n",
    "\n",
    "if \"year_month\" in df.columns:\n",
    "    print(\"\\nUnique Months:\", df[\"year_month\"].nunique())\n",
    "\n",
    "if \"exposure\" in df.columns:\n",
    "    print(\"\\nExposure Sample:\")\n",
    "    print(df[[\"year_month\",\"exposure\"]].drop_duplicates().head())\n",
    "\n",
    "print(\"\\nSTAGE 1 COMPLETE â€” SAFE & FULLY FIXED\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d1683b",
   "metadata": {
    "papermill": {
     "duration": 0.003706,
     "end_time": "2026-02-14T09:39:59.688341",
     "exception": false,
     "start_time": "2026-02-14T09:39:59.684635",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TIME-SERIES DATASET ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "071a5405",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T09:39:59.697568Z",
     "iopub.status.busy": "2026-02-14T09:39:59.697052Z",
     "iopub.status.idle": "2026-02-14T09:39:59.757261Z",
     "shell.execute_reply": "2026-02-14T09:39:59.756063Z"
    },
    "papermill": {
     "duration": 0.068258,
     "end_time": "2026-02-14T09:39:59.760184",
     "exception": false,
     "start_time": "2026-02-14T09:39:59.691926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Monthly Dataset Shape: (21, 25)\n",
      "Time Range: 2024-04 to 2025-12\n",
      "\n",
      "Columns:\n",
      "['year_month', 'total_claim', 'frequency', 'avg_age', 'avg_tenure', 'avg_los', 'avg_claim_ratio', 'severity', 'year_month_dt', 'month_index', 'month', 'month_sin', 'month_cos', 'freq_lag1', 'sev_lag1', 'total_lag1', 'freq_expanding', 'sev_expanding', 'total_expanding', 'freq_roll3', 'sev_roll3', 'freq_growth', 'sev_growth', 'freq_shrink', 'sev_shrink']\n",
      "\n",
      "Preview:\n",
      "  year_month   total_claim  frequency    avg_age   avg_tenure   avg_los  \\\n",
      "0    2024-04  9.281203e+09        218  57.657471  3361.160550  1.050459   \n",
      "1    2024-05  1.103847e+10        233  57.683685  3327.296137  1.158798   \n",
      "2    2024-06  1.127720e+10        221  56.691502  3417.877828  1.389140   \n",
      "3    2024-07  1.159773e+10        202  58.999864  3434.153465  1.608911   \n",
      "4    2024-08  1.895989e+10        283  58.629827  3470.325088  1.572438   \n",
      "\n",
      "   avg_claim_ratio      severity year_month_dt  month_index  ...  \\\n",
      "0         0.914381  4.257433e+07    2024-04-01            3  ...   \n",
      "1         0.891208  4.737540e+07    2024-05-01            4  ...   \n",
      "2         0.885082  5.102806e+07    2024-06-01            5  ...   \n",
      "3         0.899471  5.741451e+07    2024-07-01            6  ...   \n",
      "4         0.962240  6.699608e+07    2024-08-01            7  ...   \n",
      "\n",
      "     total_lag1  freq_expanding  sev_expanding  total_expanding  freq_roll3  \\\n",
      "0  3.809944e+09       65.666667   2.816436e+07     2.207477e+09   65.666667   \n",
      "1  9.281203e+09      103.750000   3.176685e+07     3.975909e+09  135.666667   \n",
      "2  1.103847e+10      129.600000   3.488856e+07     5.388421e+09  182.666667   \n",
      "3  1.127720e+10      144.833333   3.757848e+07     6.369884e+09  224.000000   \n",
      "4  1.159773e+10      153.000000   4.041220e+07     7.116719e+09  218.666667   \n",
      "\n",
      "      sev_roll3  freq_growth  sev_growth  freq_shrink    sev_shrink  \n",
      "0  2.816436e+07     0.054348    0.346246   103.204167  3.566603e+07  \n",
      "1  3.700929e+07     1.000000    0.083929   129.862500  3.818778e+07  \n",
      "2  4.307584e+07     0.068807    0.112769   147.957500  4.037297e+07  \n",
      "3  4.699260e+07    -0.051502    0.077100   158.620833  4.225592e+07  \n",
      "4  5.193932e+07    -0.085973    0.125156   164.337500  4.423952e+07  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "\n",
      "STAGE 2 COMPLETE â€” SMALL DATA OPTIMIZED\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 2 â€” TIME SERIES ENGINEERING (SMALL DATA OPTIMIZED)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# 1. MONTHLY AGGREGATION\n",
    "# ============================================================\n",
    "\n",
    "agg_dict = {\n",
    "    \"nominal_klaim_yang_disetujui\": \"sum\",\n",
    "    \"claim_id\": \"count\",\n",
    "    \"age\": \"mean\",\n",
    "    \"tenure_days\": \"mean\",\n",
    "    \"los\": \"mean\",\n",
    "    \"claim_ratio\": \"mean\"\n",
    "}\n",
    "\n",
    "monthly = (\n",
    "    df.groupby(\"year_month\")\n",
    "      .agg(agg_dict)\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "monthly = monthly.rename(columns={\n",
    "    \"nominal_klaim_yang_disetujui\": \"total_claim\",\n",
    "    \"claim_id\": \"frequency\",\n",
    "    \"age\": \"avg_age\",\n",
    "    \"tenure_days\": \"avg_tenure\",\n",
    "    \"los\": \"avg_los\",\n",
    "    \"claim_ratio\": \"avg_claim_ratio\"\n",
    "})\n",
    "\n",
    "# ============================================================\n",
    "# 2. BASIC METRICS\n",
    "# ============================================================\n",
    "\n",
    "monthly[\"severity\"] = monthly[\"total_claim\"] / monthly[\"frequency\"]\n",
    "\n",
    "monthly = monthly.sort_values(\"year_month\").reset_index(drop=True)\n",
    "monthly[\"year_month_dt\"] = monthly[\"year_month\"].dt.to_timestamp()\n",
    "\n",
    "monthly[\"month_index\"] = np.arange(len(monthly))\n",
    "monthly[\"month\"] = monthly[\"year_month_dt\"].dt.month\n",
    "\n",
    "monthly[\"month_sin\"] = np.sin(2 * np.pi * monthly[\"month\"] / 12)\n",
    "monthly[\"month_cos\"] = np.cos(2 * np.pi * monthly[\"month\"] / 12)\n",
    "\n",
    "# ============================================================\n",
    "# 3. CORE LAG FEATURES (MINIMAL & STRONG)\n",
    "# ============================================================\n",
    "\n",
    "monthly[\"freq_lag1\"] = monthly[\"frequency\"].shift(1)\n",
    "monthly[\"sev_lag1\"] = monthly[\"severity\"].shift(1)\n",
    "monthly[\"total_lag1\"] = monthly[\"total_claim\"].shift(1)\n",
    "\n",
    "# ============================================================\n",
    "# 4. EXPANDING MEAN (VERY IMPORTANT FOR SMALL DATA)\n",
    "# ============================================================\n",
    "\n",
    "monthly[\"freq_expanding\"] = (\n",
    "    monthly[\"frequency\"].shift(1).expanding().mean()\n",
    ")\n",
    "\n",
    "monthly[\"sev_expanding\"] = (\n",
    "    monthly[\"severity\"].shift(1).expanding().mean()\n",
    ")\n",
    "\n",
    "monthly[\"total_expanding\"] = (\n",
    "    monthly[\"total_claim\"].shift(1).expanding().mean()\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 5. STABLE ROLLING (ONLY 3)\n",
    "# ============================================================\n",
    "\n",
    "monthly[\"freq_roll3\"] = (\n",
    "    monthly[\"frequency\"].shift(1).rolling(3).mean()\n",
    ")\n",
    "\n",
    "monthly[\"sev_roll3\"] = (\n",
    "    monthly[\"severity\"].shift(1).rolling(3).mean()\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 6. STABILIZED GROWTH (CLIPPED)\n",
    "# ============================================================\n",
    "\n",
    "monthly[\"freq_growth\"] = (\n",
    "    monthly[\"frequency\"].pct_change().shift(1)\n",
    ")\n",
    "\n",
    "monthly[\"sev_growth\"] = (\n",
    "    monthly[\"severity\"].pct_change().shift(1)\n",
    ")\n",
    "\n",
    "monthly[\"freq_growth\"] = monthly[\"freq_growth\"].clip(-1, 1)\n",
    "monthly[\"sev_growth\"] = monthly[\"sev_growth\"].clip(-1, 1)\n",
    "\n",
    "# ============================================================\n",
    "# 7. SHRINKAGE FEATURE (ANTI-OVERFIT)\n",
    "# ============================================================\n",
    "\n",
    "global_freq_mean = monthly[\"frequency\"].mean()\n",
    "global_sev_mean = monthly[\"severity\"].mean()\n",
    "\n",
    "monthly[\"freq_shrink\"] = (\n",
    "    0.7 * monthly[\"freq_expanding\"] +\n",
    "    0.3 * global_freq_mean\n",
    ")\n",
    "\n",
    "monthly[\"sev_shrink\"] = (\n",
    "    0.7 * monthly[\"sev_expanding\"] +\n",
    "    0.3 * global_sev_mean\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 8. DROP NA\n",
    "# ============================================================\n",
    "\n",
    "monthly = monthly.dropna().reset_index(drop=True)\n",
    "\n",
    "# ============================================================\n",
    "# FINAL CHECK\n",
    "# ============================================================\n",
    "\n",
    "print(\"Final Monthly Dataset Shape:\", monthly.shape)\n",
    "print(\"Time Range:\",\n",
    "      monthly[\"year_month\"].min(),\n",
    "      \"to\",\n",
    "      monthly[\"year_month\"].max())\n",
    "\n",
    "print(\"\\nColumns:\")\n",
    "print(monthly.columns.tolist())\n",
    "\n",
    "print(\"\\nPreview:\")\n",
    "print(monthly.head())\n",
    "\n",
    "print(\"\\nSTAGE 2 COMPLETE â€” SMALL DATA OPTIMIZED\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9020563d",
   "metadata": {
    "papermill": {
     "duration": 0.003573,
     "end_time": "2026-02-14T09:39:59.767976",
     "exception": false,
     "start_time": "2026-02-14T09:39:59.764403",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MODEL DEVELOPMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "709f1f32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T09:39:59.777293Z",
     "iopub.status.busy": "2026-02-14T09:39:59.776898Z",
     "iopub.status.idle": "2026-02-14T09:40:09.893925Z",
     "shell.execute_reply": "2026-02-14T09:40:09.892288Z"
    },
    "papermill": {
     "duration": 10.125892,
     "end_time": "2026-02-14T09:40:09.897163",
     "exception": false,
     "start_time": "2026-02-14T09:39:59.771271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 12\n",
      "Valid size: 4\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 40\n",
      "[LightGBM] [Info] Number of data points in the train set: 12, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 5.526780\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "MAPE Frequency : 0.1738\n",
      "MAPE Severity  : 0.4136\n",
      "MAPE Total     : 0.4532\n",
      "Final Score    : 0.3469\n",
      "\n",
      "STAGE 3 COMPLETE â€” SMALL DATA OPTIMIZED\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 3 â€” SMALL DATA OPTIMIZED MODEL (FINAL SAFE VERSION)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# ============================================================\n",
    "# 1. TIME-BASED SPLIT\n",
    "# ============================================================\n",
    "\n",
    "train = monthly[monthly[\"year_month\"] < \"2025-04\"].copy()\n",
    "valid = monthly[(monthly[\"year_month\"] >= \"2025-04\") & \n",
    "                (monthly[\"year_month\"] < \"2025-08\")].copy()\n",
    "\n",
    "print(\"Train size:\", len(train))\n",
    "print(\"Valid size:\", len(valid))\n",
    "\n",
    "if len(valid) == 0:\n",
    "    raise ValueError(\"Validation set kosong.\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. FEATURE SET (SIMPLIFIED â€” SMALL DATA FRIENDLY)\n",
    "# ============================================================\n",
    "\n",
    "features = [\n",
    "    \"month_index\",\n",
    "    \"month_sin\",\n",
    "    \"month_cos\",\n",
    "    \"freq_lag1\",\n",
    "    \"sev_lag1\",\n",
    "    \"total_lag1\",\n",
    "    \"freq_roll3\",\n",
    "    \"sev_roll3\"\n",
    "]\n",
    "\n",
    "features = [f for f in features if f in monthly.columns]\n",
    "\n",
    "X_train = train[features]\n",
    "X_valid = valid[features]\n",
    "\n",
    "# ============================================================\n",
    "# 3. FREQUENCY MODEL â€” POISSON LIGHTGBM (SMALL TREE)\n",
    "# ============================================================\n",
    "\n",
    "y_train_freq = train[\"frequency\"]\n",
    "y_valid_freq = valid[\"frequency\"]\n",
    "\n",
    "model_freq = lgb.LGBMRegressor(\n",
    "    objective=\"poisson\",\n",
    "    n_estimators=40,        # ðŸ”¥ kecil\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=4,\n",
    "    max_depth=2,\n",
    "    min_child_samples=3,\n",
    "    reg_lambda=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model_freq.fit(X_train, y_train_freq)\n",
    "\n",
    "freq_pred_valid = model_freq.predict(X_valid)\n",
    "freq_pred_valid = np.clip(freq_pred_valid, 1, None)\n",
    "\n",
    "# ============================================================\n",
    "# 4. SEVERITY MODEL â€” RIDGE REGRESSION (STABLE)\n",
    "# ============================================================\n",
    "\n",
    "y_train_sev = np.log1p(train[\"severity\"])\n",
    "y_valid_sev = valid[\"severity\"]\n",
    "\n",
    "model_sev = Ridge(alpha=10)   # ðŸ”¥ strong regularization\n",
    "model_sev.fit(X_train, y_train_sev)\n",
    "\n",
    "sev_pred_valid = np.expm1(model_sev.predict(X_valid))\n",
    "sev_pred_valid = np.clip(sev_pred_valid, 1, None)\n",
    "\n",
    "# ============================================================\n",
    "# 5. TOTAL CLAIM â€” ACTUARIAL\n",
    "# ============================================================\n",
    "\n",
    "total_pred_valid = freq_pred_valid * sev_pred_valid\n",
    "\n",
    "# ============================================================\n",
    "# 6. SAFE MAPE\n",
    "# ============================================================\n",
    "\n",
    "def safe_mape(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask]))\n",
    "\n",
    "mape_freq = safe_mape(valid[\"frequency\"], freq_pred_valid)\n",
    "mape_sev = safe_mape(valid[\"severity\"], sev_pred_valid)\n",
    "mape_total = safe_mape(valid[\"total_claim\"], total_pred_valid)\n",
    "\n",
    "final_score = (mape_freq + mape_sev + mape_total) / 3\n",
    "\n",
    "print(\"\\nMAPE Frequency :\", round(mape_freq, 4))\n",
    "print(\"MAPE Severity  :\", round(mape_sev, 4))\n",
    "print(\"MAPE Total     :\", round(mape_total, 4))\n",
    "print(\"Final Score    :\", round(final_score, 4))\n",
    "\n",
    "print(\"\\nSTAGE 3 COMPLETE â€” SMALL DATA OPTIMIZED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6894208",
   "metadata": {
    "papermill": {
     "duration": 0.004477,
     "end_time": "2026-02-14T09:40:09.906772",
     "exception": false,
     "start_time": "2026-02-14T09:40:09.902295",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TOTAL CLAIM OPTIMIZATION & VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d890cf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T09:40:09.916450Z",
     "iopub.status.busy": "2026-02-14T09:40:09.915646Z",
     "iopub.status.idle": "2026-02-14T09:40:10.486045Z",
     "shell.execute_reply": "2026-02-14T09:40:10.485027Z"
    },
    "papermill": {
     "duration": 0.578193,
     "end_time": "2026-02-14T09:40:10.488586",
     "exception": false,
     "start_time": "2026-02-14T09:40:09.910393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Individual MAPE:\n",
      "Actuarial : 0.4532\n",
      "Ridge : 0.411\n",
      "Holt : 0.4557\n",
      "\n",
      "Blend Weights:\n",
      "Actuarial : 0.323\n",
      "Ridge : 0.355\n",
      "Holt : 0.322\n",
      "\n",
      "MAPE Total (Final Blend): 0.417\n",
      "\n",
      "Final Competition Score\n",
      "MAPE Frequency : 0.1738\n",
      "MAPE Severity  : 0.4136\n",
      "MAPE Total     : 0.417\n",
      "Final Score    : 0.3348\n",
      "\n",
      "STAGE 4 COMPLETE â€” STABLE SMALL DATA BLEND\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 4 â€” TOTAL CLAIM OPTIMIZATION (SMALL DATA MASTER v2)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# ============================================================\n",
    "# 1. SAFE MAPE\n",
    "# ============================================================\n",
    "\n",
    "def safe_mape(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask]))\n",
    "\n",
    "# ============================================================\n",
    "# 2. ACTUARIAL (FROM STAGE 3)\n",
    "# ============================================================\n",
    "\n",
    "total_pred_valid_actuarial = freq_pred_valid * sev_pred_valid\n",
    "\n",
    "# ============================================================\n",
    "# 3. DIRECT RIDGE TOTAL (LOG STABLE)\n",
    "# ============================================================\n",
    "\n",
    "features_total = [\n",
    "    \"month_index\",\n",
    "    \"month_sin\",\n",
    "    \"month_cos\",\n",
    "    \"total_lag1\",\n",
    "    \"freq_lag1\",\n",
    "    \"sev_lag1\"\n",
    "]\n",
    "\n",
    "features_total = [f for f in features_total if f in monthly.columns]\n",
    "\n",
    "X_train_total = train[features_total]\n",
    "X_valid_total = valid[features_total]\n",
    "\n",
    "y_train_total = np.log1p(train[\"total_claim\"])\n",
    "y_valid_total = valid[\"total_claim\"]\n",
    "\n",
    "ridge_total = Ridge(alpha=50)   # sedikit lebih regularized\n",
    "ridge_total.fit(X_train_total, y_train_total)\n",
    "\n",
    "total_pred_valid_ridge = np.expm1(ridge_total.predict(X_valid_total))\n",
    "total_pred_valid_ridge = np.clip(total_pred_valid_ridge, 1, None)\n",
    "\n",
    "ridge_features = features_total.copy()\n",
    "\n",
    "# ============================================================\n",
    "# 4. HOLT TREND MODEL (DAMPED)\n",
    "# ============================================================\n",
    "\n",
    "holt_model = ExponentialSmoothing(\n",
    "    train[\"total_claim\"],\n",
    "    trend=\"add\",\n",
    "    damped_trend=True,\n",
    "    seasonal=None\n",
    ").fit(optimized=True)\n",
    "\n",
    "total_pred_valid_holt = holt_model.forecast(len(valid))\n",
    "total_pred_valid_holt = np.clip(total_pred_valid_holt, 1, None)\n",
    "\n",
    "# ============================================================\n",
    "# 5. EVALUATE INDIVIDUAL MODELS\n",
    "# ============================================================\n",
    "\n",
    "models = {\n",
    "    \"Actuarial\": total_pred_valid_actuarial,\n",
    "    \"Ridge\": total_pred_valid_ridge,\n",
    "    \"Holt\": total_pred_valid_holt\n",
    "}\n",
    "\n",
    "mape_scores = {k: safe_mape(y_valid_total, v) for k, v in models.items()}\n",
    "\n",
    "print(\"\\nIndividual MAPE:\")\n",
    "for k, v in mape_scores.items():\n",
    "    print(k, \":\", round(v, 4))\n",
    "\n",
    "# ============================================================\n",
    "# 6. STABLE BLEND (SOFT INVERSE ERROR)\n",
    "# ============================================================\n",
    "\n",
    "# gunakan soft weighting agar tidak overfit valid\n",
    "epsilon = 0.02\n",
    "weights = {k: 1 / (v + epsilon) for k, v in mape_scores.items()}\n",
    "\n",
    "total_weight = sum(weights.values())\n",
    "weights = {k: v / total_weight for k, v in weights.items()}\n",
    "\n",
    "print(\"\\nBlend Weights:\")\n",
    "for k, v in weights.items():\n",
    "    print(k, \":\", round(v, 3))\n",
    "\n",
    "# ============================================================\n",
    "# 7. FINAL BLEND\n",
    "# ============================================================\n",
    "\n",
    "total_pred_valid_blend = sum(\n",
    "    weights[k] * models[k] for k in models\n",
    ")\n",
    "\n",
    "mape_total_blend = safe_mape(y_valid_total, total_pred_valid_blend)\n",
    "\n",
    "print(\"\\nMAPE Total (Final Blend):\", round(mape_total_blend, 4))\n",
    "\n",
    "# ============================================================\n",
    "# 8. FINAL COMPETITION SCORE\n",
    "# ============================================================\n",
    "\n",
    "mape_freq_final = safe_mape(valid[\"frequency\"], freq_pred_valid)\n",
    "mape_sev_final  = safe_mape(valid[\"severity\"], sev_pred_valid)\n",
    "mape_total_final = mape_total_blend\n",
    "\n",
    "final_score = (mape_freq_final + mape_sev_final + mape_total_final) / 3\n",
    "\n",
    "print(\"\\nFinal Competition Score\")\n",
    "print(\"MAPE Frequency :\", round(mape_freq_final, 4))\n",
    "print(\"MAPE Severity  :\", round(mape_sev_final, 4))\n",
    "print(\"MAPE Total     :\", round(mape_total_final, 4))\n",
    "print(\"Final Score    :\", round(final_score, 4))\n",
    "\n",
    "print(\"\\nSTAGE 4 COMPLETE â€” STABLE SMALL DATA BLEND\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b33d495",
   "metadata": {
    "papermill": {
     "duration": 0.005946,
     "end_time": "2026-02-14T09:40:10.500104",
     "exception": false,
     "start_time": "2026-02-14T09:40:10.494158",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TEST PREDICTION & KAGGLE SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a7312ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T09:40:10.514906Z",
     "iopub.status.busy": "2026-02-14T09:40:10.514061Z",
     "iopub.status.idle": "2026-02-14T09:40:10.645447Z",
     "shell.execute_reply": "2026-02-14T09:40:10.644132Z"
    },
    "papermill": {
     "duration": 0.142478,
     "end_time": "2026-02-14T09:40:10.648156",
     "exception": false,
     "start_time": "2026-02-14T09:40:10.505678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All IDs matched perfectly.\n",
      "\n",
      "Submission file created successfully.\n",
      "                        id         value\n",
      "0  2025_08_Claim_Frequency  2.519411e+02\n",
      "1   2025_08_Claim_Severity  1.670376e+08\n",
      "2      2025_08_Total_Claim  5.855885e+10\n",
      "3  2025_09_Claim_Frequency  2.509336e+02\n",
      "4   2025_09_Claim_Severity  7.313531e+06\n",
      "5      2025_09_Total_Claim  2.619107e+08\n",
      "6  2025_10_Claim_Frequency  2.606578e+02\n",
      "7   2025_10_Claim_Severity  2.283074e+07\n",
      "8      2025_10_Total_Claim  2.186332e+10\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 5 â€” FINAL SUBMISSION GENERATOR (MASTER SAFE VERSION)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/datasets/dimaspashaakrilian/dsc-itb/\"\n",
    "sample_sub = pd.read_csv(BASE_PATH + \"sample_submission.csv\")\n",
    "\n",
    "# ============================================================\n",
    "# EXTRACT FUTURE MONTHS (SORTED)\n",
    "# ============================================================\n",
    "\n",
    "sample_sub[\"year\"]  = sample_sub[\"id\"].str.split(\"_\").str[0]\n",
    "sample_sub[\"month\"] = sample_sub[\"id\"].str.split(\"_\").str[1]\n",
    "\n",
    "sample_sub[\"month_key\"] = sample_sub[\"year\"] + \"-\" + sample_sub[\"month\"]\n",
    "\n",
    "future_periods = (\n",
    "    pd.PeriodIndex(sample_sub[\"month_key\"], freq=\"M\")\n",
    "      .unique()\n",
    "      .sort_values()\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# COPY HISTORICAL DATA\n",
    "# ============================================================\n",
    "\n",
    "future_df = monthly.copy().sort_values(\"year_month\").reset_index(drop=True)\n",
    "predictions = {}\n",
    "\n",
    "# ============================================================\n",
    "# FEATURE LIST (SAME AS STAGE 3)\n",
    "# ============================================================\n",
    "\n",
    "features = [\n",
    "    \"month_index\",\n",
    "    \"month_sin\",\n",
    "    \"month_cos\",\n",
    "    \"freq_lag1\",\n",
    "    \"sev_lag1\",\n",
    "    \"total_lag1\",\n",
    "    \"freq_roll3\",\n",
    "    \"sev_roll3\"\n",
    "]\n",
    "\n",
    "features = [f for f in features if f in monthly.columns]\n",
    "\n",
    "# ============================================================\n",
    "# RECURSIVE FORECASTING LOOP\n",
    "# ============================================================\n",
    "\n",
    "for period in future_periods:\n",
    "\n",
    "    last_row = future_df.iloc[-1]\n",
    "\n",
    "    new_row = {}\n",
    "\n",
    "    # ---- TIME FEATURES ----\n",
    "    new_row[\"year_month\"] = period\n",
    "    new_row[\"year_month_dt\"] = period.to_timestamp()\n",
    "    new_row[\"month_index\"] = last_row[\"month_index\"] + 1\n",
    "    new_row[\"month\"] = period.month\n",
    "\n",
    "    new_row[\"month_sin\"] = np.sin(2 * np.pi * period.month / 12)\n",
    "    new_row[\"month_cos\"] = np.cos(2 * np.pi * period.month / 12)\n",
    "\n",
    "    # ---- LAG FEATURES ----\n",
    "    new_row[\"freq_lag1\"] = last_row[\"frequency\"]\n",
    "    new_row[\"sev_lag1\"] = last_row[\"severity\"]\n",
    "    new_row[\"total_lag1\"] = last_row[\"total_claim\"]\n",
    "\n",
    "    new_row[\"freq_roll3\"] = future_df[\"frequency\"].tail(3).mean()\n",
    "    new_row[\"sev_roll3\"]  = future_df[\"severity\"].tail(3).mean()\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    temp = pd.DataFrame([new_row])\n",
    "\n",
    "    # Ensure no missing columns\n",
    "    for col in features:\n",
    "        if col not in temp.columns:\n",
    "            temp[col] = 0\n",
    "\n",
    "    temp = temp.fillna(0)\n",
    "\n",
    "    X_temp = temp[features]\n",
    "\n",
    "    # =====================================================\n",
    "    # PREDICT FREQUENCY (POISSON â†’ NO EXP)\n",
    "    # =====================================================\n",
    "\n",
    "    freq_pred = model_freq.predict(X_temp)[0]\n",
    "    freq_pred = max(freq_pred, 1)\n",
    "\n",
    "    # =====================================================\n",
    "    # PREDICT SEVERITY (RIDGE LOG1P)\n",
    "    # =====================================================\n",
    "\n",
    "    sev_pred = np.expm1(model_sev.predict(X_temp))[0]\n",
    "    sev_pred = max(sev_pred, 1)\n",
    "\n",
    "    # =====================================================\n",
    "    # TOTAL CLAIM\n",
    "    # =====================================================\n",
    "\n",
    "    total_actuarial = freq_pred * sev_pred\n",
    "\n",
    "    # If Stage 4 ridge_total exists\n",
    "    if \"ridge_total\" in globals():\n",
    "\n",
    "        # Ensure feature order EXACT SAME\n",
    "        for col in ridge_features:\n",
    "            if col not in temp.columns:\n",
    "                temp[col] = 0\n",
    "\n",
    "        X_ridge = temp[ridge_features]\n",
    "        X_ridge = X_ridge.fillna(0)\n",
    "\n",
    "        total_ridge = np.expm1(\n",
    "            ridge_total.predict(X_ridge)\n",
    "        )[0]\n",
    "\n",
    "        total_ridge = max(total_ridge, 1)\n",
    "        total_pred = total_ridge\n",
    "\n",
    "    else:\n",
    "        total_pred = total_actuarial\n",
    "\n",
    "    # =====================================================\n",
    "    # STORE BACK FOR NEXT ITERATION\n",
    "    # =====================================================\n",
    "\n",
    "    new_row[\"frequency\"] = freq_pred\n",
    "    new_row[\"severity\"] = sev_pred\n",
    "    new_row[\"total_claim\"] = total_pred\n",
    "\n",
    "    future_df = pd.concat(\n",
    "        [future_df, pd.DataFrame([new_row])],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # =====================================================\n",
    "    # SAVE USING EXACT FORMAT YYYY_MM\n",
    "    # =====================================================\n",
    "\n",
    "    month_str = f\"{period.year}_{str(period.month).zfill(2)}\"\n",
    "\n",
    "    predictions[f\"{month_str}_Claim_Frequency\"] = freq_pred\n",
    "    predictions[f\"{month_str}_Claim_Severity\"]  = sev_pred\n",
    "    predictions[f\"{month_str}_Total_Claim\"]     = total_pred\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# BUILD FINAL SUBMISSION\n",
    "# ============================================================\n",
    "\n",
    "submission = sample_sub.copy()\n",
    "submission[\"value\"] = submission[\"id\"].map(predictions)\n",
    "\n",
    "missing = submission[\"value\"].isna().sum()\n",
    "\n",
    "if missing > 0:\n",
    "    print(\"ERROR: Ada ID tidak cocok sebanyak:\", missing)\n",
    "else:\n",
    "    print(\"All IDs matched perfectly.\")\n",
    "\n",
    "submission = submission[[\"id\", \"value\"]]\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"\\nSubmission file created successfully.\")\n",
    "print(submission.head(9))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 9488145,
     "sourceId": 14836320,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17.806262,
   "end_time": "2026-02-14T09:40:11.776260",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-14T09:39:53.969998",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
